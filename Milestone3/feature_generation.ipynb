{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Milestone 2 Model\n","  * Please use GPUs to run this in an shorter time frame. It may take an hour or more to run a few of our cells otherwise"],"metadata":{"id":"lIROfmnLiul6"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"8UMpvvqHiz1K"}},{"cell_type":"code","source":["!pip install poetry\n","!pip install langchain\n","!pip install sentence_transformers\n","!pip install openai\n","!pip install pypdf\n","!poetry config virtualenvs.in-project true\n","!poetry install --no-ansi"],"metadata":{"id":"KIrhD55VfRrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, sys\n","\n","VENV_PATH = \"/content/gdrive/MyDrive/test-poetry/.venv/lib/python3.8/site-packages\"\n","LOCAL_VENV_PATH = '/content/venv' # local notebook\n","os.symlink(VENV_PATH, LOCAL_VENV_PATH) # connect to directory in drive\n","sys.path.insert(0, LOCAL_VENV_PATH)"],"metadata":{"id":"NzH5-0ahiG4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEfRE1WUe92L"},"outputs":[],"source":["from langchain.storage import InMemoryStore\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","from langchain.document_loaders import DirectoryLoader\n","from langchain.retrievers.multi_vector import MultiVectorRetriever\n","from sentence_transformers import SentenceTransformer, util\n","from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceHubEmbeddings\n","from langchain.llms import OpenAI\n","from langchain.document_loaders import DirectoryLoader, UnstructuredFileLoader, TextLoader, Docx2txtLoader, PyPDFDirectoryLoader, PyPDFLoader\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores.faiss import FAISS\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n","from langchain.prompts import PromptTemplate\n","from openai import OpenAI\n","from langchain.chat_models import ChatOpenAI\n","\n","import textwrap\n","import nltk\n","import os\n","import pandas as pd\n","import glob"]},{"cell_type":"markdown","source":["## Data + Model Set-up"],"metadata":{"id":"dsfTx0-hi_m_"}},{"cell_type":"code","source":["df = pd.read_excel('530_project_test_dev.xlsx')\n","mylist = df['Test/Dev Input'].tolist()"],"metadata":{"id":"nLlaJIx6hIVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUMBER_OF_RESULTS = 10\n","SEARCH_DISTANCE_THRESHOLD = 0.6\n","OPENAI_API_KEY = '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'\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"],"metadata":{"id":"c4rmlNLtjReX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(\n","    model_name=\"gpt-3.5-turbo-16k\",\n","    temperature=1\n",")"],"metadata":{"id":"Lh1vZ7IT81hU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["store = InMemoryStore()\n","\n","embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/msmarco-bert-base-dot-v5')"],"metadata":{"id":"_soeCAP-k8_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip TrainingDataSmall.zip"],"metadata":{"id":"V_b4RPzRqIbB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d83c4ec-0086-4d3f-e2c7-5a8d3360bd09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  TrainingDataSmall.zip\n","replace TrainingDataSmall/Weijie Su_document.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n","  inflating: TrainingDataSmall/Weijie Su_document.pdf  \n","replace TrainingDataSmall/Weijie Su_2307.02792.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["%%time\n","loader = PyPDFDirectoryLoader(\"TrainingDataSmall\")\n","docs = loader.load()\n","docs"],"metadata":{"id":"NmLcwbBBlFQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=100000,\n","                                               chunk_overlap=50,\n","                                               separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"], )\n","all_splits = text_splitter.split_documents(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fSVfKGbPmsc2","outputId":"b5f67d3d-dfce-47eb-c59d-9b382e88a4c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 235 ms, sys: 994 µs, total: 236 ms\n","Wall time: 237 ms\n"]}]},{"cell_type":"code","source":["!pip install chromadb"],"metadata":{"id":"BUz31oHg3tmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","vectorstore = Chroma(\n","    collection_name=\"professor-data\",\n","    embedding_function=embeddings\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYu566nlmtcu","outputId":"5a161c04-4bf0-4415-bae0-02169e96b10e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 615 ms, sys: 23.8 ms, total: 638 ms\n","Wall time: 730 ms\n"]}]},{"cell_type":"code","source":["%%time\n","store = InMemoryStore()\n","id_key = \"doc_id\"\n","\n","retriever = MultiVectorRetriever(\n","     vectorstore=vectorstore,\n","     docstore=store,\n","     id_key=id_key,\n","      search_type=\"similarity\",\n","      search_kwargs={\n","          \"k\": NUMBER_OF_RESULTS,\n","          \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n","      })"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zyzrQYR3oPD","outputId":"7be2a6a4-a2ba-4fa3-989d-ea2f4c041c28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 151 µs, sys: 0 ns, total: 151 µs\n","Wall time: 156 µs\n"]}]},{"cell_type":"code","source":["%%time\n","import uuid\n","\n","doc_ids = [str(uuid.uuid4()) for _ in all_splits]\n","\n","for i, doc in enumerate(all_splits):\n","    _id = doc_ids[i]\n","    doc.metadata[id_key] = _id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqLjoTqr5mX_","outputId":"8cdfeb58-d53c-4463-fc15-21eb587e5a49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 15.7 ms, sys: 28 µs, total: 15.7 ms\n","Wall time: 14.4 ms\n"]}]},{"cell_type":"code","source":["%%time\n","retriever.vectorstore.add_documents(all_splits)\n","retriever.docstore.mset(list(zip(doc_ids, all_splits)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g90UNL924UGL","outputId":"a24089e9-5e13-4c8d-e35c-1094520eaae5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 33s, sys: 1.49 s, total: 1min 34s\n","Wall time: 1min 36s\n"]}]},{"cell_type":"code","source":["# Links to papers\n","#https://www.cis.upenn.edu/~mkearns/papers/pricemodel.pdf\n","#https://arxiv.org/pdf/2211.11158.pdf\n","#https://link.springer.com/content/pdf/10.1023/A:1017984413808.pdf"],"metadata":{"id":"CcGqHQcVwVwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["template = \"\"\"\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: {question}\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","We examine a Markovian model for the price evolution of a\n","stock, in which the probability of local upward or downward movement\n","is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information).\n","This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","(which knows all of the underlying parameters of the infinite and possibly\n","nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","{context}:\"\"\""],"metadata":{"id":"F5_zUP3J_3QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=retriever,\n","    return_source_documents=True,\n","    verbose=True,\n","    chain_type_kwargs={\"prompt\": PromptTemplate(\n","            template=template,\n","            input_variables=[\"context\"]),\n","    },\n",")\n","\n","qa.combine_documents_chain.verbose = True\n","qa.combine_documents_chain.llm_chain.verbose = True\n","qa.combine_documents_chain.llm_chain.llm.verbose = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFRGuYFT8_zo","outputId":"9d652494-af15-4483-95a6-1a3f8624eeec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 750 µs, sys: 0 ns, total: 750 µs\n","Wall time: 760 µs\n"]}]},{"cell_type":"code","source":["professor_list = [\n","  'Bong Ho Kim',\n"," 'Junhyong Kim ',\n"," 'Daniel E. Koditschek',\n"," 'Konrad Koering',\n"," 'Lingjie Liu',\n"," 'Adam David Mally',\n"," 'Andre Scedrov',\n"," 'Ryan Marcus',\n"," 'Linh Thi Xuan Phan',\n"," 'Rahul Mangharam',\n"," 'Tal Rabin',\n"," 'Vincent Liu',\n"," 'Jérémie O. Lumbroso',\n"," 'George J. Pappas',\n"," 'Qi Long',\n"," 'Nikolai Matni',\n"," 'Harvey Rubin',\n"," 'Mayur Naik',\n"," 'Joshua B. Plotkin',\n"," 'Dan Roth',\n"," 'Alejandro Ribeiro',\n"," 'Travis Q. McGaha',\n"," 'Mark L. Liberman',\n"," 'Robin Pemantle',\n"," 'Danaë Metaxa',\n"," 'Victor M. Preciado',\n"," 'Benjamin C. Pierce',\n"," 'Aaron Roth',\n"," 'Pratyush Mishra',\n"," 'Shirin Saeedi Bidokhti',\n"," 'Boon Thau Loo',\n"," 'Michael Posa',\n"," 'Surbi Goel',\n"," 'Jacob Gardner',\n"," 'Rajiv Gandhi',\n"," 'Eric Fouh',\n"," 'Gushu Li',\n"," 'Jing Li',\n"," 'Insup Lee',\n"," 'Benjamin Lee',\n"," 'Stephen Lane',\n"," 'Vijay Kumar',\n"," 'Nadia Figueroa',\n"," 'Joe Devietti',\n"," 'Pratik Chaudhari',\n"," 'Damon Centola',\n"," 'Chris Callison-Burch',\n"," 'Susan Davidson',\n"," 'Osbert Bastani',\n"," 'Yoseph Barash',\n"," 'Sharath Chandra Guntuku',\n"," 'Andreas Haeberlen',\n"," 'Daniel Hashimoto',\n"," 'Hamed Hassani',\n"," 'Andrew Head',\n"," 'Brett Hemenway ',\n"," 'Daniel J Hopkins',\n"," 'M. Ani Hsieh ',\n"," 'Zachary Ives',\n"," 'Sanjeev Khanna',\n"," 'Michael Kearns ',\n"," 'Dinesh Jayaraman',\n"," 'Kevin B. Johnson ',\n"," 'Yasmin Kafai ',\n"," 'Sampath K. Kannan ',\n"," 'Sebastian Angel',\n"," 'Rajeev Alur',\n"," 'Shivani Agarwal',\n"," 'James Gee',\n"," 'Jean Gallier',\n"," 'Thomas Farmer',\n"," 'Eric Eaton',\n"," 'Andre DeHon',\n"," 'Anindya De',\n"," 'Kostas Daniilidis',\n"," 'Ryan Baker',\n"," 'Justin Gottschlich',\n"," 'Norman I. Badler',\n"," 'Mingmin Zhao',\n"," 'Val B. Tannen',\n"," 'Mark Yatskar',\n"," 'Mark Yim',\n"," 'Li-San Wang',\n"," 'Christopher S. Yoo',\n"," 'Charles Yang',\n"," 'Camillo Taylor',\n"," 'Cynthia Sung',\n"," 'Duncan Watts',\n"," 'Eric Weingarten',\n"," 'Eric Wong',\n"," 'Harry Smith',\n"," 'Jianbo Shi',\n"," 'Jonathan Smith',\n"," 'Lyle Ungar',\n"," 'Oleg Sokolsky',\n"," 'Renee Vidal',\n"," 'Rakesh Vohra',\n"," 'Scott Weinstein',\n"," 'Stephanie Weirich',\n"," 'Steven Zdancemic',\n"," 'Swapneel Seth',\n"," 'Weijie Su']"],"metadata":{"id":"0v_u7yNYEpTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","professor_question_list=[]\n","for item in professor_list:\n","  string = \"What are key features of {}'s work?\".format(item)\n","  professor_question_list.append(string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qU28FhLlGGq7","outputId":"db331d9b-c6bc-419a-b2cb-a694da676a83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 48 µs, sys: 0 ns, total: 48 µs\n","Wall time: 50.5 µs\n"]}]},{"cell_type":"markdown","source":["## Results\n","- Use this section to get the results for either the weak or strong baseline"],"metadata":{"id":"DppiqgAHkU6O"}},{"cell_type":"code","source":["def formatter(result):\n","    print(f\"Query: {result['query']}\")\n","    print(\".\" * 80)\n","    if \"source_documents\" in result.keys():\n","        for idx, ref in enumerate(result[\"source_documents\"]):\n","            print(\"-\" * 80)\n","            print(f\"REFERENCE #{idx}\")\n","            print(\"-\" * 80)\n","            if \"score\" in ref.metadata:\n","                print(f\"Matching Score: {ref.metadata['score']}\")\n","            if \"source\" in ref.metadata:\n","                print(f\"Document Source: {ref.metadata['source']}\")\n","            if \"document_name\" in ref.metadata:\n","                print(f\"Document Name: {ref.metadata['document_name']}\")\n","            print(\".\" * 80)\n","            print(f\"Content: \\n{wrap(ref.page_content)}\")\n","    print(\".\" * 80)\n","    print(f\"Response: {wrap(result['result'])}\")\n","    print(\".\" * 80)\n","    return wrap(result['result'])\n","\n","def wrap(s):\n","    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n","\n","def ask(query, qa=qa, k=NUMBER_OF_RESULTS, search_distance=SEARCH_DISTANCE_THRESHOLD):\n","    qa.retriever.search_kwargs[\"search_distance\"] = search_distance\n","    qa.retriever.search_kwargs[\"k\"] = k\n","    result = qa({\"query\": query})\n","    return formatter(result)"],"metadata":{"id":"OK6Hm4R49D5X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","#Please note this cell will take between 15 and 20 minutes to run with GPUs\n","import time\n","results = []\n","for question in professor_question_list:\n","    time.sleep(7)\n","    print(question)\n","    results.append(ask(question))\n","\n","print(results)"],"metadata":{"id":"7NZMgB_J9JXb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d362b489-568c-4d6f-d3e5-5010a4600fb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","What are key features of Renee Vidal's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Renee Vidal's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","Current\n","research\n","Past\n","projects\n","Publications\n","Community\n","service\n","Education\n","Oleg\n","Sokolsky\n","Research\n","Professor\n","\n","The Role of Visualization in Computer Science Education  \n"," \n"," \n","ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER  \n"," \n","Department of Computer Science  \n","Virginia Tech  \n","Blacksburg, VA 24061  \n"," \n"," \n","Computer Science core instruction attempts to provide a detailed understanding o f dy-\n","namic processes such as the working of an algorithm or the flow of information between \n","computing ent ities. Such dynamic processes are not well explained by static media such \n","as text and images, and are difficult to convey in le cture. We survey the hist ory of visual i-\n","zation in CS education, foc using on artifacts that have  a documented positive educational \n","assessment. We discuss how changes in computing technology have affected the deve l-\n","opment and uptake of such visualization artifacts in CS education, and how recent tech-\n","nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization, data structure visualization, program visualiz a-\n","tion, eTextbooks, hypertextbooks.  \n","  \n","(Draft of paper to appear in Computers in the Schools , 2012)\n","\n","Maximum Satisﬁability in Program Analysis:\n","Applications and Techniques\n","Mayur Naik1, Xujie Si1, Xin Zhang1, and Radu Grigore2\n","1University of Pennsylvania2University of Kent\n","Abstract. A central challenge in program analysis concerns balanc-\n","ing diﬀerent competing tradeoﬀs. To address this challenge, we propose\n","an approach based on the Maximum Satisﬁability (MaxSAT) problem,\n","an optimization extension of the Boolean Satisﬁability (SAT) problem.\n","We demonstrate the approach on three diverse applications that ad-\n","vance the state-of-the-art in balancing tradeoﬀs in program analysis.\n","Enabling these applications on real-world programs necessitates solv-\n","ing large MaxSAT instances comprising over 1030clauses in a sound and\n","optimal manner. We propose a general framework that scales to such\n","instances by iteratively expanding a subset of clauses while providing\n","soundness and optimality guarantees. We also present new techniques to\n","instantiate and optimize the framework.\n","Keywords: maximum satisﬁability, program analysis\n","\n","Inductive Biases and Variable Creation\n","in Self-Attention Mechanisms\n","Benjamin L. Edelman1Surbhi Goel2Sham Kakade1,2Cyril Zhang2\n","1Harvard University2Microsoft Research NYC\n","bedelman@g.harvard.edu, {goel.surbhi, sham.kakade, cyrilzhang }@microsoft.com\n","Abstract\n","Self-attention, an architectural motif designed to model long-range interactions in sequential data, has\n","driven numerous recent breakthroughs in natural language processing and beyond. This work provides a\n","theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish\n","which functions and long-range dependencies self-attention blocks prefer to represent. Our main result\n","shows that bounded-norm Transformer networks “create sparse variables”: a single self-attention head\n","can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically\n","with the context length. To support our analysis, we present synthetic experiments to probe the sample\n","complexity of learning sparse Boolean functions with Transformers.\n","1 Introduction\n","Self-attention mechanisms have comprised an era-deﬁning cornerstone of deep learning in recent years, ap-\n","pearing ubiquitously in empirical breakthroughs in generative sequence modeling and unsupervised represen-\n","tation learning. Starting with natural language (Vaswani et al., 2017), self-attention has enjoyed surprising\n","empirical successes in numerous and diverse modalities of data. In many of these settings, self-attention has\n","supplanted traditional recurrent and convolutional architectures, which are understood to incorporate in-\n","ductive biases about temporal and translational invariances in the data. Self-attention models discard these\n","functional forms, in favor of directly and globally modeling long-range interactions within the input context.\n","The proliferation of self-attention raises a fundamental question about its inductive biases: which func-\n","tions do self-attention networks prefer to represent? Various intuitions and empirics inform the design of\n","these architectures, but formal statistical abstractions and analyses are missing in this space. To this end,\n","this work initiates an analysis of the statistical foundations of self-attention.\n","We identify an inductive bias for self-attention, for which we coin the term sparse variable creation : a\n","bounded-norm self-attention head learns a sparse function (which only depends on a small subset of input\n","coordinates, such as a constant-fan-in gate in a Boolean circuit) of a length- Tcontext, with sample complexity\n","scaling as log( T). The main technical novelty in this work is a covering number-based capacity bound for\n","attention mechanisms (including Transformer heads, as well as related and future architectures), implying\n","norm-based generalization bounds. This is accompanied by a matching representational result, showing that\n","bounded-norm self-attention heads are indeed capable of representing s-sparse functions with weight norms\n","2O(s)(or poly(s), for symmetric sparse functions). This provides a theoretical account for why attention\n","models can learn long-range dependencies without overﬁtting.\n","Finally, we conduct synthetic experiments to probe the sample eﬃciency of learning sparse interactions\n","with self-attention. We train Transformer models to identify sparse Boolean functions with randomly chosen\n","indices, and corroborate the sample complexity scaling law predicted by the theory. A variant of this\n","experiment (with i.i.d. samples) reveals a computational mystery, beyond the scope of our current statistical\n","analysis: we ﬁnd that Transformers can successfully learn the “hardest” (in the sense of SQ-dimension)\n","s-sparse functions: the XOR (parity) functions.\n","1arXiv:2110.10090v2  [cs.LG]  24 Jun 2022\n","\n","erikwaing\n"," \n","Erik\n","Waingarten\n"," \n","Publications\n"," \n","Teaching \n","Erik\n","Waingarten\n","\n","11/8/23, 1:20 PMVictor M. Preciado - Research Interests\n","https://sites.google.com/site/victormpreciado/research-11/7\n","Research Interests\n","My primary research interests are modeling, analysis, and control of complex networked systems andengineering infrastructures, with applications in social networks, technological infrastructure, andbiological systems. Since critical societal functions are increasingly dependent on complex networks, it isimportant to acquire a deeper understanding of the relationship between the structure and the dynamicsof these systems. Research lines of particular interest are described below:\n","Victor M. PreciadoHomeResearch InterestsList of PublicationsT\n","\n","Eric\n","Wong\n"," \n","GROUP\n"," \n","BLOG\n"," \n","RESEARCH\n"," \n","TEACHING\n"," \n","PAPERS\n"," \n","ABOUT\n","Eric\n","Wong\n","Assistant\n","Professor ,\n","University\n","of\n","Pennsylvania\n","\n","Parsing Randomness∗\n","Unifying and Differentiating Parsers and Random Generators\n","Harrison Goldstein\n","University of Pennsylvania\n","Philadelphia, PA, USA\n","hgo@seas.upenn.eduBenjamin C. Pierce\n","University of Pennsylvania\n","Philadelphia, PA, USA\n","bcpierce@cis.upenn.edu\n","Abstract\n","“A generator is a parser of randomness. ” This perspective on\n","generators for random data structures is well established as\n","folklore in the programming languages community, but it has\n","apparently never been formalized, nor have its consequences\n","been deeply explored.\n","We present free generators , which unify parsing and gener-\n","ation using a common structure that makes the relationship\n","between the two concepts precise. Free generators lead natu-\n","rally to a proof that a large class of generators can be factored\n","into a parser plus a distribution over choice sequences. Fur-\n","ther, free generators support a notion of derivative , analogous\n","to familiar Brzozowski derivatives of formal languages, that\n","allows analysis tools to “preview” the effect of a particular\n","generator choice. This, in turn, gives rise to a novel algo-\n","rithm for generating data structures satisfying user-specified\n","preconditions.\n","Keywords: Random generation, Parsing, Property-based test-\n","ing, Formal languages\n","1 Introduction\n","“A generator is a parser of randomness. . . ” It’s one of those ob-\n","servations that’s totally puzzling right up to the moment it be-\n","comes totally obvious: a random generator—such as might be\n","found in a property-based testing tool like QuickCheck [ 4]—\n","is a transformer from a series of random choices into a data\n","structure, just as a parser transforms a series of characters\n","into a data structure.\n","Although this connection may be obvious once it is pointed\n","out, few actually think of generators this way. Indeed, to our\n","knowledge the framing of random generators as parsers has\n","never been explored formally. But this is a shame! The re-\n","lationship between these fundamental concepts deserves a\n","deeper look.\n","A generator is a program that builds a data structure by\n","making a sequence of random choices—those choices are\n","the key. A “traditional” generator makes decisions using a\n","stored source of randomness (e.g., a seed) that it consults and\n","updates whenever it must make a choice. Equivalently, if we\n","like, we can pre-compute a list of choices and pass it in to the\n","∗This paper was originally submitted to PLDI’22 and not accepted. The\n","reviewers felt that the ideas were good, but that the presentation was\n","incomplete.generator, which gradually walks down the list whenever it\n","needs to make random decisions. In this mode of operation,\n","the generator is effectively parsing the sequence of choices\n","into a data structure!\n","To connect generators and parsers, we introduce a data\n","structure called a free generator that can be interpreted as\n","either a generator or as a parser. Free generators have a rich\n","theory; in particular, we can use them to prove that a subset\n","of generator programs can be factored into a parser and a\n","distribution over sequences of choices.\n","Besides clarifying folklore, free generators admit transfor-\n","mations that cannot be implemented for standard generators\n","and parsers. A particularly exciting one is a notion of de-\n","rivative which modifies a generator by asking the question:\n","“what would this generator look like after it makes choice 𝑐?”\n","The derivative gives a way of previewing a particular choice\n","to determine how likely it is to lead us to useful values.\n","We use derivatives of free generators to tackle a well-\n","known problem—we call it the valid generation problem . The\n","challenge is to generate a large number of random values\n","that satisfy some validity condition. This problem comes up\n","often in property-based testing, where the validity condition\n","is the precondition of some functional specification. Since\n","generator derivatives give a way of previewing the effects\n","of a particular choice, we can use gradients (derivatives with\n","respect to a vector of choices) to preview all possible choices\n","and pick a promising one. This leads us to an elegant algo-\n","rithm for turning a naïve free generator into one that only\n","generates valid values.\n","In §2 below, we introduce the ideas behind free generators\n","and the operations that can be defined on them. We then\n","present our main contributions:\n","•We formalize the folklore analogy between parsers\n","and generators using free generators , a novel class of\n","structures that make choices explicit and support syn-\n","tactic transformations (§3). We use free generators to\n","prove that every “applicative” generator can factored\n","into a parser and a probability distribution.\n","•We exploit free generators to to transport an idea from\n","formal languages—the Brzozowski derivative —to the\n","context of generators (§4).\n","•To illustrate the potential applications of these formal\n","results, we present an algorithm that uses derivatives\n","to turn a naïve generator into one that produces only\n","1arXiv:2203.00652v1  [cs.PL]  1 Mar 2022\n","\n","Data Provenance at Internet Scale:\n","Architecture, Experiences, and the Road Ahead\n","Ang Chen*\n","University of PennsylvaniaY ang Wu*\n","University of Pennsylvania\n","Andreas Haeberlen\n","University of PennsylvaniaBoon Thau Loo\n","University of PennsylvaniaWenchao Zhou\n","Georgetown University\n","ABSTRACT\n","Provenance is a way to answer “why” questions about computa-\n","tions. It has found a number of uses in the database community,\n","such as debugging query answers or tracing unexpected results to\n","database tuples. In fact, the ability to ask “why” can be useful\n","for a much broader range of applications. In this paper, we summa-\n","rize our experiences over the past few years in adapting provenance\n","for diagnostic and forensic uses in networks and distributed sys-\n","tems. Our work draws inspirations from database provenance, yet\n","the deployment scale, use cases, and distributed nature of networks\n","require a signiﬁcant re-design of traditional data provenance mod-\n","els. We review a number of use cases, ranging from investigating\n","intrusions to diagnosing (and even automatically ﬁxing) software-\n","deﬁned networks, and present a uniﬁed system architecture that we\n","have designed and implemented for provenance in distributed sys-\n","tems. We conclude with a discussion of open issues in this space.\n","1 Introduction\n","Provenance is, in essence, a way to answer “why” questions about\n","computations. It works by linking each effect, such as a computed\n","value, to its direct causes, such as the corresponding inputs and the\n","operation that was performed on them. Thus, it becomes possible\n","to recursively “explain” a particular result by showing its direct\n","causes, and then their own causes, and so on, until a set of basic\n","inputs is reached. This explanation is the provenance of the result:\n","it is a tree whose vertexes represent computations or data and whose\n","edges indicate direct causal connections.\n","Provenance originated in the database community [13], and it has\n","found a number of interesting uses, such as diagnosing query an-\n","swers [31], reverse data management [34], or tracking unexpected\n","results to speciﬁc tuples in the input data [32]. However, the con-\n","cept itself is not database-speciﬁc: it can potentially help in any\n","situation where a system has shown some unexpected behavior that\n","must now be investigated and tracked to a set of “root causes”. This\n","kind of situation is common in many areas of computer science.\n","Over the past several years, we have been working on network\n","provenance , which is a general class of solutions that adapt prove-\n","nance for diagnostic and forensic uses in computer networks and,\n","more generally, distributed systems. As with database applications,\n","*These authors contributed equally to this work.\n","This article is published under a Creative Commons Attribution License\n","(http://creativecommons.org/licenses/by/3.0/), which permits distribution\n","and reproduction in any medium as well allowing derivative works, pro-\n","vided that you attribute the original work to the author(s) and CIDR 2017.\n","8th Biennial Conference on Innovative Data Systems Research (CIDR ‘17)\n","January 8-11, 2017, Chaminade, California, USA.these systems frequently do something that their operators did not\n","expect. In the context of database applications, a SQL query may\n","have been written incorrectly, resulting in erroneous query results.\n","Likewise, in computer networks, a network operator may observe\n","unusual routes or dropped packets, which may be symptoms of er-\n","rors in network conﬁgurations, or worse still, bugs introduced by\n","intentional manipulation and even targeted attacks. And, just as\n","in the database world, the operators are faced with the challenging\n","problem of tracing the observed symptoms to a set of root causes.\n","One way to apply provenance to distributed systems is to essen-\n","tially model the distributed system as a giant database: the state of\n","each node can be stored in tables, and the programs can be mod-\n","eled as a set of declarative rules [29]. The provenance of each tuple\n","can then be tracked just like it would be in a database, and diag-\n","nostic queries (say, “Why is the network using route R to reach\n","host H?”) can be formulated as provenance queries (say, “What is\n","the provenance of route R?”), which then reveal the corresponding\n","root causes (say, a recent conﬁguration change).\n","Given the distributed nature of networks, we can partition and\n","distribute the state such that each node maintains only a portion\n","of the information required to reconstruct any tuple provenance,\n","rather than storing all the network state in a centralized database.\n","This was the approach we took in our ﬁrst solution [51], but it\n","quickly became clear that there were a number of additional chal-\n","lenges. One simple example is the fact that network state, unlike\n","tuples in traditional databases, tends to be short-lived: by the time\n","that the operator is notiﬁed of the problem with route R, that route\n","may already have been replaced by another. We solved this by\n","adding a temporal dimension to the provenance, so that questions\n","could be asked about past states [50]; however, this massively in-\n","creased the amount of metadata that needed to be kept, so we added\n","garbage collection and a range of strong compression techniques,\n","along with a cost model to choose the “right” technique for a given\n","system [50].\n","Other challenges were more fundamental, however. For instance,\n","one interesting application area is security. Given that the prove-\n","nance graph in our setting is stored in a distributed and open fash-\n","ion across administrative domains, the entire system is vulnerable\n","to attacks. The operator might wish to learn how an attacker “got\n","into” the system, or what changes she has made to it. However, if\n","the attacker has compromised the system, what prevents her from\n","tampering with the provenance and giving false or misleading re-\n","sponses? In another class of scenarios in network debugging, the\n","problem is not the presence of an unexpected event, but rather the\n","absence of an expected event; here, it is not immediately clear how\n","to even apply provenance, since there is no starting point for a pos-\n","sible explanation. Our solutions to both problems involve new data\n","structures, as well as substantial re-designs and reﬁnements of the\n","1\n","\n","Data Intimacy, Machine Learning, and Consumer Privacy  \n"," \n","Prof. Michael Kearns1 \n","University of Pennsylvania  \n"," \n","Summary. We discuss and analyze the data sources and practices at large consumer -facing \n","technology companies such as Google and Facebook, and examine the central role of machine \n","learning and artificial intelligence at such companies. We focus in particular on the notion of \n","data intimacy --- the fa ct that machine learning enables  companies to routinely draw accurate \n","predictions and inferences abo ut users that go far deeper than what is merely on the “surface” \n","of the data collected. We discuss the consequences  for consumer privacy, and briefly discuss \n","broad implications for policy and regulation.  \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","                                                           \n","1 The author would like to thank AT&T for their support of this work. All analyses, exposition and opinions are \n","exclusively those of the author.:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Renee Vidal's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Oleg Sokolsky_sokolsky_bio.pdf\n","................................................................................\n","Content: \n","Current research Past projects Publications Community service Education Oleg Sokolsky Research Professor\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Fouh_AVpedagogypost.pdf\n","................................................................................\n","Content: \n","The Role of Visualization in Computer Science Education       ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER\n","Department of Computer Science   Virginia Tech   Blacksburg, VA 24061       Computer Science core instruction attempts\n","to provide a detailed understanding o f dy- namic processes such as the working of an algorithm or the flow of\n","information between  computing ent ities. Such dynamic processes are not well explained by static media such  as text\n","and images, and are difficult to convey in le cture. We survey the hist ory of visual i- zation in CS education, foc\n","using on artifacts that have  a documented positive educational  assessment. We discuss how changes in computing\n","technology have affected the deve l- opment and uptake of such visualization artifacts in CS education, and how recent\n","tech- nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization,\n","data structure visualization, program visualiz a- tion, eTextbooks, hypertextbooks.      (Draft of paper to appear in\n","Computers in the Schools , 2012)\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Mayur Naik_vmcai18.pdf\n","................................................................................\n","Content: \n","Maximum Satisﬁability in Program Analysis: Applications and Techniques Mayur Naik1, Xujie Si1, Xin Zhang1, and Radu\n","Grigore2 1University of Pennsylvania2University of Kent Abstract. A central challenge in program analysis concerns\n","balanc- ing diﬀerent competing tradeoﬀs. To address this challenge, we propose an approach based on the Maximum\n","Satisﬁability (MaxSAT) problem, an optimization extension of the Boolean Satisﬁability (SAT) problem. We demonstrate the\n","approach on three diverse applications that ad- vance the state-of-the-art in balancing tradeoﬀs in program analysis.\n","Enabling these applications on real-world programs necessitates solv- ing large MaxSAT instances comprising over\n","1030clauses in a sound and optimal manner. We propose a general framework that scales to such instances by iteratively\n","expanding a subset of clauses while providing soundness and optimality guarantees. We also present new techniques to\n","instantiate and optimize the framework. Keywords: maximum satisﬁability, program analysis\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Surbi Goel_2110.10090.pdf\n","................................................................................\n","Content: \n","Inductive Biases and Variable Creation in Self-Attention Mechanisms Benjamin L. Edelman1Surbhi Goel2Sham Kakade1,2Cyril\n","Zhang2 1Harvard University2Microsoft Research NYC bedelman@g.harvard.edu, {goel.surbhi, sham.kakade, cyrilzhang\n","}@microsoft.com Abstract Self-attention, an architectural motif designed to model long-range interactions in sequential\n","data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a\n","theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which\n","functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm\n","Transformer networks “create sparse variables”: a single self-attention head can represent a sparse function of the\n","input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we\n","present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers. 1\n","Introduction Self-attention mechanisms have comprised an era-deﬁning cornerstone of deep learning in recent years, ap-\n","pearing ubiquitously in empirical breakthroughs in generative sequence modeling and unsupervised represen- tation\n","learning. Starting with natural language (Vaswani et al., 2017), self-attention has enjoyed surprising empirical\n","successes in numerous and diverse modalities of data. In many of these settings, self-attention has supplanted\n","traditional recurrent and convolutional architectures, which are understood to incorporate in- ductive biases about\n","temporal and translational invariances in the data. Self-attention models discard these functional forms, in favor of\n","directly and globally modeling long-range interactions within the input context. The proliferation of self-attention\n","raises a fundamental question about its inductive biases: which func- tions do self-attention networks prefer to\n","represent? Various intuitions and empirics inform the design of these architectures, but formal statistical abstractions\n","and analyses are missing in this space. To this end, this work initiates an analysis of the statistical foundations of\n","self-attention. We identify an inductive bias for self-attention, for which we coin the term sparse variable creation :\n","a bounded-norm self-attention head learns a sparse function (which only depends on a small subset of input coordinates,\n","such as a constant-fan-in gate in a Boolean circuit) of a length- Tcontext, with sample complexity scaling as log( T).\n","The main technical novelty in this work is a covering number-based capacity bound for attention mechanisms (including\n","Transformer heads, as well as related and future architectures), implying norm-based generalization bounds. This is\n","accompanied by a matching representational result, showing that bounded-norm self-attention heads are indeed capable of\n","representing s-sparse functions with weight norms 2O(s)(or poly(s), for symmetric sparse functions). This provides a\n","theoretical account for why attention models can learn long-range dependencies without overﬁtting. Finally, we conduct\n","synthetic experiments to probe the sample eﬃciency of learning sparse interactions with self-attention. We train\n","Transformer models to identify sparse Boolean functions with randomly chosen indices, and corroborate the sample\n","complexity scaling law predicted by the theory. A variant of this experiment (with i.i.d. samples) reveals a\n","computational mystery, beyond the scope of our current statistical analysis: we ﬁnd that Transformers can successfully\n","learn the “hardest” (in the sense of SQ-dimension) s-sparse functions: the XOR (parity) functions. 1arXiv:2110.10090v2\n","[cs.LG]  24 Jun 2022\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Weingarten_weingarten_bio.pdf\n","................................................................................\n","Content: \n","erikwaing   Erik Waingarten   Publications   Teaching  Erik Waingarten\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Victor M. Preciado_Victor M. Preciado - Research Interests.pdf\n","................................................................................\n","Content: \n","11/8/23, 1:20 PMVictor M. Preciado - Research Interests https://sites.google.com/site/victormpreciado/research-11/7\n","Research Interests My primary research interests are modeling, analysis, and control of complex networked systems\n","andengineering infrastructures, with applications in social networks, technological infrastructure, andbiological\n","systems. Since critical societal functions are increasingly dependent on complex networks, it isimportant to acquire a\n","deeper understanding of the relationship between the structure and the dynamicsof these systems. Research lines of\n","particular interest are described below: Victor M. PreciadoHomeResearch InterestsList of PublicationsT\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Wong_wong_bio.pdf\n","................................................................................\n","Content: \n","Eric Wong   GROUP   BLOG   RESEARCH   TEACHING   PAPERS   ABOUT Eric Wong Assistant Professor , University of\n","Pennsylvania\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Benjamin C. Pierce_2203.00652.pdf\n","................................................................................\n","Content: \n","Parsing Randomness∗ Unifying and Differentiating Parsers and Random Generators Harrison Goldstein University of\n","Pennsylvania Philadelphia, PA, USA hgo@seas.upenn.eduBenjamin C. Pierce University of Pennsylvania Philadelphia, PA, USA\n","bcpierce@cis.upenn.edu Abstract “A generator is a parser of randomness. ” This perspective on generators for random data\n","structures is well established as folklore in the programming languages community, but it has apparently never been\n","formalized, nor have its consequences been deeply explored. We present free generators , which unify parsing and gener-\n","ation using a common structure that makes the relationship between the two concepts precise. Free generators lead natu-\n","rally to a proof that a large class of generators can be factored into a parser plus a distribution over choice\n","sequences. Fur- ther, free generators support a notion of derivative , analogous to familiar Brzozowski derivatives of\n","formal languages, that allows analysis tools to “preview” the effect of a particular generator choice. This, in turn,\n","gives rise to a novel algo- rithm for generating data structures satisfying user-specified preconditions. Keywords:\n","Random generation, Parsing, Property-based test- ing, Formal languages 1 Introduction “A generator is a parser of\n","randomness. . . ” It’s one of those ob- servations that’s totally puzzling right up to the moment it be- comes totally\n","obvious: a random generator—such as might be found in a property-based testing tool like QuickCheck [ 4]— is a\n","transformer from a series of random choices into a data structure, just as a parser transforms a series of characters\n","into a data structure. Although this connection may be obvious once it is pointed out, few actually think of generators\n","this way. Indeed, to our knowledge the framing of random generators as parsers has never been explored formally. But\n","this is a shame! The re- lationship between these fundamental concepts deserves a deeper look. A generator is a program\n","that builds a data structure by making a sequence of random choices—those choices are the key. A “traditional” generator\n","makes decisions using a stored source of randomness (e.g., a seed) that it consults and updates whenever it must make a\n","choice. Equivalently, if we like, we can pre-compute a list of choices and pass it in to the ∗This paper was originally\n","submitted to PLDI’22 and not accepted. The reviewers felt that the ideas were good, but that the presentation was\n","incomplete.generator, which gradually walks down the list whenever it needs to make random decisions. In this mode of\n","operation, the generator is effectively parsing the sequence of choices into a data structure! To connect generators and\n","parsers, we introduce a data structure called a free generator that can be interpreted as either a generator or as a\n","parser. Free generators have a rich theory; in particular, we can use them to prove that a subset of generator programs\n","can be factored into a parser and a distribution over sequences of choices. Besides clarifying folklore, free generators\n","admit transfor- mations that cannot be implemented for standard generators and parsers. A particularly exciting one is a\n","notion of de- rivative which modifies a generator by asking the question: “what would this generator look like after it\n","makes choice 𝑐?” The derivative gives a way of previewing a particular choice to determine how likely it is to lead us\n","to useful values. We use derivatives of free generators to tackle a well- known problem—we call it the valid generation\n","problem . The challenge is to generate a large number of random values that satisfy some validity condition. This\n","problem comes up often in property-based testing, where the validity condition is the precondition of some functional\n","specification. Since generator derivatives give a way of previewing the effects of a particular choice, we can use\n","gradients (derivatives with respect to a vector of choices) to preview all possible choices and pick a promising one.\n","This leads us to an elegant algo- rithm for turning a naïve free generator into one that only generates valid values. In\n","§2 below, we introduce the ideas behind free generators and the operations that can be defined on them. We then present\n","our main contributions: •We formalize the folklore analogy between parsers and generators using free generators , a\n","novel class of structures that make choices explicit and support syn- tactic transformations (§3). We use free\n","generators to prove that every “applicative” generator can factored into a parser and a probability distribution. •We\n","exploit free generators to to transport an idea from formal languages—the Brzozowski derivative —to the context of\n","generators (§4). •To illustrate the potential applications of these formal results, we present an algorithm that uses\n","derivatives to turn a naïve generator into one that produces only 1arXiv:2203.00652v1  [cs.PL]  1 Mar 2022\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Andreas Haeberlen_provenance-cidr2017.pdf\n","................................................................................\n","Content: \n","Data Provenance at Internet Scale: Architecture, Experiences, and the Road Ahead Ang Chen* University of PennsylvaniaY\n","ang Wu* University of Pennsylvania Andreas Haeberlen University of PennsylvaniaBoon Thau Loo University of\n","PennsylvaniaWenchao Zhou Georgetown University ABSTRACT Provenance is a way to answer “why” questions about computa-\n","tions. It has found a number of uses in the database community, such as debugging query answers or tracing unexpected\n","results to database tuples. In fact, the ability to ask “why” can be useful for a much broader range of applications. In\n","this paper, we summa- rize our experiences over the past few years in adapting provenance for diagnostic and forensic\n","uses in networks and distributed sys- tems. Our work draws inspirations from database provenance, yet the deployment\n","scale, use cases, and distributed nature of networks require a signiﬁcant re-design of traditional data provenance mod-\n","els. We review a number of use cases, ranging from investigating intrusions to diagnosing (and even automatically ﬁxing)\n","software- deﬁned networks, and present a uniﬁed system architecture that we have designed and implemented for provenance\n","in distributed sys- tems. We conclude with a discussion of open issues in this space. 1 Introduction Provenance is, in\n","essence, a way to answer “why” questions about computations. It works by linking each effect, such as a computed value,\n","to its direct causes, such as the corresponding inputs and the operation that was performed on them. Thus, it becomes\n","possible to recursively “explain” a particular result by showing its direct causes, and then their own causes, and so\n","on, until a set of basic inputs is reached. This explanation is the provenance of the result: it is a tree whose\n","vertexes represent computations or data and whose edges indicate direct causal connections. Provenance originated in the\n","database community [13], and it has found a number of interesting uses, such as diagnosing query an- swers [31], reverse\n","data management [34], or tracking unexpected results to speciﬁc tuples in the input data [32]. However, the con- cept\n","itself is not database-speciﬁc: it can potentially help in any situation where a system has shown some unexpected\n","behavior that must now be investigated and tracked to a set of “root causes”. This kind of situation is common in many\n","areas of computer science. Over the past several years, we have been working on network provenance , which is a general\n","class of solutions that adapt prove- nance for diagnostic and forensic uses in computer networks and, more generally,\n","distributed systems. As with database applications, *These authors contributed equally to this work. This article is\n","published under a Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits\n","distribution and reproduction in any medium as well allowing derivative works, pro- vided that you attribute the\n","original work to the author(s) and CIDR 2017. 8th Biennial Conference on Innovative Data Systems Research (CIDR ‘17)\n","January 8-11, 2017, Chaminade, California, USA.these systems frequently do something that their operators did not\n","expect. In the context of database applications, a SQL query may have been written incorrectly, resulting in erroneous\n","query results. Likewise, in computer networks, a network operator may observe unusual routes or dropped packets, which\n","may be symptoms of er- rors in network conﬁgurations, or worse still, bugs introduced by intentional manipulation and\n","even targeted attacks. And, just as in the database world, the operators are faced with the challenging problem of\n","tracing the observed symptoms to a set of root causes. One way to apply provenance to distributed systems is to essen-\n","tially model the distributed system as a giant database: the state of each node can be stored in tables, and the\n","programs can be mod- eled as a set of declarative rules [29]. The provenance of each tuple can then be tracked just like\n","it would be in a database, and diag- nostic queries (say, “Why is the network using route R to reach host H?”) can be\n","formulated as provenance queries (say, “What is the provenance of route R?”), which then reveal the corresponding root\n","causes (say, a recent conﬁguration change). Given the distributed nature of networks, we can partition and distribute\n","the state such that each node maintains only a portion of the information required to reconstruct any tuple provenance,\n","rather than storing all the network state in a centralized database. This was the approach we took in our ﬁrst solution\n","[51], but it quickly became clear that there were a number of additional chal- lenges. One simple example is the fact\n","that network state, unlike tuples in traditional databases, tends to be short-lived: by the time that the operator is\n","notiﬁed of the problem with route R, that route may already have been replaced by another. We solved this by adding a\n","temporal dimension to the provenance, so that questions could be asked about past states [50]; however, this massively\n","in- creased the amount of metadata that needed to be kept, so we added garbage collection and a range of strong\n","compression techniques, along with a cost model to choose the “right” technique for a given system [50]. Other\n","challenges were more fundamental, however. For instance, one interesting application area is security. Given that the\n","prove- nance graph in our setting is stored in a distributed and open fash- ion across administrative domains, the\n","entire system is vulnerable to attacks. The operator might wish to learn how an attacker “got into” the system, or what\n","changes she has made to it. However, if the attacker has compromised the system, what prevents her from tampering with\n","the provenance and giving false or misleading re- sponses? In another class of scenarios in network debugging, the\n","problem is not the presence of an unexpected event, but rather the absence of an expected event; here, it is not\n","immediately clear how to even apply provenance, since there is no starting point for a pos- sible explanation. Our\n","solutions to both problems involve new data structures, as well as substantial re-designs and reﬁnements of the 1\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Michael Kearns _kearns-finalpdf.pdf\n","................................................................................\n","Content: \n","Data Intimacy, Machine Learning, and Consumer Privacy     Prof. Michael Kearns1  University of Pennsylvania     Summary.\n","We discuss and analyze the data sources and practices at large consumer -facing  technology companies such as Google and\n","Facebook, and examine the central role of machine  learning and artificial intelligence at such companies. We focus in\n","particular on the notion of  data intimacy --- the fa ct that machine learning enables  companies to routinely draw\n","accurate  predictions and inferences abo ut users that go far deeper than what is merely on the “surface”  of the data\n","collected. We discuss the consequences  for consumer privacy, and briefly discuss  broad implications for policy and\n","regulation.                                                                                                 1 The author\n","would like to thank AT&T for their support of this work. All analyses, exposition and opinions are  exclusively those of\n","the author.\n","................................................................................\n","Response: [Data Intimacy, Machine Learning, Artificial Intelligence, Consumer Privacy, Data Sources, Data Practices, Technology\n","Companies, Google, Facebook, Predictions, Inferences, User Data, Surface Data, Policy, Regulation]\n","................................................................................\n","What are key features of Rakesh Vohra's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Rakesh Vohra's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","Talaria: A Framework for\n","Simulation of Permissioned\n","Blockchains for Logistics and\n","Beyond\n","Jiali Xing\n","University of Pennsylvania\n","David Fischer\n","Duke University\n","Nitya Labh\n","The College of William and Mary\n","Ryan Piersma\n","Duke University\n","Benjamin C. Lee\n","University of Pennsylvania\n","Yu Amy Xia\n","The College of William and Mary\n","Tuhin Sahai\n","Raytheon Technologies Research Center\n","Vahid Tarokh\n","Duke University\n","Abstract —In this paper, we present Talaria, a novel permissioned blockchain simulator that\n","supports numerous protocols and use cases, most notably in supply chain management. Talaria\n","extends the capability of BlockSim, an existing blockchain simulator, to include permissioned\n","blockchains and serves as a foundation for further private blockchain assessment. Talaria is\n","designed with both practical Byzantine Fault Tolerance (pBFT) and simpliﬁed version of\n","Proof-of-Authority consensus protocols, but can be revised to include other permissioned\n","protocols within its modular framework. Moreover, Talaria is able to simulate different types of\n","malicious authorities and a variable daily transaction load at each node. In using Talaria,\n","business practitioners and policy planners have an opportunity to measure, evaluate, and adapt\n","a range of blockchain solutions for commercial operations.\n","IT Professional Published by the IEEE Computer Society © 2021 IEEE1arXiv:2103.02260v2  [cs.CR]  31 Mar 2021\n","\n","PDF hosted at the Radboud Repository of the Radboud University\n","Nijmegen \n"," \n"," \n"," \n"," \n","The following full text is a publisher's version. \n"," \n"," \n","For additional information about this publication click this link.\n","http://hdl.handle.net/2066/96888\n"," \n"," \n"," \n","Please be advised that this information was generated on 2023-11-20 and may be subject to\n","change.\n","\n","Auditing Algorithms\n","Understanding Algorithmic Systems\n","from the Outside In\n","Full text available at: http://dx.doi.org/10.1561/1100000083\n","\n","Using Replication and Partitioning to Build Secure \n","Distributed Systems\n","Citation\n","Stephen Chong. Using Replication and Partitioning to Build Secure Distributed Systems. \n","Proceedings of the 2003 IEEE Symposium on Security and Privacy, Berkeley, CA, USA, 11-14 May \n","2003, 236-250.\n","Permanent link\n","http://nrs.harvard.edu/urn-3:HUL.InstRepos:42663148\n","Terms of Use\n","This article was downloaded from Harvard University’s DASH repository, and is made available \n","under the terms and conditions applicable to Open Access Policy Articles, as set forth at http://\n","nrs.harvard.edu/urn-3:HUL.InstRepos:dash.current.terms-of-use#OAP\n","Share Your Story\n","The Harvard community has made this article openly available.\n","Please share how this access benefits you.  Submit a story .\n","Accessibility\n","\n","An Algorithmic Framework for Bias Bounties\n","Ira Globus-Harris1,2, Michael Kearns1,2, and Aaron Roth1,2\n","1University of Pennsylvania\n","2Amazon AWS AI\n","May 11, 2022\n","Abstract\n","We propose and analyze an algorithmic framework for “bias bounties” — events in which external\n","participants are invited to propose improvements to a trained model, akin to bug bounty events in soft-\n","ware and security. Our framework allows participants to submit arbitrary subgroup improvements, which\n","are then algorithmically incorporated into an updated model. Our algorithm has the property that there\n","is no tension between overall and subgroup accuracies, nor between diﬀerent subgroup accuracies, and it\n","enjoys provable convergence to either the Bayes optimal model or a state in which no further improve-\n","ments can be found by the participants. We provide formal analyses of our framework, experimental\n","evaluation, and ﬁndings from a preliminary bias bounty event.1\n","1The most up-to-date version of this paper may be found at https://arxiv.org/abs/2201.10408arXiv:2201.10408v4  [cs.LG]  9 May 2022\n","\n","The Role of Visualization in Computer Science Education  \n"," \n"," \n","ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER  \n"," \n","Department of Computer Science  \n","Virginia Tech  \n","Blacksburg, VA 24061  \n"," \n"," \n","Computer Science core instruction attempts to provide a detailed understanding o f dy-\n","namic processes such as the working of an algorithm or the flow of information between \n","computing ent ities. Such dynamic processes are not well explained by static media such \n","as text and images, and are difficult to convey in le cture. We survey the hist ory of visual i-\n","zation in CS education, foc using on artifacts that have  a documented positive educational \n","assessment. We discuss how changes in computing technology have affected the deve l-\n","opment and uptake of such visualization artifacts in CS education, and how recent tech-\n","nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization, data structure visualization, program visualiz a-\n","tion, eTextbooks, hypertextbooks.  \n","  \n","(Draft of paper to appear in Computers in the Schools , 2012)\n","\n","The mapping between syntactic and prosodic phrasing in English and\n","Mandarin\n","Jianjing Kuang1, May Pik Yu Chan1, Nari Rhee1, Mark Liberman1, Hongwei Ding2\n","1Department of Linguistics, University of Pennsylvania, USA\n","2Speech-Language-Hearing Center, Shanghai Jiao Tong University, China\n","kuangj@ling.upenn.edu, pikyu@sas.upenn.edu, nrhee@sas.upenn.edu, myl@cis.upenn.edu,\n","hwding@sjtu.edu.cn\n","Abstract\n","To achieve a better understanding of the relationship between\n","syntactic parsing and prosodic phrasing in speech production\n","cross-linguistically, we investigated how syntactic constituents\n","map onto a high-dimensional acoustic space of prosodic phras-\n","ing in two read-speech corpora of Mandarin and English with\n","syntactic annotations. The left and right edges of the con-\n","stituents from the syntactic parsings were used as a proxy for the\n","relative strength of the syntactic boundaries. A wide range of\n","acoustic cues capturing pauses, duration cues, F0, energy, and\n","voice quality cues were extracted. Our results showed that there\n","is a clear correlation between the strength of syntactic bound-\n","ary and prosodic phrasing, and the syntax-prosody mapping is\n","much stronger for the right boundaries than for the left bound-\n","aries. Moreover, the prosodic realization of syntactic bound-\n","aries is gradient (especially for right boundaries), and acous-\n","tic cues scale up or down collectively to indicate different ex-\n","tents of phrasing, rather than being specific to certain levels of\n","phrasing. We discuss the findings’ implications in relation to\n","the prosodic hierarchy and the nature of the prosody-syntax in-\n","terface.\n","Index Terms : Prosodic phrasing, Prosody-Syntax interface,\n","syntactic parsing\n","1. Introduction\n","Prosody plays important roles in speech parsing and under-\n","standing. For human speakers, prosody helps with locating the\n","major syntactic boundaries [1, 2, 3], resolving syntactic ambi-\n","guity [4, 5], and syntactic bootstrapping during language acqui-\n","sition [6]. For speech technology, prosody helps improve the\n","accuracy in parsing [7]. Even though a close link between syn-\n","tactic boundaries and prosodic boundaries has been recognized\n","in prosodic theories [8, 9, 10, 11, 12], the nature of the mapping\n","between prosodic boundaries and syntactic boundaries remains\n","an unsettled question. In particular, the mapping between syn-\n","tax and prosody appears to be non-isomorphic, and mismatches\n","between syntax and prosody are very common. This challenge\n","is related to the representation of prosodic structure and syntac-\n","tic structure.\n","On the one hand, traditionally, prosodic phrasing hierarchy\n","has been defined as discrete categories [13, 14]. Since many\n","acoustic cues (e.g., duration) of prosodic boundaries are rather\n","gradient in nature, it is often up to debate how many phras-\n","ing levels are in the hierarchical structure and how each level is\n","defined acoustically. Both Mainstream American English and\n","Standard Mandarin are considered to have at least three levels\n","of prosodic phrasing: prosodic word, intermediate phrase (ip),\n","and intonational phrase (IP) [13] (Mandarin is proposed to have\n","two levels of intermediate phrases [15]), where each level ofprosodic phrasing is defined with specific boundary cues. For\n","example, in English, prosodic words are expected to have no fi-\n","nal lengthening and no pause. Between the two higher prosodic\n","hierarchies, IP is primarily associated with pause, and ip is\n","mostly associated with final lengthening. However, studies with\n","non-expert annotators suggest the agreement for boundary de-\n","tection is not high, especially for intermediate levels of phrasing\n","[16].\n","On the other hand, syntactic phrasal structures are often\n","modelled via a tree structure of branching XP’s (X-Phrases),\n","where X stands for the syntactic category of the head compo-\n","nent (e.g. Noun, Preposition, Verb, Complementizer, etc.), and\n","are thus defined as a hierarchy of categories. However, due to\n","the recursiveness and compositionality of syntactic phrases, an\n","XP of a certain syntactic category can be found at both higher\n","and lower levels of the syntactic tree. Conceivably, the syn-\n","tactic categories of the phrases do not have a good correlation\n","with prosodic boundaries. Indeed, as noted by [17] and [18],\n","almost any syntactic edge is a potential location for a prosodic\n","phrase boundary. Due to the recursiveness, a better proxy for\n","measuring the strength of the syntactic boundary is the depth of\n","the constituency trees (i.e. whether the boundary is located at a\n","major branch split).\n","Moreover, the syntax-prosody mapping is likely to be\n","language-specific. At the syntax level, [10, 19] proposed that\n","languages differ parametrically as to whether the left or right\n","edges (or both) of syntactic constituents are aligned with the\n","prosodic domains [10, 19]. For example, Japanese aligns to the\n","left boundary, while English aligns to the right. Empirically,\n","[16] indeed found that English listeners are more consistent in\n","detecting the right boundaries. At the acoustic level, it is also\n","well-known that the acoustic encoding of prosodic boundaries\n","is highly variable across languages.\n","Overall, there has been much theoretical discussion on the\n","nature of syntax-prosody mapping, but empirical studies on nat-\n","uralistic speech that directly address this issue are still very\n","few. Most studies were done based on a small number of sen-\n","tences, and very few production studies based on large-scale\n","continuous speech corpora have been done, because of the lack\n","of syntactically-parsed speech corpora. In this study, we built\n","two such corpora for English and Mandarin, and extracted the\n","acoustic and syntactic features for both languages. By explor-\n","ing how syntactic phrasing is mapping onto a high-dimensional\n","acoustic space of prosodic phrasing, we will provide valuable\n","empirical insight into the nature of syntax-prosody mapping.\n","Specifically, we test 1) whether there is a reliable correlation\n","between syntactic phrasing and prosodic phrasing; 2) whether\n","there are discrete hierarchies in the mapping between syntax\n","and prosody; 3) whether there is any boundary-alignment pref-\n","erence in the syntax-prosody mapping.Interspeech 2022\n","18-22 September 2022, Incheon, Korea\n","Copyright ©2022 ISCA 3443 10.21437/Interspeech.2022-10726\n","\n","MIT Open Access Articles\n","Threshold Cryptography as a Service \n","(in the Multiserver and YOSO Models)\n","The MIT Faculty has made this article openly available. Please share\n","how this access benefits you. Your story matters.\n","Citation: Benhamouda, Fabrice, Halevi, Shai, Krawczyk, Hugo, Miao, Alex and Rabin, Tal. 2022. \n","\"Threshold Cryptography as a Service (in the Multiserver and YOSO Models).\"\n","As Published: https://doi.org/10.1145/3548606.3559397\n","Publisher: ACM|Proceedings of the 2022 ACM SIGSAC Conference on Computer and \n","Communications Security\n","Persistent URL: https://hdl.handle.net/1721.1/147692\n","Version: Final published version: final published article, as it appeared in a journal, conference \n","proceedings, or other formally published context\n","Terms of Use: Article is made available in accordance with the publisher's policy and may be \n","subject to US copyright law. Please refer to the publisher's site for terms of use.\n","\n","AUTOMATIC RECOGNITION OF SUPRASEGMENTALS IN SPEECH\n","Jiahong Yuan1, Neville Ryant2, Xingyu Cai1, Kenneth Church1, Mark Liberman2\n","1Baidu Research USA\n","2Linguistic Data Consortium, University of Pennsylvania\n","ABSTRACT\n","This study reports our efforts to improve automatic recog-\n","nition of suprasegmentals by ﬁne-tuning wav2vec 2.0 with\n","CTC, a method that has been successful in automatic speech\n","recognition. We demonstrate that the method can improve the\n","state-of-the-art on automatic recognition of syllables, tones,\n","and pitch accents. Utilizing segmental information, by em-\n","ploying tonal ﬁnals or tonal syllables as recognition units,\n","can signiﬁcantly improve Mandarin tone recognition. Lan-\n","guage models are helpful when tonal syllables are used as\n","recognition units, but not helpful when tones are recognition\n","units. Finally, Mandarin tone recognition can beneﬁt from\n","English phoneme recognition by combining the two tasks in\n","ﬁne-tuning wav2vec 2.0.\n","Index Terms —Syllables, Mandarin Tones, Pitch Ac-\n","cents, wav2vec 2.0, Multitask\n","1. INTRODUCTION\n","Suprasegmentals are phonological units in speech that are\n","larger than segments (i.e., consonants and vowels), such as\n","syllables, lexical stress, tones, and intonation [1]. In this\n","study, we propose using wav2vec 2.0 [2] ﬁned-tuned with\n","a Connectionist Temporal Classiﬁcation (CTC) loss [3] for\n","automatic recognition of suprasegmentals, similar to the ap-\n","proach used by [2] for phoneme recognition on TIMIT [4].\n","Speech segments and suprasegmental units are different\n","not only in the domain of realization, but also in their features.\n","“Suprasegmental” has often been used to refer to the phonetic\n","features of suprasegmental units, and used interchangeably\n","with prosodic features. It is well accepted that suprasegmen-\n","tals are mainly distinguished by pitch, duration, and energy,\n","whereas segments are distinguished by spectral information.\n","There are studies, however, suggesting that spectral informa-\n","tion may be also important for suprasegmentals. For example,\n","[5] found that spectral balance is a reliable acoustic correlate\n","of lexical stress, and [6, 7] demonstrated that MFCCs outper-\n","form prosodic features for automatic recognition of Mandarin\n","tones.\n","With the advent of deep learning and end-to-end models,\n","feature engineering has been largely abandoned with feature\n","representations learned implicitly during training by the neu-ral network. In earlier work, these featured were learned in\n","a supervised fashion from paired audio and transcripts [8].\n","However, more recent work has focused on unsupervised\n","learning of representations using only audio [9, 10, 2], which\n","are then used by downstream tasks such as speech-to-text.\n","When compared to conventional acoustic features such as\n","MFCCs, these representations substantially lower the amount\n","of labeled data needed to train state-of-the-art speech-to-text\n","systems for English [2] and low-resource languages [11].\n","Similarly encouraging results have been demonstrate for\n","speaker recognition [9] and phone recognition [2]. However,\n","it is not yet clear how well these representations perform for\n","suprasegmental recognition, which requires the network to\n","learn prosodic instead of, or in addition to, spectral features\n","and representations.\n","We conducted experiments with ﬁne-tuning wav2vec 2.0\n","models using CTC for recognition of suprasegmentals, in-\n","cluding syllables, tones, and pitch accents. We also made an\n","effort to improve recognition of Mandarin tones by utilizing\n","segmental information. The main results of our study are as\n","follows:\n","1. We demonstrate that ﬁne-tuning wav2vec 2.0 with a\n","CTC loss can improve the state-of-the-art for automatic\n","recognition of suprasegmentals, including syllables,\n","tones, and pitch accents.\n","2. Utilizing segmental information, by employing tonal ﬁ-\n","nals or tonal syllables as recognition units, can signiﬁ-\n","cantly improve Mandarin tone recognition. Language\n","models are helpful when tonal syllables are used as\n","recognition units, but not helpful when tones are recog-\n","nition units.\n","3. Mandarin tone recognition beneﬁts from English phoneme\n","recognition by combing the two tasks in ﬁne-tuning\n","wav2vec 2.0.\n","2. RELATED WORK\n","2.1. wav2vec 2.0\n","Wav2vec 2.0 is a framework for self-supervised learning of\n","speech representations. The speech signal is processed by aarXiv:2108.01122v2  [cs.CL]  4 Aug 2021\n","\n","Using Forced Alignment for Phonetics Research  Jiahong Yuan*, Wei Lai, Chris Cieri, and Mark Liberman  University of Pennsylvania, Philadelphia, USA jiahong@ldc.upenn.edu, weilai@sas.upenn.edu, ccieri@ldc.upenn.edu, myl@ldc.upenn.edu   Abstract. Forced alignment has been at the core of speech recognition technology since the 1970s, and was first used in phonetics research in the 1990s. Progress in digital multimedia, networking and mass storage is creating enormous and growing volumes of transcribed speech, which forced alignment can turn into vast phonetic databases. However, speech science has so far taken relatively little advantage of this opportunity, because it requires tools and methods that are now difficult for most speech researchers to access, and are incompletely developed and tested for many applications. But these technologies are leading the study of human speech into a revolutionary new era: a movement from the study of small, private, and mostly artificial datasets to the analysis of published collections of natural speech that are thousands or even millions of times larger. In this chapter, we illustrate some of the ways that forced alignment can be used as a tool in speech science, and discuss directions for improvement.    Keywords: Forced alignment · Corpus phonetics · Phonetic segmentation   1 Introduction  In the last twenty-five years, an enormous and growing body of digital speech has become available: archived broadcasts of news reports, interviews, speeches, and debates; oral histories; court recordings; podcasts; audiobooks; and so on. A small fraction of this material – still comprising many thousands of hours – has been collected and published in the form of corpora for speech technology research. These very-large-scale bodies of data make it possible to use natural speech in developing and testing hypotheses across many types of individual, social, regional, temporal, and contextual variation, as well as across languages. However, in contrast to speech technology research, speech science has so far taken relatively little advantage of this opportunity. This is partly because most researchers lack the knowledge and skills required to access the needed tools and methods, and partly because the tools and methods themselves are incomplete and untested.      Given only digital audio, we can study the distribution of speech and silence segments, or of purely acoustic-phonetic features such as fundamental frequency. But for most kinds of speech science, we need to know which words were said when, and how they were pronounced – and this entails the availability of phonetic segmentation and transcription. Relatively few speech corpora come with such annotations, because manual phonetic segmentation is time-consuming, expensive and inconsistent, with much less than perfect inter-annotator agreement (Godfrey et al. 1992; Leung and Zue 1984; Cucchiarini 1993). Automatic phonetic segmentation is, therefore, necessary for corpus-based phonetics research.  Luckily, automatic phonetic segmentation is the essential result of forced alignment, a technique developed for training automatic speech recognition systems (Jelinek 1976) and for extracting acoustic units for speech synthesis systems  (Wightman and Talkin 1997).  This task normally requires two inputs: recorded audio and a conventional (orthographic) transcription. The transcribed words are mapped into a phone sequence or a lattice of possible phone sequences, by using a pronouncing dictionary and/or grapheme to phoneme rules. Phone boundaries are determined by comparing the observed speech signal and pre-trained, Hidden Markov Model (HMM) based acoustic models. Typically every phone in the acoustic models is represented as an HMM that consists of three left-to-right non-skipping states (as shown in Figure 1): the beginning (s1), middle (s2), and ending (s3) parts of the phone, plus empty start (s0) and end states (s4) for entering and exiting the phone. From the training data, an acoustic model (e.g., a Gaussian Mixture Model) is built for each state (except s0 and s4), as well as the transition probabilities between pairs of states (Figure 1). The speech signal is analyzed as a successive:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Rakesh Vohra's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Benjamin Lee_xing2021-talaria.pdf\n","................................................................................\n","Content: \n","Talaria: A Framework for Simulation of Permissioned Blockchains for Logistics and Beyond Jiali Xing University of\n","Pennsylvania David Fischer Duke University Nitya Labh The College of William and Mary Ryan Piersma Duke University\n","Benjamin C. Lee University of Pennsylvania Yu Amy Xia The College of William and Mary Tuhin Sahai Raytheon Technologies\n","Research Center Vahid Tarokh Duke University Abstract —In this paper, we present Talaria, a novel permissioned\n","blockchain simulator that supports numerous protocols and use cases, most notably in supply chain management. Talaria\n","extends the capability of BlockSim, an existing blockchain simulator, to include permissioned blockchains and serves as\n","a foundation for further private blockchain assessment. Talaria is designed with both practical Byzantine Fault\n","Tolerance (pBFT) and simpliﬁed version of Proof-of-Authority consensus protocols, but can be revised to include other\n","permissioned protocols within its modular framework. Moreover, Talaria is able to simulate different types of malicious\n","authorities and a variable daily transaction load at each node. In using Talaria, business practitioners and policy\n","planners have an opportunity to measure, evaluate, and adapt a range of blockchain solutions for commercial operations.\n","IT Professional Published by the IEEE Computer Society © 2021 IEEE1arXiv:2103.02260v2  [cs.CR]  31 Mar 2021\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/James Gee_96888.pdf\n","................................................................................\n","Content: \n","PDF hosted at the Radboud Repository of the Radboud University Nijmegen          The following full text is a\n","publisher's version.      For additional information about this publication click this link.\n","http://hdl.handle.net/2066/96888       Please be advised that this information was generated on 2023-11-20 and may be\n","subject to change.\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Danaë Metaxa_9781680839173-summary.pdf\n","................................................................................\n","Content: \n","Auditing Algorithms Understanding Algorithmic Systems from the Outside In Full text available at:\n","http://dx.doi.org/10.1561/1100000083\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Steven Zdancemic_Using_Replication_and_Partitioning_to_Build_Secure_Distributed_Systems.pdf\n","................................................................................\n","Content: \n","Using Replication and Partitioning to Build Secure  Distributed Systems Citation Stephen Chong. Using Replication and\n","Partitioning to Build Secure Distributed Systems.  Proceedings of the 2003 IEEE Symposium on Security and Privacy,\n","Berkeley, CA, USA, 11-14 May  2003, 236-250. Permanent link http://nrs.harvard.edu/urn-3:HUL.InstRepos:42663148 Terms of\n","Use This article was downloaded from Harvard University’s DASH repository, and is made available  under the terms and\n","conditions applicable to Open Access Policy Articles, as set forth at http://\n","nrs.harvard.edu/urn-3:HUL.InstRepos:dash.current.terms-of-use#OAP Share Your Story The Harvard community has made this\n","article openly available. Please share how this access benefits you.  Submit a story . Accessibility\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Aaron Roth_2201.10408.pdf\n","................................................................................\n","Content: \n","An Algorithmic Framework for Bias Bounties Ira Globus-Harris1,2, Michael Kearns1,2, and Aaron Roth1,2 1University of\n","Pennsylvania 2Amazon AWS AI May 11, 2022 Abstract We propose and analyze an algorithmic framework for “bias bounties” —\n","events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events\n","in soft- ware and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then\n","algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between\n","overall and subgroup accuracies, nor between diﬀerent subgroup accuracies, and it enjoys provable convergence to either\n","the Bayes optimal model or a state in which no further improve- ments can be found by the participants. We provide\n","formal analyses of our framework, experimental evaluation, and ﬁndings from a preliminary bias bounty event.1 1The most\n","up-to-date version of this paper may be found at https://arxiv.org/abs/2201.10408arXiv:2201.10408v4  [cs.LG]  9 May 2022\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Fouh_AVpedagogypost.pdf\n","................................................................................\n","Content: \n","The Role of Visualization in Computer Science Education       ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER\n","Department of Computer Science   Virginia Tech   Blacksburg, VA 24061       Computer Science core instruction attempts\n","to provide a detailed understanding o f dy- namic processes such as the working of an algorithm or the flow of\n","information between  computing ent ities. Such dynamic processes are not well explained by static media such  as text\n","and images, and are difficult to convey in le cture. We survey the hist ory of visual i- zation in CS education, foc\n","using on artifacts that have  a documented positive educational  assessment. We discuss how changes in computing\n","technology have affected the deve l- opment and uptake of such visualization artifacts in CS education, and how recent\n","tech- nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization,\n","data structure visualization, program visualiz a- tion, eTextbooks, hypertextbooks.      (Draft of paper to appear in\n","Computers in the Schools , 2012)\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Mark L. Liberman_kuang22b_interspeech.pdf\n","................................................................................\n","Content: \n","The mapping between syntactic and prosodic phrasing in English and Mandarin Jianjing Kuang1, May Pik Yu Chan1, Nari\n","Rhee1, Mark Liberman1, Hongwei Ding2 1Department of Linguistics, University of Pennsylvania, USA 2Speech-Language-\n","Hearing Center, Shanghai Jiao Tong University, China kuangj@ling.upenn.edu, pikyu@sas.upenn.edu, nrhee@sas.upenn.edu,\n","myl@cis.upenn.edu, hwding@sjtu.edu.cn Abstract To achieve a better understanding of the relationship between syntactic\n","parsing and prosodic phrasing in speech production cross-linguistically, we investigated how syntactic constituents map\n","onto a high-dimensional acoustic space of prosodic phras- ing in two read-speech corpora of Mandarin and English with\n","syntactic annotations. The left and right edges of the con- stituents from the syntactic parsings were used as a proxy\n","for the relative strength of the syntactic boundaries. A wide range of acoustic cues capturing pauses, duration cues,\n","F0, energy, and voice quality cues were extracted. Our results showed that there is a clear correlation between the\n","strength of syntactic bound- ary and prosodic phrasing, and the syntax-prosody mapping is much stronger for the right\n","boundaries than for the left bound- aries. Moreover, the prosodic realization of syntactic bound- aries is gradient\n","(especially for right boundaries), and acous- tic cues scale up or down collectively to indicate different ex- tents of\n","phrasing, rather than being specific to certain levels of phrasing. We discuss the findings’ implications in relation to\n","the prosodic hierarchy and the nature of the prosody-syntax in- terface. Index Terms : Prosodic phrasing, Prosody-Syntax\n","interface, syntactic parsing 1. Introduction Prosody plays important roles in speech parsing and under- standing. For\n","human speakers, prosody helps with locating the major syntactic boundaries [1, 2, 3], resolving syntactic ambi- guity\n","[4, 5], and syntactic bootstrapping during language acqui- sition [6]. For speech technology, prosody helps improve the\n","accuracy in parsing [7]. Even though a close link between syn- tactic boundaries and prosodic boundaries has been\n","recognized in prosodic theories [8, 9, 10, 11, 12], the nature of the mapping between prosodic boundaries and syntactic\n","boundaries remains an unsettled question. In particular, the mapping between syn- tax and prosody appears to be non-\n","isomorphic, and mismatches between syntax and prosody are very common. This challenge is related to the representation\n","of prosodic structure and syntac- tic structure. On the one hand, traditionally, prosodic phrasing hierarchy has been\n","defined as discrete categories [13, 14]. Since many acoustic cues (e.g., duration) of prosodic boundaries are rather\n","gradient in nature, it is often up to debate how many phras- ing levels are in the hierarchical structure and how each\n","level is defined acoustically. Both Mainstream American English and Standard Mandarin are considered to have at least\n","three levels of prosodic phrasing: prosodic word, intermediate phrase (ip), and intonational phrase (IP) [13] (Mandarin\n","is proposed to have two levels of intermediate phrases [15]), where each level ofprosodic phrasing is defined with\n","specific boundary cues. For example, in English, prosodic words are expected to have no fi- nal lengthening and no\n","pause. Between the two higher prosodic hierarchies, IP is primarily associated with pause, and ip is mostly associated\n","with final lengthening. However, studies with non-expert annotators suggest the agreement for boundary de- tection is\n","not high, especially for intermediate levels of phrasing [16]. On the other hand, syntactic phrasal structures are often\n","modelled via a tree structure of branching XP’s (X-Phrases), where X stands for the syntactic category of the head\n","compo- nent (e.g. Noun, Preposition, Verb, Complementizer, etc.), and are thus defined as a hierarchy of categories.\n","However, due to the recursiveness and compositionality of syntactic phrases, an XP of a certain syntactic category can\n","be found at both higher and lower levels of the syntactic tree. Conceivably, the syn- tactic categories of the phrases\n","do not have a good correlation with prosodic boundaries. Indeed, as noted by [17] and [18], almost any syntactic edge is\n","a potential location for a prosodic phrase boundary. Due to the recursiveness, a better proxy for measuring the strength\n","of the syntactic boundary is the depth of the constituency trees (i.e. whether the boundary is located at a major branch\n","split). Moreover, the syntax-prosody mapping is likely to be language-specific. At the syntax level, [10, 19] proposed\n","that languages differ parametrically as to whether the left or right edges (or both) of syntactic constituents are\n","aligned with the prosodic domains [10, 19]. For example, Japanese aligns to the left boundary, while English aligns to\n","the right. Empirically, [16] indeed found that English listeners are more consistent in detecting the right boundaries.\n","At the acoustic level, it is also well-known that the acoustic encoding of prosodic boundaries is highly variable across\n","languages. Overall, there has been much theoretical discussion on the nature of syntax-prosody mapping, but empirical\n","studies on nat- uralistic speech that directly address this issue are still very few. Most studies were done based on a\n","small number of sen- tences, and very few production studies based on large-scale continuous speech corpora have been\n","done, because of the lack of syntactically-parsed speech corpora. In this study, we built two such corpora for English\n","and Mandarin, and extracted the acoustic and syntactic features for both languages. By explor- ing how syntactic\n","phrasing is mapping onto a high-dimensional acoustic space of prosodic phrasing, we will provide valuable empirical\n","insight into the nature of syntax-prosody mapping. Specifically, we test 1) whether there is a reliable correlation\n","between syntactic phrasing and prosodic phrasing; 2) whether there are discrete hierarchies in the mapping between\n","syntax and prosody; 3) whether there is any boundary-alignment pref- erence in the syntax-prosody mapping.Interspeech\n","2022 18-22 September 2022, Incheon, Korea Copyright ©2022 ISCA 3443 10.21437/Interspeech.2022-10726\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Tal Rabin_3548606.3559397.pdf\n","................................................................................\n","Content: \n","MIT Open Access Articles Threshold Cryptography as a Service  (in the Multiserver and YOSO Models) The MIT Faculty has\n","made this article openly available. Please share how this access benefits you. Your story matters. Citation: Benhamouda,\n","Fabrice, Halevi, Shai, Krawczyk, Hugo, Miao, Alex and Rabin, Tal. 2022.  \"Threshold Cryptography as a Service (in the\n","Multiserver and YOSO Models).\" As Published: https://doi.org/10.1145/3548606.3559397 Publisher: ACM|Proceedings of the\n","2022 ACM SIGSAC Conference on Computer and  Communications Security Persistent URL: https://hdl.handle.net/1721.1/147692\n","Version: Final published version: final published article, as it appeared in a journal, conference  proceedings, or\n","other formally published context Terms of Use: Article is made available in accordance with the publisher's policy and\n","may be  subject to US copyright law. Please refer to the publisher's site for terms of use.\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Mark L. Liberman_2108.01122.pdf\n","................................................................................\n","Content: \n","AUTOMATIC RECOGNITION OF SUPRASEGMENTALS IN SPEECH Jiahong Yuan1, Neville Ryant2, Xingyu Cai1, Kenneth Church1, Mark\n","Liberman2 1Baidu Research USA 2Linguistic Data Consortium, University of Pennsylvania ABSTRACT This study reports our\n","efforts to improve automatic recog- nition of suprasegmentals by ﬁne-tuning wav2vec 2.0 with CTC, a method that has been\n","successful in automatic speech recognition. We demonstrate that the method can improve the state-of-the-art on automatic\n","recognition of syllables, tones, and pitch accents. Utilizing segmental information, by em- ploying tonal ﬁnals or tonal\n","syllables as recognition units, can signiﬁcantly improve Mandarin tone recognition. Lan- guage models are helpful when\n","tonal syllables are used as recognition units, but not helpful when tones are recognition units. Finally, Mandarin tone\n","recognition can beneﬁt from English phoneme recognition by combining the two tasks in ﬁne-tuning wav2vec 2.0. Index\n","Terms —Syllables, Mandarin Tones, Pitch Ac- cents, wav2vec 2.0, Multitask 1. INTRODUCTION Suprasegmentals are\n","phonological units in speech that are larger than segments (i.e., consonants and vowels), such as syllables, lexical\n","stress, tones, and intonation [1]. In this study, we propose using wav2vec 2.0 [2] ﬁned-tuned with a Connectionist\n","Temporal Classiﬁcation (CTC) loss [3] for automatic recognition of suprasegmentals, similar to the ap- proach used by\n","[2] for phoneme recognition on TIMIT [4]. Speech segments and suprasegmental units are different not only in the domain\n","of realization, but also in their features. “Suprasegmental” has often been used to refer to the phonetic features of\n","suprasegmental units, and used interchangeably with prosodic features. It is well accepted that suprasegmen- tals are\n","mainly distinguished by pitch, duration, and energy, whereas segments are distinguished by spectral information. There\n","are studies, however, suggesting that spectral informa- tion may be also important for suprasegmentals. For example, [5]\n","found that spectral balance is a reliable acoustic correlate of lexical stress, and [6, 7] demonstrated that MFCCs\n","outper- form prosodic features for automatic recognition of Mandarin tones. With the advent of deep learning and end-to-\n","end models, feature engineering has been largely abandoned with feature representations learned implicitly during\n","training by the neu-ral network. In earlier work, these featured were learned in a supervised fashion from paired audio\n","and transcripts [8]. However, more recent work has focused on unsupervised learning of representations using only audio\n","[9, 10, 2], which are then used by downstream tasks such as speech-to-text. When compared to conventional acoustic\n","features such as MFCCs, these representations substantially lower the amount of labeled data needed to train state-of-\n","the-art speech-to-text systems for English [2] and low-resource languages [11]. Similarly encouraging results have been\n","demonstrate for speaker recognition [9] and phone recognition [2]. However, it is not yet clear how well these\n","representations perform for suprasegmental recognition, which requires the network to learn prosodic instead of, or in\n","addition to, spectral features and representations. We conducted experiments with ﬁne-tuning wav2vec 2.0 models using\n","CTC for recognition of suprasegmentals, in- cluding syllables, tones, and pitch accents. We also made an effort to\n","improve recognition of Mandarin tones by utilizing segmental information. The main results of our study are as follows:\n","1. We demonstrate that ﬁne-tuning wav2vec 2.0 with a CTC loss can improve the state-of-the-art for automatic recognition\n","of suprasegmentals, including syllables, tones, and pitch accents. 2. Utilizing segmental information, by employing\n","tonal ﬁ- nals or tonal syllables as recognition units, can signiﬁ- cantly improve Mandarin tone recognition. Language\n","models are helpful when tonal syllables are used as recognition units, but not helpful when tones are recog- nition\n","units. 3. Mandarin tone recognition beneﬁts from English phoneme recognition by combing the two tasks in ﬁne-tuning\n","wav2vec 2.0. 2. RELATED WORK 2.1. wav2vec 2.0 Wav2vec 2.0 is a framework for self-supervised learning of speech\n","representations. The speech signal is processed by aarXiv:2108.01122v2  [cs.CL]  4 Aug 2021\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Mark L. Liberman_ForcedAlignment_Final_edited.pdf\n","................................................................................\n","Content: \n","Using Forced Alignment for Phonetics Research  Jiahong Yuan*, Wei Lai, Chris Cieri, and Mark Liberman  University of\n","Pennsylvania, Philadelphia, USA jiahong@ldc.upenn.edu, weilai@sas.upenn.edu, ccieri@ldc.upenn.edu, myl@ldc.upenn.edu\n","Abstract. Forced alignment has been at the core of speech recognition technology since the 1970s, and was first used in\n","phonetics research in the 1990s. Progress in digital multimedia, networking and mass storage is creating enormous and\n","growing volumes of transcribed speech, which forced alignment can turn into vast phonetic databases. However, speech\n","science has so far taken relatively little advantage of this opportunity, because it requires tools and methods that are\n","now difficult for most speech researchers to access, and are incompletely developed and tested for many applications.\n","But these technologies are leading the study of human speech into a revolutionary new era: a movement from the study of\n","small, private, and mostly artificial datasets to the analysis of published collections of natural speech that are\n","thousands or even millions of times larger. In this chapter, we illustrate some of the ways that forced alignment can be\n","used as a tool in speech science, and discuss directions for improvement.    Keywords: Forced alignment · Corpus\n","phonetics · Phonetic segmentation   1 Introduction  In the last twenty-five years, an enormous and growing body of\n","digital speech has become available: archived broadcasts of news reports, interviews, speeches, and debates; oral\n","histories; court recordings; podcasts; audiobooks; and so on. A small fraction of this material – still comprising many\n","thousands of hours – has been collected and published in the form of corpora for speech technology research. These very-\n","large-scale bodies of data make it possible to use natural speech in developing and testing hypotheses across many types\n","of individual, social, regional, temporal, and contextual variation, as well as across languages. However, in contrast\n","to speech technology research, speech science has so far taken relatively little advantage of this opportunity. This is\n","partly because most researchers lack the knowledge and skills required to access the needed tools and methods, and\n","partly because the tools and methods themselves are incomplete and untested.      Given only digital audio, we can study\n","the distribution of speech and silence segments, or of purely acoustic-phonetic features such as fundamental frequency.\n","But for most kinds of speech science, we need to know which words were said when, and how they were pronounced – and\n","this entails the availability of phonetic segmentation and transcription. Relatively few speech corpora come with such\n","annotations, because manual phonetic segmentation is time-consuming, expensive and inconsistent, with much less than\n","perfect inter-annotator agreement (Godfrey et al. 1992; Leung and Zue 1984; Cucchiarini 1993). Automatic phonetic\n","segmentation is, therefore, necessary for corpus-based phonetics research.  Luckily, automatic phonetic segmentation is\n","the essential result of forced alignment, a technique developed for training automatic speech recognition systems\n","(Jelinek 1976) and for extracting acoustic units for speech synthesis systems  (Wightman and Talkin 1997).  This task\n","normally requires two inputs: recorded audio and a conventional (orthographic) transcription. The transcribed words are\n","mapped into a phone sequence or a lattice of possible phone sequences, by using a pronouncing dictionary and/or grapheme\n","to phoneme rules. Phone boundaries are determined by comparing the observed speech signal and pre-trained, Hidden Markov\n","Model (HMM) based acoustic models. Typically every phone in the acoustic models is represented as an HMM that consists\n","of three left-to-right non-skipping states (as shown in Figure 1): the beginning (s1), middle (s2), and ending (s3)\n","parts of the phone, plus empty start (s0) and end states (s4) for entering and exiting the phone. From the training\n","data, an acoustic model (e.g., a Gaussian Mixture Model) is built for each state (except s0 and s4), as well as the\n","transition probabilities between pairs of states (Figure 1). The speech signal is analyzed as a successive\n","................................................................................\n","Response: - Forced Alignment - Corpus phonetics - Phonetic segmentation - Digital speech - Phonetics research - Phonetic features\n","- Speech recognition - Acoustic models - Hidden Markov Models (HMM) - Gaussian Mixture Model (GMM) - Automatic phonetic\n","segmentation - Orthographic transcription - Pronouncing dictionary - Grapheme to phoneme rules - Phone boundaries -\n","Acoustic-phonetic features - Fundamental frequency - Speech synthesis systems - Inter-annotator agreement - Speech\n","corpora - Pronunciation variation\n","................................................................................\n","What are key features of Scott Weinstein's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Scott Weinstein's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","erikwaing\n"," \n","Erik\n","Waingarten\n"," \n","Publications\n"," \n","Teaching \n","Erik\n","Waingarten\n","\n","Inductive Biases and Variable Creation\n","in Self-Attention Mechanisms\n","Benjamin L. Edelman1Surbhi Goel2Sham Kakade1,2Cyril Zhang2\n","1Harvard University2Microsoft Research NYC\n","bedelman@g.harvard.edu, {goel.surbhi, sham.kakade, cyrilzhang }@microsoft.com\n","Abstract\n","Self-attention, an architectural motif designed to model long-range interactions in sequential data, has\n","driven numerous recent breakthroughs in natural language processing and beyond. This work provides a\n","theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish\n","which functions and long-range dependencies self-attention blocks prefer to represent. Our main result\n","shows that bounded-norm Transformer networks “create sparse variables”: a single self-attention head\n","can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically\n","with the context length. To support our analysis, we present synthetic experiments to probe the sample\n","complexity of learning sparse Boolean functions with Transformers.\n","1 Introduction\n","Self-attention mechanisms have comprised an era-deﬁning cornerstone of deep learning in recent years, ap-\n","pearing ubiquitously in empirical breakthroughs in generative sequence modeling and unsupervised represen-\n","tation learning. Starting with natural language (Vaswani et al., 2017), self-attention has enjoyed surprising\n","empirical successes in numerous and diverse modalities of data. In many of these settings, self-attention has\n","supplanted traditional recurrent and convolutional architectures, which are understood to incorporate in-\n","ductive biases about temporal and translational invariances in the data. Self-attention models discard these\n","functional forms, in favor of directly and globally modeling long-range interactions within the input context.\n","The proliferation of self-attention raises a fundamental question about its inductive biases: which func-\n","tions do self-attention networks prefer to represent? Various intuitions and empirics inform the design of\n","these architectures, but formal statistical abstractions and analyses are missing in this space. To this end,\n","this work initiates an analysis of the statistical foundations of self-attention.\n","We identify an inductive bias for self-attention, for which we coin the term sparse variable creation : a\n","bounded-norm self-attention head learns a sparse function (which only depends on a small subset of input\n","coordinates, such as a constant-fan-in gate in a Boolean circuit) of a length- Tcontext, with sample complexity\n","scaling as log( T). The main technical novelty in this work is a covering number-based capacity bound for\n","attention mechanisms (including Transformer heads, as well as related and future architectures), implying\n","norm-based generalization bounds. This is accompanied by a matching representational result, showing that\n","bounded-norm self-attention heads are indeed capable of representing s-sparse functions with weight norms\n","2O(s)(or poly(s), for symmetric sparse functions). This provides a theoretical account for why attention\n","models can learn long-range dependencies without overﬁtting.\n","Finally, we conduct synthetic experiments to probe the sample eﬃciency of learning sparse interactions\n","with self-attention. We train Transformer models to identify sparse Boolean functions with randomly chosen\n","indices, and corroborate the sample complexity scaling law predicted by the theory. A variant of this\n","experiment (with i.i.d. samples) reveals a computational mystery, beyond the scope of our current statistical\n","analysis: we ﬁnd that Transformers can successfully learn the “hardest” (in the sense of SQ-dimension)\n","s-sparse functions: the XOR (parity) functions.\n","1arXiv:2110.10090v2  [cs.LG]  24 Jun 2022\n","\n","This is a free offprint provided to the author by the publisher. Copyright restrictions may apply.TRANSACTIONS OF THE\n","AMERICAN MATHEMATICAL SOCIETY\n","Volume 369, Number 12, December 2017, Pages 8743–8764\n","http://dx.doi.org/10.1090/tran/6929Article electronically published on June 27, 2017\n","ZEROS OF A RANDOM ANALYTIC FUNCTION APPROACH\n","PERFECT SPACING UNDER REPEATED DIFFERENTIATION\n","ROBIN PEMANTLE AND SNEHA SUBRAMANIAN\n","Abstract. We consider an analytic function whose zero set forms a unit in-\n","tensity Poisson process on the real line. We show that repeated diﬀerentiationcauses the zero set to converge in distribution to a random translate of theintegers.\n","1.Introduction\n","Study of the relation of the zero set of a function fto the zero set of its derivative\n","has a rich history. The Gauss-Lucas theorem (see, e.g., [Mar49, Theorem 6.1]) says\n","that iffis a polynomial, then the zero set of f′lies in the convex hull of the zero\n","set off. Another property of the diﬀerentiation operator is that it is complex zero\n","decreasing : the number of nonreal zeros of f′is at most the number of nonreal zeros\n","off. This property is studied by [CC95] in the more general context of P´olya-Schur\n","operators, which multiply the coeﬃcients of a power series by a predetermined se-\n","quence. Much of the recent interest in such properties of the derivative and otheroperators stems from proposed attacks on the Riemann Hypothesis involving be-havior of zeros under these operators [LM74,Con83]. See also [Pem12, section 4] for\n","a survey of combinatorial reasons to study locations of zeros such as log-concavity\n","of coeﬃcients [Bre89] and negative dependence properties [BBL09].\n","The vague statement that diﬀerentiation should even out the spacings of zeros\n","is generally believed, and a number of proven results bear this out. For example, a\n","theorem attributed to Riesz (later rediscovered by others) states that the minimum\n","distancebetweenzerosofcertainentirefunctionswithonlyrealzerosisincreasedbydiﬀerentiation; see [FR05, section 2] for a history of this result and its appearancein [Sto26] and subsequent works of J. v. Sz.-Nagy and of P. Walker.\n","The logical extreme is that repeated diﬀerentiation should lead to zeros that\n","are as evenly spaced as possible. If the original function fhas real zeros, then all\n","derivatives of falso have all real zeros. If the zeros of fhave some long-run density\n","on the real line, then one might expect the zero set under repeated diﬀerentiation\n","to approach a lattice with this density. A sequence of results leading up to this was\n","proved in [FR05]. The authors show that the gaps between zeros of f\n","′+afare\n","bounded between the inﬁmum and supremum of gaps between consecutive zeros offand generalize this to a local density result that is applicable to the Riemann zeta\n","function. They claim a result [FR05, Theorem 2.4.1] that implies the convergence\n","Received by the editors October 5, 2014 and, in revised form, March 1, 2016.\n","2010Mathematics Subject Classiﬁcation. Primary 30B20, 60G55; Secondary 30C15.\n","Key words and phrases. Poisson, coeﬃcient, saddle point, lattice, Cauchy integral, random\n","series, translation-invariant.\n","The ﬁrst author’s research was supported by NSF grant DMS-1209117.\n","c⃝2017 American Mathematical Society\n","8743\n","\n","Multi-Layer Perceptrons \n","with B-SpIine Receptive Field Functions \n","Stephen H. Lane, Marshall G. Flax, David A. Handelman and JackJ. Gelfand \n","Human Information Processing Group \n","Department of Psychology \n","Princeton University \n","Princeton, New Jersey 08544 \n","ABSTRACT \n","Multi-layer perceptrons are often slow to learn nonlinear functions \n","with complex local structure due to the global nature of their function \n","approximations. It is shown that standard multi-layer perceptrons are \n","actually a special case of a more general network formulation that \n","incorporates B-splines into the node computations. This allows novel \n","spline network architectures to be developed that can combine the \n","generalization capabilities and scaling properties of global multi-layer \n","feedforward networks with the computational efficiency and learning \n","speed of local computational paradigms. Simulation results are \n","presented for the well known spiral problem of Weiland and of Lang \n","and Witbrock to show the effectiveness of the Spline Net approach. \n","1. INTRODUCTION \n","Recently, it has been shown that multi-layer feedforward neural networks, such as \n","Multi-Layer Perceptrons (MLPs) , are theoretically capable of representing arbitrary \n","mappings, provided that a sufficient number of units are included in the hidden layers \n","(Hornik et aI., 1989). Since all network weights are updated with each training \n","exemplar, these networks construct global approximations to multi-input/multi-output \n","function data in a manner analogous to fitting a low-order polynomial through a set of \n","684\n","\n","Eric\n","Wong\n"," \n","GROUP\n"," \n","BLOG\n"," \n","RESEARCH\n"," \n","TEACHING\n"," \n","PAPERS\n"," \n","ABOUT\n","Eric\n","Wong\n","Assistant\n","Professor ,\n","University\n","of\n","Pennsylvania\n","\n","1 \n","  \n"," \n"," \n"," \n"," \n","Influencers, Backfire Effects and the Power of the Periphery \n","(forthcoming in Personal Networks, edited by Mario L. Small, Brea L. Perry, Bernice \n","Pescosolido, and Edward Smith. Camb ridge: Cambridge University Press.) \n"," \n"," \n"," \n"," \n","Damon Centola * \n","  \n","*Annenberg School of Communication, School of Engineering and Applied Sciences \n","& Department of Sociology, University  of Pennsylvania, 3620 Walnut St., \n","Philadelphia, PA 19104. Contact: dcentola@asc.upenn.edu\n","\n","The Role of Visualization in Computer Science Education  \n"," \n"," \n","ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER  \n"," \n","Department of Computer Science  \n","Virginia Tech  \n","Blacksburg, VA 24061  \n"," \n"," \n","Computer Science core instruction attempts to provide a detailed understanding o f dy-\n","namic processes such as the working of an algorithm or the flow of information between \n","computing ent ities. Such dynamic processes are not well explained by static media such \n","as text and images, and are difficult to convey in le cture. We survey the hist ory of visual i-\n","zation in CS education, foc using on artifacts that have  a documented positive educational \n","assessment. We discuss how changes in computing technology have affected the deve l-\n","opment and uptake of such visualization artifacts in CS education, and how recent tech-\n","nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization, data structure visualization, program visualiz a-\n","tion, eTextbooks, hypertextbooks.  \n","  \n","(Draft of paper to appear in Computers in the Schools , 2012)\n","\n","1 \n","Chapter 17. How Social Networks Shape Social Comparison  \n"," \n","Jingwen Zhang (University of California, Davis)  \n","Damon Centola (Annenberg School for Communication/University of Pennsylvania)  \n","Abstract: While social comparison research has focused on  the processes and consequences \n","of how the comparer gleans information from the comparison other (individual or group) , \n","recent research on social networks demonstrates how information and influence is \n","distributed across persons in a network. This chapter r eviews social influence  processes in \n","social networks . We first review recent research on social compar ison and its negative \n","consequences in online social networks. Then we delve into discussing  the social network \n","causes of biased social perceptions online and  how this can be remedied by building more \n","accurate perceptions through constructed online networks.  Lastly , we discuss findings from \n","recent experimental studies  that illustrate how construc ted online networks can harness \n","social comparison to induce significant changes in health behavior.\n","\n","11/7/23, 10:40 AMVincent Liu - Publications\n","https://vincen.tl/publications.html1/5\n","\n","TANGENT BUNDLE FILTERS AND NEURAL NETWORKS:\n","FROM MANIFOLDS TO CELLULAR SHEA VES AND BACK\n","C. Battiloro1,2, Z. Wang1, H. Riess3, P . Di Lorenzo2, A. Ribeiro1\n","1ESE Department, University of Pennsylvania, Philadelphia, USA\n","2DIET Department, Sapienza University of Rome, Rome, Italy\n","3ECE Department, Duke University, Durham, USA\n","E-mail: claudio.battiloro@uniroma1.it, zhiyangw@seas.upenn.edu\n","ABSTRACT\n","In this work we introduce a convolution operation over the tangent\n","bundle of Riemannian manifolds exploiting the Connection Lapla-\n","cian operator. We use the convolution to deﬁne tangent bundle ﬁl-\n","ters and tangent bundle neural networks (TNNs), novel continuous\n","architectures operating on tangent bundle signals, i.e. vector ﬁelds\n","over manifolds. We discretize TNNs both in space and time do-\n","mains, showing that their discrete counterpart is a principled vari-\n","ant of the recently introduced Sheaf Neural Networks. We formally\n","prove that this discrete architecture converges to the underlying con-\n","tinuous TNN. We numerically evaluate the effectiveness of the pro-\n","posed architecture on a denoising task of a tangent vector ﬁeld over\n","the unit 2-sphere.\n","Index Terms —Geometric Deep Learning, Tangent Bundle Sig-\n","nal Processing, Tangent Bundle Neural Networks, Cellular Sheaves\n","1. INTRODUCTION\n","The success of deep learning is mostly the success of Convolutional\n","Neural Networks (CNNs) [1]. CNNs have achieved impressive per-\n","formance in a wide range of applications showing good generaliza-\n","tion ability. Based on shift operators in the space domain, one (but\n","not the only one) key attribute is that the convolutional ﬁlters sat-\n","isfy the property of shift equivariance. Nowadays, data deﬁned on\n","irregular (non-Euclidean) domains are pervasive, with applications\n","ranging from detection and recommendation in social networks pro-\n","cessing [2], to resource allocations over wireless networks [3], or\n","point clouds for shape segmentation [4], just to name a few. For\n","this reason, the notions of shifts in CNNs have been adapted to\n","convolutional architectures on graphs (GNNs) [5, 6] as well as a\n","plethora of other structures, e.g. simplicial complexes [7–10], cell\n","complexes [11,12], and manifolds [13]. In [14], a framework for al-\n","gebraic neural networks has been proposed exploiting commutative\n","algebras. In this work we focus on tangent bundles, a formal tool for\n","describing and processing vector ﬁelds on manifolds, which are key\n","elements in tasks such as robot navigation or ﬂocking modeling.\n","Related Works. The renowned manifold assumption states that high\n","dimensional data examples are sampled from a low-dimensional\n","Riemannian manifold. This assumption is the fundamental block\n","of manifold learning, a class of methods for non-linear dimension-\n","ality reduction. Some of these methods approximate manifolds\n","with k-NN or geometric graphs via sampling points, i.e., for a ﬁne\n","enough sampling resolution, the graph Laplacian of the approxi-\n","mating graph “converges” to the Laplace-Beltrami operator of the\n","manifold [15]. These techniques rely on the eigenvalues and eigen-\n","vectors of the graph Laplacian [16], and they give rise to a novelperspective on manifold learning. In particular, the above approx-\n","imation leads to important transferability results of graph neural\n","networks (GNNs) [17,18], as well as to the introduction of Graphon\n","and Manifold Neural Networks, continuous architectures shown to\n","be limit objects of GNNs [19, 20]. However, most of the previ-\n","ous works focus on scalar signals, e.g. one or more scalar values\n","attached to each node of graphs or point of manifolds; recent devel-\n","opments [21] show that processing vector data deﬁned on tangent\n","bundles of manifolds or discrete vector bundles [22, 23] comes with\n","a series of beneﬁts. Moreover, the work in [24] proves that it is\n","possible to approximate both manifolds and their tangent bundles\n","with certain cellular sheaves obtained from a point cloud via k-NN\n","and Local PCA, such that, for a ﬁne enough sampling resolution,\n","the Sheaf Laplacian of the approximating sheaf “converges” to the\n","Connection Laplacian operator. Finally, the work in [25] generalizes\n","the result of [24] by proving the spectral convergence of a large class\n","of Laplacian operators via the Principal Bundle set up.\n","Contributions. In this work we deﬁne a convolution operation over\n","the tangent bundles of Riemannian manifolds with the Connection\n","Laplacian operator. Our deﬁnition is consistent, i.e. it reduces to\n","manifold convolution [19] in the one-dimensional bundle case, and\n","to the standard convolution if the manifold is the real line. We intro-\n","duce tangent bundle convolutional ﬁlters to process tangent bundle\n","signals (i.e. vector ﬁelds over manifolds), we deﬁne a frequency\n","representation for them and, by cascading layers consisting of tan-\n","gent bundle ﬁlters banks and nonlinearities, we introduce Tangent\n","Bundle Neural Networks (TNNs). We then discretize the TNNs in\n","the space domain by sampling points on the manifold and build-\n","ing a cellular sheaf [26] representing a legit approximation of both\n","the manifold and its tangent bundle [24]. We formally prove that\n","the discretized architecture over the cellular sheaf converges to the\n","underlying TNN as the number of sampled points increases. More-\n","over, we further discretize the architecture in the time domain by\n","sampling the ﬁlter impulse function in discrete and ﬁnite time steps,\n","showing that space-time discretized TNNs are a principled variant\n","of the very recently introduced Sheaf Neural Networks [23, 27, 28],\n","discrete architectures operating on cellular sheaves and generalizing\n","graph neural networks. Finally, we numerically evaluate the perfor-\n","mance of TNNs on a denoising task of a tangent vector ﬁeld of the\n","unit 2-sphere.\n","Paper Outline. The paper is organized as follows. We start with\n","some preliminary concepts in Section 2. We deﬁne the tangent bun-\n","dle convolution and ﬁlters in Section 3, and Tangent Bundle Neural\n","Networks (TNNs) in Section 4. In Section 5, we discretize TNNs in\n","space and time domains, showing that discretized TNNs are Sheaf\n","Neural Networks and proving the convergence result. Numerical re-\n","sults are in Section 6 and conclusions are in Section 7.arXiv:2210.15058v3  [eess.SP]  18 Nov 2022:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Scott Weinstein's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Weingarten_weingarten_bio.pdf\n","................................................................................\n","Content: \n","erikwaing   Erik Waingarten   Publications   Teaching  Erik Waingarten\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Surbi Goel_2110.10090.pdf\n","................................................................................\n","Content: \n","Inductive Biases and Variable Creation in Self-Attention Mechanisms Benjamin L. Edelman1Surbhi Goel2Sham Kakade1,2Cyril\n","Zhang2 1Harvard University2Microsoft Research NYC bedelman@g.harvard.edu, {goel.surbhi, sham.kakade, cyrilzhang\n","}@microsoft.com Abstract Self-attention, an architectural motif designed to model long-range interactions in sequential\n","data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a\n","theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which\n","functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm\n","Transformer networks “create sparse variables”: a single self-attention head can represent a sparse function of the\n","input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we\n","present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers. 1\n","Introduction Self-attention mechanisms have comprised an era-deﬁning cornerstone of deep learning in recent years, ap-\n","pearing ubiquitously in empirical breakthroughs in generative sequence modeling and unsupervised represen- tation\n","learning. Starting with natural language (Vaswani et al., 2017), self-attention has enjoyed surprising empirical\n","successes in numerous and diverse modalities of data. In many of these settings, self-attention has supplanted\n","traditional recurrent and convolutional architectures, which are understood to incorporate in- ductive biases about\n","temporal and translational invariances in the data. Self-attention models discard these functional forms, in favor of\n","directly and globally modeling long-range interactions within the input context. The proliferation of self-attention\n","raises a fundamental question about its inductive biases: which func- tions do self-attention networks prefer to\n","represent? Various intuitions and empirics inform the design of these architectures, but formal statistical abstractions\n","and analyses are missing in this space. To this end, this work initiates an analysis of the statistical foundations of\n","self-attention. We identify an inductive bias for self-attention, for which we coin the term sparse variable creation :\n","a bounded-norm self-attention head learns a sparse function (which only depends on a small subset of input coordinates,\n","such as a constant-fan-in gate in a Boolean circuit) of a length- Tcontext, with sample complexity scaling as log( T).\n","The main technical novelty in this work is a covering number-based capacity bound for attention mechanisms (including\n","Transformer heads, as well as related and future architectures), implying norm-based generalization bounds. This is\n","accompanied by a matching representational result, showing that bounded-norm self-attention heads are indeed capable of\n","representing s-sparse functions with weight norms 2O(s)(or poly(s), for symmetric sparse functions). This provides a\n","theoretical account for why attention models can learn long-range dependencies without overﬁtting. Finally, we conduct\n","synthetic experiments to probe the sample eﬃciency of learning sparse interactions with self-attention. We train\n","Transformer models to identify sparse Boolean functions with randomly chosen indices, and corroborate the sample\n","complexity scaling law predicted by the theory. A variant of this experiment (with i.i.d. samples) reveals a\n","computational mystery, beyond the scope of our current statistical analysis: we ﬁnd that Transformers can successfully\n","learn the “hardest” (in the sense of SQ-dimension) s-sparse functions: the XOR (parity) functions. 1arXiv:2110.10090v2\n","[cs.LG]  24 Jun 2022\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Robin Pemantle_spacings.pdf\n","................................................................................\n","Content: \n","This is a free offprint provided to the author by the publisher. Copyright restrictions may apply.TRANSACTIONS OF THE\n","AMERICAN MATHEMATICAL SOCIETY Volume 369, Number 12, December 2017, Pages 8743–8764\n","http://dx.doi.org/10.1090/tran/6929Article electronically published on June 27, 2017 ZEROS OF A RANDOM ANALYTIC FUNCTION\n","APPROACH PERFECT SPACING UNDER REPEATED DIFFERENTIATION ROBIN PEMANTLE AND SNEHA SUBRAMANIAN Abstract. We consider an\n","analytic function whose zero set forms a unit in- tensity Poisson process on the real line. We show that repeated\n","diﬀerentiationcauses the zero set to converge in distribution to a random translate of theintegers. 1.Introduction Study\n","of the relation of the zero set of a function fto the zero set of its derivative has a rich history. The Gauss-Lucas\n","theorem (see, e.g., [Mar49, Theorem 6.1]) says that iffis a polynomial, then the zero set of f′lies in the convex hull\n","of the zero set off. Another property of the diﬀerentiation operator is that it is complex zero decreasing : the number\n","of nonreal zeros of f′is at most the number of nonreal zeros off. This property is studied by [CC95] in the more general\n","context of P´olya-Schur operators, which multiply the coeﬃcients of a power series by a predetermined se- quence. Much\n","of the recent interest in such properties of the derivative and otheroperators stems from proposed attacks on the\n","Riemann Hypothesis involving be-havior of zeros under these operators [LM74,Con83]. See also [Pem12, section 4] for a\n","survey of combinatorial reasons to study locations of zeros such as log-concavity of coeﬃcients [Bre89] and negative\n","dependence properties [BBL09]. The vague statement that diﬀerentiation should even out the spacings of zeros is\n","generally believed, and a number of proven results bear this out. For example, a theorem attributed to Riesz (later\n","rediscovered by others) states that the minimum\n","distancebetweenzerosofcertainentirefunctionswithonlyrealzerosisincreasedbydiﬀerentiation; see [FR05, section 2] for a\n","history of this result and its appearancein [Sto26] and subsequent works of J. v. Sz.-Nagy and of P. Walker. The logical\n","extreme is that repeated diﬀerentiation should lead to zeros that are as evenly spaced as possible. If the original\n","function fhas real zeros, then all derivatives of falso have all real zeros. If the zeros of fhave some long-run density\n","on the real line, then one might expect the zero set under repeated diﬀerentiation to approach a lattice with this\n","density. A sequence of results leading up to this was proved in [FR05]. The authors show that the gaps between zeros of\n","f ′+afare bounded between the inﬁmum and supremum of gaps between consecutive zeros offand generalize this to a local\n","density result that is applicable to the Riemann zeta function. They claim a result [FR05, Theorem 2.4.1] that implies\n","the convergence Received by the editors October 5, 2014 and, in revised form, March 1, 2016. 2010Mathematics Subject\n","Classiﬁcation. Primary 30B20, 60G55; Secondary 30C15. Key words and phrases. Poisson, coeﬃcient, saddle point, lattice,\n","Cauchy integral, random series, translation-invariant. The ﬁrst author’s research was supported by NSF grant\n","DMS-1209117. c⃝2017 American Mathematical Society 8743\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Stephen Lane_NIPS-1990-multi-layer-perceptrons-with-b-spline-receptive-field-functions-Paper.pdf\n","................................................................................\n","Content: \n","Multi-Layer Perceptrons  with B-SpIine Receptive Field Functions  Stephen H. Lane, Marshall G. Flax, David A. Handelman\n","and JackJ. Gelfand  Human Information Processing Group  Department of Psychology  Princeton University  Princeton, New\n","Jersey 08544  ABSTRACT  Multi-layer perceptrons are often slow to learn nonlinear functions  with complex local\n","structure due to the global nature of their function  approximations. It is shown that standard multi-layer perceptrons\n","are  actually a special case of a more general network formulation that  incorporates B-splines into the node\n","computations. This allows novel  spline network architectures to be developed that can combine the  generalization\n","capabilities and scaling properties of global multi-layer  feedforward networks with the computational efficiency and\n","learning  speed of local computational paradigms. Simulation results are  presented for the well known spiral problem of\n","Weiland and of Lang  and Witbrock to show the effectiveness of the Spline Net approach.  1. INTRODUCTION  Recently, it\n","has been shown that multi-layer feedforward neural networks, such as  Multi-Layer Perceptrons (MLPs) , are theoretically\n","capable of representing arbitrary  mappings, provided that a sufficient number of units are included in the hidden\n","layers  (Hornik et aI., 1989). Since all network weights are updated with each training  exemplar, these networks\n","construct global approximations to multi-input/multi-output  function data in a manner analogous to fitting a low-order\n","polynomial through a set of  684\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Wong_wong_bio.pdf\n","................................................................................\n","Content: \n","Eric Wong   GROUP   BLOG   RESEARCH   TEACHING   PAPERS   ABOUT Eric Wong Assistant Professor , University of\n","Pennsylvania\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Damon Centola_Centola_InfluencersBackfire-Effects-and-the-Power-of-the-Periphery_in_PersonalNetworks.pdf\n","................................................................................\n","Content: \n","1             Influencers, Backfire Effects and the Power of the Periphery  (forthcoming in Personal Networks, edited by\n","Mario L. Small, Brea L. Perry, Bernice  Pescosolido, and Edward Smith. Camb ridge: Cambridge University Press.)\n","Damon Centola *     *Annenberg School of Communication, School of Engineering and Applied Sciences  & Department of\n","Sociology, University  of Pennsylvania, 3620 Walnut St.,  Philadelphia, PA 19104. Contact: dcentola@asc.upenn.edu\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Fouh_AVpedagogypost.pdf\n","................................................................................\n","Content: \n","The Role of Visualization in Computer Science Education       ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER\n","Department of Computer Science   Virginia Tech   Blacksburg, VA 24061       Computer Science core instruction attempts\n","to provide a detailed understanding o f dy- namic processes such as the working of an algorithm or the flow of\n","information between  computing ent ities. Such dynamic processes are not well explained by static media such  as text\n","and images, and are difficult to convey in le cture. We survey the hist ory of visual i- zation in CS education, foc\n","using on artifacts that have  a documented positive educational  assessment. We discuss how changes in computing\n","technology have affected the deve l- opment and uptake of such visualization artifacts in CS education, and how recent\n","tech- nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization,\n","data structure visualization, program visualiz a- tion, eTextbooks, hypertextbooks.      (Draft of paper to appear in\n","Computers in the Schools , 2012)\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Damon Centola_Chapter-17-social-comparison.pdf\n","................................................................................\n","Content: \n","1  Chapter 17. How Social Networks Shape Social Comparison     Jingwen Zhang (University of California, Davis)   Damon\n","Centola (Annenberg School for Communication/University of Pennsylvania)   Abstract: While social comparison research has\n","focused on  the processes and consequences  of how the comparer gleans information from the comparison other (individual\n","or group) ,  recent research on social networks demonstrates how information and influence is  distributed across\n","persons in a network. This chapter r eviews social influence  processes in  social networks . We first review recent\n","research on social compar ison and its negative  consequences in online social networks. Then we delve into discussing\n","the social network  causes of biased social perceptions online and  how this can be remedied by building more  accurate\n","perceptions through constructed online networks.  Lastly , we discuss findings from  recent experimental studies  that\n","illustrate how construc ted online networks can harness  social comparison to induce significant changes in health\n","behavior.\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Vincent Liu_Vincent Liu - Publications.pdf\n","................................................................................\n","Content: \n","11/7/23, 10:40 AMVincent Liu - Publications https://vincen.tl/publications.html1/5\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Alejandro Ribeiro_2210.15058.pdf\n","................................................................................\n","Content: \n","TANGENT BUNDLE FILTERS AND NEURAL NETWORKS: FROM MANIFOLDS TO CELLULAR SHEA VES AND BACK C. Battiloro1,2, Z. Wang1, H.\n","Riess3, P . Di Lorenzo2, A. Ribeiro1 1ESE Department, University of Pennsylvania, Philadelphia, USA 2DIET Department,\n","Sapienza University of Rome, Rome, Italy 3ECE Department, Duke University, Durham, USA E-mail:\n","claudio.battiloro@uniroma1.it, zhiyangw@seas.upenn.edu ABSTRACT In this work we introduce a convolution operation over\n","the tangent bundle of Riemannian manifolds exploiting the Connection Lapla- cian operator. We use the convolution to\n","deﬁne tangent bundle ﬁl- ters and tangent bundle neural networks (TNNs), novel continuous architectures operating on\n","tangent bundle signals, i.e. vector ﬁelds over manifolds. We discretize TNNs both in space and time do- mains, showing\n","that their discrete counterpart is a principled vari- ant of the recently introduced Sheaf Neural Networks. We formally\n","prove that this discrete architecture converges to the underlying con- tinuous TNN. We numerically evaluate the\n","effectiveness of the pro- posed architecture on a denoising task of a tangent vector ﬁeld over the unit 2-sphere. Index\n","Terms —Geometric Deep Learning, Tangent Bundle Sig- nal Processing, Tangent Bundle Neural Networks, Cellular Sheaves 1.\n","INTRODUCTION The success of deep learning is mostly the success of Convolutional Neural Networks (CNNs) [1]. CNNs have\n","achieved impressive per- formance in a wide range of applications showing good generaliza- tion ability. Based on shift\n","operators in the space domain, one (but not the only one) key attribute is that the convolutional ﬁlters sat- isfy the\n","property of shift equivariance. Nowadays, data deﬁned on irregular (non-Euclidean) domains are pervasive, with\n","applications ranging from detection and recommendation in social networks pro- cessing [2], to resource allocations over\n","wireless networks [3], or point clouds for shape segmentation [4], just to name a few. For this reason, the notions of\n","shifts in CNNs have been adapted to convolutional architectures on graphs (GNNs) [5, 6] as well as a plethora of other\n","structures, e.g. simplicial complexes [7–10], cell complexes [11,12], and manifolds [13]. In [14], a framework for al-\n","gebraic neural networks has been proposed exploiting commutative algebras. In this work we focus on tangent bundles, a\n","formal tool for describing and processing vector ﬁelds on manifolds, which are key elements in tasks such as robot\n","navigation or ﬂocking modeling. Related Works. The renowned manifold assumption states that high dimensional data\n","examples are sampled from a low-dimensional Riemannian manifold. This assumption is the fundamental block of manifold\n","learning, a class of methods for non-linear dimension- ality reduction. Some of these methods approximate manifolds with\n","k-NN or geometric graphs via sampling points, i.e., for a ﬁne enough sampling resolution, the graph Laplacian of the\n","approxi- mating graph “converges” to the Laplace-Beltrami operator of the manifold [15]. These techniques rely on the\n","eigenvalues and eigen- vectors of the graph Laplacian [16], and they give rise to a novelperspective on manifold\n","learning. In particular, the above approx- imation leads to important transferability results of graph neural networks\n","(GNNs) [17,18], as well as to the introduction of Graphon and Manifold Neural Networks, continuous architectures shown\n","to be limit objects of GNNs [19, 20]. However, most of the previ- ous works focus on scalar signals, e.g. one or more\n","scalar values attached to each node of graphs or point of manifolds; recent devel- opments [21] show that processing\n","vector data deﬁned on tangent bundles of manifolds or discrete vector bundles [22, 23] comes with a series of beneﬁts.\n","Moreover, the work in [24] proves that it is possible to approximate both manifolds and their tangent bundles with\n","certain cellular sheaves obtained from a point cloud via k-NN and Local PCA, such that, for a ﬁne enough sampling\n","resolution, the Sheaf Laplacian of the approximating sheaf “converges” to the Connection Laplacian operator. Finally,\n","the work in [25] generalizes the result of [24] by proving the spectral convergence of a large class of Laplacian\n","operators via the Principal Bundle set up. Contributions. In this work we deﬁne a convolution operation over the tangent\n","bundles of Riemannian manifolds with the Connection Laplacian operator. Our deﬁnition is consistent, i.e. it reduces to\n","manifold convolution [19] in the one-dimensional bundle case, and to the standard convolution if the manifold is the\n","real line. We intro- duce tangent bundle convolutional ﬁlters to process tangent bundle signals (i.e. vector ﬁelds over\n","manifolds), we deﬁne a frequency representation for them and, by cascading layers consisting of tan- gent bundle ﬁlters\n","banks and nonlinearities, we introduce Tangent Bundle Neural Networks (TNNs). We then discretize the TNNs in the space\n","domain by sampling points on the manifold and build- ing a cellular sheaf [26] representing a legit approximation of\n","both the manifold and its tangent bundle [24]. We formally prove that the discretized architecture over the cellular\n","sheaf converges to the underlying TNN as the number of sampled points increases. More- over, we further discretize the\n","architecture in the time domain by sampling the ﬁlter impulse function in discrete and ﬁnite time steps, showing that\n","space-time discretized TNNs are a principled variant of the very recently introduced Sheaf Neural Networks [23, 27, 28],\n","discrete architectures operating on cellular sheaves and generalizing graph neural networks. Finally, we numerically\n","evaluate the perfor- mance of TNNs on a denoising task of a tangent vector ﬁeld of the unit 2-sphere. Paper Outline. The\n","paper is organized as follows. We start with some preliminary concepts in Section 2. We deﬁne the tangent bun- dle\n","convolution and ﬁlters in Section 3, and Tangent Bundle Neural Networks (TNNs) in Section 4. In Section 5, we discretize\n","TNNs in space and time domains, showing that discretized TNNs are Sheaf Neural Networks and proving the convergence\n","result. Numerical re- sults are in Section 6 and conclusions are in Section 7.arXiv:2210.15058v3  [eess.SP]  18 Nov 2022\n","................................................................................\n","Response: From Scott Weinstein's work: - Language Guided Bottlenecks (LaBo) - Concept Bottleneck Models (CBM) - GPT-3 - Inherently\n","interpretable models - Visual recognition - Computer Vision - Image Processing - High-stakes applications - Manual\n","specification of concepts - Black box models - Linear probes - Few-shot classification - Performance comparison with\n","black box approaches  From the second paper: - Trading - Markovian Model - High Frequency Trading - Pricing Models -\n","Quantitative Finance - Classical Finance - Random Walk - Statistics - Mathematical Finance - Probabilistic Models  From\n","the third paper: - Reinforcement Learning - Markov Decision Process - Artificial Intelligence - Algorithmic Run-time\n","From the fourth paper: - Self-attention mechanisms - Inductive biases - Sparse variables - Bounded-norm Transformer\n","networks - Sample complexity - Synthetic experiments - Sparse Boolean functions  From the fifth paper: - Zeros of a\n","random analytic function - Perfect spacing under repeated differentiation - Gauss-Lucas theorem - Diﬀerentiation\n","operator - Complex zero decreasing - Riemann Hypothesis - Combinatorial reasons for studying locations of zeros - Zero\n","set convergence - Maximum distance between zeros - Convergence of zero set under repeated differentiation - Zero spacing\n","- Evenly spaced zeros  From the sixth paper: - Multi-Layer Perceptrons (MLPs) - B-Spline Receptive Field Functions -\n","Nonlinear functions - Global nature of function approximations - Computational efficiency - Learning speed - Spiral\n","problem - Effectiveness of Spline Net approach  From the seventh paper: - Visualization in Computer Science Education -\n","Dynamic processes - Information flow - Static media - Lectures - History of visualization in CS education - Positive\n","educational assessment - Computing technology - Online hypertextbooks - Algorithm visualization - Data structure\n","visualization - Program visualization - eTextbooks - Hypertextbooks  From the eighth paper: - Social Networks - Social\n","Comparison - Social influence processes - Negative consequences of social comparison - Biased social perceptions -\n","Constructed online networks - Changes in health behavior  From the ninth paper: - Tangent Bundle Filters - Tangent\n","Bundle Neural Networks (TNNs) - Manifolds - Cellular Sheaves - Connection Laplacian operator - Convergence of discrete\n","TNNs to continuous TNNs - Denoising task - Unit 2-sphere\n","................................................................................\n","What are key features of Stephanie Weirich's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Stephanie Weirich's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","Adaptive Query\n","Processing\n","Full text available at: http://dx.doi.org/10.1561/1900000001\n","\n","The Role of Visualization in Computer Science Education  \n"," \n"," \n","ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER  \n"," \n","Department of Computer Science  \n","Virginia Tech  \n","Blacksburg, VA 24061  \n"," \n"," \n","Computer Science core instruction attempts to provide a detailed understanding o f dy-\n","namic processes such as the working of an algorithm or the flow of information between \n","computing ent ities. Such dynamic processes are not well explained by static media such \n","as text and images, and are difficult to convey in le cture. We survey the hist ory of visual i-\n","zation in CS education, foc using on artifacts that have  a documented positive educational \n","assessment. We discuss how changes in computing technology have affected the deve l-\n","opment and uptake of such visualization artifacts in CS education, and how recent tech-\n","nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization, data structure visualization, program visualiz a-\n","tion, eTextbooks, hypertextbooks.  \n","  \n","(Draft of paper to appear in Computers in the Schools , 2012)\n","\n","11/8/23, 1:20 PMVictor M. Preciado - Research Interests\n","https://sites.google.com/site/victormpreciado/research-11/7\n","Research Interests\n","My primary research interests are modeling, analysis, and control of complex networked systems andengineering infrastructures, with applications in social networks, technological infrastructure, andbiological systems. Since critical societal functions are increasingly dependent on complex networks, it isimportant to acquire a deeper understanding of the relationship between the structure and the dynamicsof these systems. Research lines of particular interest are described below:\n","Victor M. PreciadoHomeResearch InterestsList of PublicationsT\n","\n","Data Intimacy, Machine Learning, and Consumer Privacy  \n"," \n","Prof. Michael Kearns1 \n","University of Pennsylvania  \n"," \n","Summary. We discuss and analyze the data sources and practices at large consumer -facing \n","technology companies such as Google and Facebook, and examine the central role of machine \n","learning and artificial intelligence at such companies. We focus in particular on the notion of \n","data intimacy --- the fa ct that machine learning enables  companies to routinely draw accurate \n","predictions and inferences abo ut users that go far deeper than what is merely on the “surface” \n","of the data collected. We discuss the consequences  for consumer privacy, and briefly discuss \n","broad implications for policy and regulation.  \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","                                                           \n","1 The author would like to thank AT&T for their support of this work. All analyses, exposition and opinions are \n","exclusively those of the author.\n","\n","My Bibliography - NCBI https://www.ncbi.nlm.nih.gov/myncbi/1NK_X-OTIL3kq/bibliography/p...\n","1 of 8 11/20/2023, 8:17 PM\n","\n","Auditing Algorithms\n","Understanding Algorithmic Systems\n","from the Outside In\n","Full text available at: http://dx.doi.org/10.1561/1100000083\n","\n","REPORT TO THE PRESIDENT\n","AND CONGRESS\n","DESIGNING A DIGITAL FUTURE:  \n","FEDERALLY FUNDED RESEARCH \n","AND DEVELOPMENT IN\n","NETWORKING AND INFORMATION  \n","TECHNOLOGY\n","Executive Office of the President\n","President’s Council of Advisors on  \n","Science and Technology\n","DE CE M BE R  2 010\n","\n","REPORT TO THE PRESIDENT\n","AND CONGRESS\n","DESIGNING A DIGITAL FUTURE:  \n","FEDERALLY FUNDED RESEARCH \n","AND DEVELOPMENT IN\n","NETWORKING AND INFORMATION  \n","TECHNOLOGY\n","Executive Office of the President\n","President’s Council of Advisors on  \n","Science and Technology\n","DE CE M BE R  2 010\n","\n","PDF hosted at the Radboud Repository of the Radboud University\n","Nijmegen \n"," \n"," \n"," \n"," \n","The following full text is a publisher's version. \n"," \n"," \n","For additional information about this publication click this link.\n","http://hdl.handle.net/2066/96888\n"," \n"," \n"," \n","Please be advised that this information was generated on 2023-11-20 and may be subject to\n","change.\n","\n","Inductive Biases and Variable Creation\n","in Self-Attention Mechanisms\n","Benjamin L. Edelman1Surbhi Goel2Sham Kakade1,2Cyril Zhang2\n","1Harvard University2Microsoft Research NYC\n","bedelman@g.harvard.edu, {goel.surbhi, sham.kakade, cyrilzhang }@microsoft.com\n","Abstract\n","Self-attention, an architectural motif designed to model long-range interactions in sequential data, has\n","driven numerous recent breakthroughs in natural language processing and beyond. This work provides a\n","theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish\n","which functions and long-range dependencies self-attention blocks prefer to represent. Our main result\n","shows that bounded-norm Transformer networks “create sparse variables”: a single self-attention head\n","can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically\n","with the context length. To support our analysis, we present synthetic experiments to probe the sample\n","complexity of learning sparse Boolean functions with Transformers.\n","1 Introduction\n","Self-attention mechanisms have comprised an era-deﬁning cornerstone of deep learning in recent years, ap-\n","pearing ubiquitously in empirical breakthroughs in generative sequence modeling and unsupervised represen-\n","tation learning. Starting with natural language (Vaswani et al., 2017), self-attention has enjoyed surprising\n","empirical successes in numerous and diverse modalities of data. In many of these settings, self-attention has\n","supplanted traditional recurrent and convolutional architectures, which are understood to incorporate in-\n","ductive biases about temporal and translational invariances in the data. Self-attention models discard these\n","functional forms, in favor of directly and globally modeling long-range interactions within the input context.\n","The proliferation of self-attention raises a fundamental question about its inductive biases: which func-\n","tions do self-attention networks prefer to represent? Various intuitions and empirics inform the design of\n","these architectures, but formal statistical abstractions and analyses are missing in this space. To this end,\n","this work initiates an analysis of the statistical foundations of self-attention.\n","We identify an inductive bias for self-attention, for which we coin the term sparse variable creation : a\n","bounded-norm self-attention head learns a sparse function (which only depends on a small subset of input\n","coordinates, such as a constant-fan-in gate in a Boolean circuit) of a length- Tcontext, with sample complexity\n","scaling as log( T). The main technical novelty in this work is a covering number-based capacity bound for\n","attention mechanisms (including Transformer heads, as well as related and future architectures), implying\n","norm-based generalization bounds. This is accompanied by a matching representational result, showing that\n","bounded-norm self-attention heads are indeed capable of representing s-sparse functions with weight norms\n","2O(s)(or poly(s), for symmetric sparse functions). This provides a theoretical account for why attention\n","models can learn long-range dependencies without overﬁtting.\n","Finally, we conduct synthetic experiments to probe the sample eﬃciency of learning sparse interactions\n","with self-attention. We train Transformer models to identify sparse Boolean functions with randomly chosen\n","indices, and corroborate the sample complexity scaling law predicted by the theory. A variant of this\n","experiment (with i.i.d. samples) reveals a computational mystery, beyond the scope of our current statistical\n","analysis: we ﬁnd that Transformers can successfully learn the “hardest” (in the sense of SQ-dimension)\n","s-sparse functions: the XOR (parity) functions.\n","1arXiv:2110.10090v2  [cs.LG]  24 Jun 2022:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Stephanie Weirich's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Zachary Ives_9781601980359-summary.pdf\n","................................................................................\n","Content: \n","Adaptive Query Processing Full text available at: http://dx.doi.org/10.1561/1900000001\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Fouh_AVpedagogypost.pdf\n","................................................................................\n","Content: \n","The Role of Visualization in Computer Science Education       ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER\n","Department of Computer Science   Virginia Tech   Blacksburg, VA 24061       Computer Science core instruction attempts\n","to provide a detailed understanding o f dy- namic processes such as the working of an algorithm or the flow of\n","information between  computing ent ities. Such dynamic processes are not well explained by static media such  as text\n","and images, and are difficult to convey in le cture. We survey the hist ory of visual i- zation in CS education, foc\n","using on artifacts that have  a documented positive educational  assessment. We discuss how changes in computing\n","technology have affected the deve l- opment and uptake of such visualization artifacts in CS education, and how recent\n","tech- nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization,\n","data structure visualization, program visualiz a- tion, eTextbooks, hypertextbooks.      (Draft of paper to appear in\n","Computers in the Schools , 2012)\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Victor M. Preciado_Victor M. Preciado - Research Interests.pdf\n","................................................................................\n","Content: \n","11/8/23, 1:20 PMVictor M. Preciado - Research Interests https://sites.google.com/site/victormpreciado/research-11/7\n","Research Interests My primary research interests are modeling, analysis, and control of complex networked systems\n","andengineering infrastructures, with applications in social networks, technological infrastructure, andbiological\n","systems. Since critical societal functions are increasingly dependent on complex networks, it isimportant to acquire a\n","deeper understanding of the relationship between the structure and the dynamicsof these systems. Research lines of\n","particular interest are described below: Victor M. PreciadoHomeResearch InterestsList of PublicationsT\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Michael Kearns _kearns-finalpdf.pdf\n","................................................................................\n","Content: \n","Data Intimacy, Machine Learning, and Consumer Privacy     Prof. Michael Kearns1  University of Pennsylvania     Summary.\n","We discuss and analyze the data sources and practices at large consumer -facing  technology companies such as Google and\n","Facebook, and examine the central role of machine  learning and artificial intelligence at such companies. We focus in\n","particular on the notion of  data intimacy --- the fa ct that machine learning enables  companies to routinely draw\n","accurate  predictions and inferences abo ut users that go far deeper than what is merely on the “surface”  of the data\n","collected. We discuss the consequences  for consumer privacy, and briefly discuss  broad implications for policy and\n","regulation.                                                                                                 1 The author\n","would like to thank AT&T for their support of this work. All analyses, exposition and opinions are  exclusively those of\n","the author.\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Daniel Hashimoto_Hashimoto Publications.pdf\n","................................................................................\n","Content: \n","My Bibliography - NCBI https://www.ncbi.nlm.nih.gov/myncbi/1NK_X-OTIL3kq/bibliography/p... 1 of 8 11/20/2023, 8:17 PM\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Danaë Metaxa_9781680839173-summary.pdf\n","................................................................................\n","Content: \n","Auditing Algorithms Understanding Algorithmic Systems from the Outside In Full text available at:\n","http://dx.doi.org/10.1561/1100000083\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Michael Kearns _nitrd.pdf\n","................................................................................\n","Content: \n","REPORT TO THE PRESIDENT AND CONGRESS DESIGNING A DIGITAL FUTURE:   FEDERALLY FUNDED RESEARCH  AND DEVELOPMENT IN\n","NETWORKING AND INFORMATION   TECHNOLOGY Executive Office of the President President’s Council of Advisors on   Science\n","and Technology DE CE M BE R  2 010\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Michael Kearns _nitrd(1).pdf\n","................................................................................\n","Content: \n","REPORT TO THE PRESIDENT AND CONGRESS DESIGNING A DIGITAL FUTURE:   FEDERALLY FUNDED RESEARCH  AND DEVELOPMENT IN\n","NETWORKING AND INFORMATION   TECHNOLOGY Executive Office of the President President’s Council of Advisors on   Science\n","and Technology DE CE M BE R  2 010\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/James Gee_96888.pdf\n","................................................................................\n","Content: \n","PDF hosted at the Radboud Repository of the Radboud University Nijmegen          The following full text is a\n","publisher's version.      For additional information about this publication click this link.\n","http://hdl.handle.net/2066/96888       Please be advised that this information was generated on 2023-11-20 and may be\n","subject to change.\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Surbi Goel_2110.10090.pdf\n","................................................................................\n","Content: \n","Inductive Biases and Variable Creation in Self-Attention Mechanisms Benjamin L. Edelman1Surbhi Goel2Sham Kakade1,2Cyril\n","Zhang2 1Harvard University2Microsoft Research NYC bedelman@g.harvard.edu, {goel.surbhi, sham.kakade, cyrilzhang\n","}@microsoft.com Abstract Self-attention, an architectural motif designed to model long-range interactions in sequential\n","data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a\n","theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which\n","functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm\n","Transformer networks “create sparse variables”: a single self-attention head can represent a sparse function of the\n","input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we\n","present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers. 1\n","Introduction Self-attention mechanisms have comprised an era-deﬁning cornerstone of deep learning in recent years, ap-\n","pearing ubiquitously in empirical breakthroughs in generative sequence modeling and unsupervised represen- tation\n","learning. Starting with natural language (Vaswani et al., 2017), self-attention has enjoyed surprising empirical\n","successes in numerous and diverse modalities of data. In many of these settings, self-attention has supplanted\n","traditional recurrent and convolutional architectures, which are understood to incorporate in- ductive biases about\n","temporal and translational invariances in the data. Self-attention models discard these functional forms, in favor of\n","directly and globally modeling long-range interactions within the input context. The proliferation of self-attention\n","raises a fundamental question about its inductive biases: which func- tions do self-attention networks prefer to\n","represent? Various intuitions and empirics inform the design of these architectures, but formal statistical abstractions\n","and analyses are missing in this space. To this end, this work initiates an analysis of the statistical foundations of\n","self-attention. We identify an inductive bias for self-attention, for which we coin the term sparse variable creation :\n","a bounded-norm self-attention head learns a sparse function (which only depends on a small subset of input coordinates,\n","such as a constant-fan-in gate in a Boolean circuit) of a length- Tcontext, with sample complexity scaling as log( T).\n","The main technical novelty in this work is a covering number-based capacity bound for attention mechanisms (including\n","Transformer heads, as well as related and future architectures), implying norm-based generalization bounds. This is\n","accompanied by a matching representational result, showing that bounded-norm self-attention heads are indeed capable of\n","representing s-sparse functions with weight norms 2O(s)(or poly(s), for symmetric sparse functions). This provides a\n","theoretical account for why attention models can learn long-range dependencies without overﬁtting. Finally, we conduct\n","synthetic experiments to probe the sample eﬃciency of learning sparse interactions with self-attention. We train\n","Transformer models to identify sparse Boolean functions with randomly chosen indices, and corroborate the sample\n","complexity scaling law predicted by the theory. A variant of this experiment (with i.i.d. samples) reveals a\n","computational mystery, beyond the scope of our current statistical analysis: we ﬁnd that Transformers can successfully\n","learn the “hardest” (in the sense of SQ-dimension) s-sparse functions: the XOR (parity) functions. 1arXiv:2110.10090v2\n","[cs.LG]  24 Jun 2022\n","................................................................................\n","Response: Concepts, features, and related topics from Stephanie Weirich's work:  1. Language Guided Bottlenecks (LaBo) 2. Concept\n","Bottleneck Models (CBM) 3. Machine learning 4. Data Science 5. GPT-3 (language model) 6. LLMs (Language models) 7.\n","Natural Language Processing (NLP) 8. Computer Vision 9. Image Processing 10. Trading 11. Markovian Model 12. High-\n","Frequency Trading 13. Pricing Models 14. Quantitative Finance 15. Classical Finance 16. Random Walk 17. Statistics 18.\n","Mathematical Finance 19. Probabilistic Models 20. Reinforcement Learning 21. Markov Decision Process 22. Artificial\n","Intelligence 23. Algorithmic Run-time 24. Adaptive Query Processing 25. Visualization in Computer Science Education 26.\n","Algorithm Visualization 27. Data Structure Visualization 28. Program Visualization 29. eTextbooks, hypertextbooks 30.\n","Data Intimacy 31. Consumer Privacy 32. Auditing Algorithms 33. Understanding Algorithmic Systems 34. Federally Funded\n","Research and Development in Networking and Information Technology 35. Complex Networked Systems 36. Engineering\n","Infrastructures 37. Social Networks 38. Technological Infrastructure 39. Biological Systems 40. Inductive Biases 41.\n","Variable Creation 42. Self-Attention Mechanisms\n","................................................................................\n","What are key features of Steven Zdancemic's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Steven Zdancemic's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","erikwaing\n"," \n","Erik\n","Waingarten\n"," \n","Publications\n"," \n","Teaching \n","Erik\n","Waingarten\n","\n","The Role of Visualization in Computer Science Education  \n"," \n"," \n","ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER  \n"," \n","Department of Computer Science  \n","Virginia Tech  \n","Blacksburg, VA 24061  \n"," \n"," \n","Computer Science core instruction attempts to provide a detailed understanding o f dy-\n","namic processes such as the working of an algorithm or the flow of information between \n","computing ent ities. Such dynamic processes are not well explained by static media such \n","as text and images, and are difficult to convey in le cture. We survey the hist ory of visual i-\n","zation in CS education, foc using on artifacts that have  a documented positive educational \n","assessment. We discuss how changes in computing technology have affected the deve l-\n","opment and uptake of such visualization artifacts in CS education, and how recent tech-\n","nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization, data structure visualization, program visualiz a-\n","tion, eTextbooks, hypertextbooks.  \n","  \n","(Draft of paper to appear in Computers in the Schools , 2012)\n","\n","Current\n","research\n","Past\n","projects\n","Publications\n","Community\n","service\n","Education\n","Oleg\n","Sokolsky\n","Research\n","Professor\n","\n","Eric\n","Wong\n"," \n","GROUP\n"," \n","BLOG\n"," \n","RESEARCH\n"," \n","TEACHING\n"," \n","PAPERS\n"," \n","ABOUT\n","Eric\n","Wong\n","Assistant\n","Professor ,\n","University\n","of\n","Pennsylvania\n","\n","Ann. Comb.\n","c⃝2023 The Author(s), under exclusive licence to\n","Springer Nature Switzerland AG\n","https://doi.org/10.1007/s00026-023-00654-2Annals of Combinatorics\n","Asymptotics of Multivariate Sequences IV:\n","Generating Functions with Poleson a Hyperplane Arrangement\n","Yuliy Baryshnikov , Stephen Melczer and Robin Pemantle\n","Abstract. LetF(z1,...,z d) be the quotient of an analytic function with\n","a product of linear functions. Working in the framework of analytic com-binatorics in several variables, we compute asymptotic formulae for the\n","Taylor coeﬃcients of Fusing multivariate residues and saddle-point ap-\n","proximations. Because the singular set of Fis the union of hyperplanes,\n","we are able to make explicit the topological decompositions which arise in\n","the multivariate singularity analysis. In addition to eﬀective and explicit\n","asymptotic results, we provide the ﬁrst results on transitions between dif-ferent asymptotic regimes, and provide the ﬁrst software package to verify\n","and compute asymptotics in non-smooth cases of analytic combinatorics\n","in several variables. It is also our hope that this paper will serve as anentry to the more advanced corners of analytic combinatorics in several\n","variables for combinatorialists.\n","1. Introduction\n","In this paper we study the coeﬃcients of meromorphic functions\n","F(z)=F(z1,...,z d)=G(z)∏m\n","j=1ℓj(z)pj(1)\n","whose denominator is the product of positive integer powers of real linear\n","functions ℓj. Such functions arise, among other places, in queuing theory.\n","Example 1.1. The so-called partition generating function for a closed multi-\n","class queuing network with one inﬁnite server has the form\n","F(z)=ez1+z2+···+zd\n","∏m\n","j=1(\n","1−∑d\n","i=1ρijzj) (2)\n","0123456789().: V,-vol\n","\n","Auditing Algorithms\n","Understanding Algorithmic Systems\n","from the Outside In\n","Full text available at: http://dx.doi.org/10.1561/1100000083\n","\n","11/8/23, 10:31 AMPratyush Mishra\n","https://pratyushmishra.com1/2\n","\n","1 \n","  \n"," \n"," \n"," \n"," \n","Influencers, Backfire Effects and the Power of the Periphery \n","(forthcoming in Personal Networks, edited by Mario L. Small, Brea L. Perry, Bernice \n","Pescosolido, and Edward Smith. Camb ridge: Cambridge University Press.) \n"," \n"," \n"," \n"," \n","Damon Centola * \n","  \n","*Annenberg School of Communication, School of Engineering and Applied Sciences \n","& Department of Sociology, University  of Pennsylvania, 3620 Walnut St., \n","Philadelphia, PA 19104. Contact: dcentola@asc.upenn.edu\n","\n","Adaptive Query\n","Processing\n","Full text available at: http://dx.doi.org/10.1561/1900000001\n","\n","Bayesian Generalized Biclustering Analysis via\n","Adaptive Structured Shrinkage1\n","Qi Long, Ph.D.\n","Department of Biostatistics, Epidemiology and Informatics\n","Perelman School of Medicine\n","University of Pennsylvania\n","November 7, 2018\n","1Joint work with Ziyi Li, Changgee Chang, Suprateek Kundu:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Steven Zdancemic's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Weingarten_weingarten_bio.pdf\n","................................................................................\n","Content: \n","erikwaing   Erik Waingarten   Publications   Teaching  Erik Waingarten\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Fouh_AVpedagogypost.pdf\n","................................................................................\n","Content: \n","The Role of Visualization in Computer Science Education       ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER\n","Department of Computer Science   Virginia Tech   Blacksburg, VA 24061       Computer Science core instruction attempts\n","to provide a detailed understanding o f dy- namic processes such as the working of an algorithm or the flow of\n","information between  computing ent ities. Such dynamic processes are not well explained by static media such  as text\n","and images, and are difficult to convey in le cture. We survey the hist ory of visual i- zation in CS education, foc\n","using on artifacts that have  a documented positive educational  assessment. We discuss how changes in computing\n","technology have affected the deve l- opment and uptake of such visualization artifacts in CS education, and how recent\n","tech- nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization,\n","data structure visualization, program visualiz a- tion, eTextbooks, hypertextbooks.      (Draft of paper to appear in\n","Computers in the Schools , 2012)\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Oleg Sokolsky_sokolsky_bio.pdf\n","................................................................................\n","Content: \n","Current research Past projects Publications Community service Education Oleg Sokolsky Research Professor\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Wong_wong_bio.pdf\n","................................................................................\n","Content: \n","Eric Wong   GROUP   BLOG   RESEARCH   TEACHING   PAPERS   ABOUT Eric Wong Assistant Professor , University of\n","Pennsylvania\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Robin Pemantle_s00026-023-00654-2.pdf\n","................................................................................\n","Content: \n","Ann. Comb. c⃝2023 The Author(s), under exclusive licence to Springer Nature Switzerland AG\n","https://doi.org/10.1007/s00026-023-00654-2Annals of Combinatorics Asymptotics of Multivariate Sequences IV: Generating\n","Functions with Poleson a Hyperplane Arrangement Yuliy Baryshnikov , Stephen Melczer and Robin Pemantle Abstract.\n","LetF(z1,...,z d) be the quotient of an analytic function with a product of linear functions. Working in the framework of\n","analytic com-binatorics in several variables, we compute asymptotic formulae for the Taylor coeﬃcients of Fusing\n","multivariate residues and saddle-point ap- proximations. Because the singular set of Fis the union of hyperplanes, we\n","are able to make explicit the topological decompositions which arise in the multivariate singularity analysis. In\n","addition to eﬀective and explicit asymptotic results, we provide the ﬁrst results on transitions between dif-ferent\n","asymptotic regimes, and provide the ﬁrst software package to verify and compute asymptotics in non-smooth cases of\n","analytic combinatorics in several variables. It is also our hope that this paper will serve as anentry to the more\n","advanced corners of analytic combinatorics in several variables for combinatorialists. 1. Introduction In this paper we\n","study the coeﬃcients of meromorphic functions F(z)=F(z1,...,z d)=G(z)∏m j=1ℓj(z)pj(1) whose denominator is the product\n","of positive integer powers of real linear functions ℓj. Such functions arise, among other places, in queuing theory.\n","Example 1.1. The so-called partition generating function for a closed multi- class queuing network with one inﬁnite\n","server has the form F(z)=ez1+z2+···+zd ∏m j=1( 1−∑d i=1ρijzj) (2) 0123456789().: V,-vol\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Danaë Metaxa_9781680839173-summary.pdf\n","................................................................................\n","Content: \n","Auditing Algorithms Understanding Algorithmic Systems from the Outside In Full text available at:\n","http://dx.doi.org/10.1561/1100000083\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Pratyush Mishra_Pratyush Mishra.pdf\n","................................................................................\n","Content: \n","11/8/23, 10:31 AMPratyush Mishra https://pratyushmishra.com1/2\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Damon Centola_Centola_InfluencersBackfire-Effects-and-the-Power-of-the-Periphery_in_PersonalNetworks.pdf\n","................................................................................\n","Content: \n","1             Influencers, Backfire Effects and the Power of the Periphery  (forthcoming in Personal Networks, edited by\n","Mario L. Small, Brea L. Perry, Bernice  Pescosolido, and Edward Smith. Camb ridge: Cambridge University Press.)\n","Damon Centola *     *Annenberg School of Communication, School of Engineering and Applied Sciences  & Department of\n","Sociology, University  of Pennsylvania, 3620 Walnut St.,  Philadelphia, PA 19104. Contact: dcentola@asc.upenn.edu\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Zachary Ives_9781601980359-summary.pdf\n","................................................................................\n","Content: \n","Adaptive Query Processing Full text available at: http://dx.doi.org/10.1561/1900000001\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Qi Long_Long_Qi.pdf\n","................................................................................\n","Content: \n","Bayesian Generalized Biclustering Analysis via Adaptive Structured Shrinkage1 Qi Long, Ph.D. Department of\n","Biostatistics, Epidemiology and Informatics Perelman School of Medicine University of Pennsylvania November 7, 2018\n","1Joint work with Ziyi Li, Changgee Chang, Suprateek Kundu\n","................................................................................\n","Response: Key Features of Steven Zdancemic's Work:  - Language Guided Bottlenecks (LaBo): A methodology for constructing high-\n","performance Concept Bottleneck Models (CBM) without manual specification. LaBo leverages a language model, GPT-3, to\n","define a large space of possible bottlenecks in CBMs. - GPT-3: A language model used in LaBo to generate factual\n","sentences about categories to form candidate concepts. - CLIP: A model used to align GPT-3's sentential concepts to\n","images in order to form a bottleneck layer in LaBo. - Inherently interpretable models: Steven Zdancemic's work focuses\n","on developing interpretable machine learning models, specifically CBMs, that factor model decisions into human-readable\n","concepts. - Visual recognition: LaBo bottlenecks excel at few-shot classification in visual recognition tasks. - High-\n","stakes applications: The interpretability of CBMs is crucial for high-stakes applications where it is important to\n","understand why a model is failing. - Machine learning and data science: Steven Zdancemic's work lies in the intersection\n","of machine learning, data science, natural language processing, and computer vision. - Performance comparison: LaBo\n","demonstrates that inherently interpretable models can achieve similar or better performance than black box approaches.\n","................................................................................\n","What are key features of Swapneel Seth's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Swapneel Seth's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","An Optimal Labeling Scheme for Workﬂow ProvenanceUsing Skeleton LabelsZhuowei Bao, Susan B. Davidson, Sanjeev Khanna, Sudeepa RoyDepartment of Computer and Information Science, University of PennsylvaniaPhiladelphia, PA 19104, USA{zhuowei, susan, sanjeev, sudeepa}@cis.upenn.eduABSTRACTWe develop a compact and eﬃcient reachability labelingscheme for answering provenance queries on workﬂow runsthat conform to a given speciﬁcation. Even though a work-ﬂow run can be structurally more complex and can be ar-bitrarily larger than the speciﬁcation due to fork (parallel)and loop executions, we show that a compact reachabilitylabeling for a run can be eﬃciently computed using the factthat it originates from a ﬁxed speciﬁcation. Our labelingscheme is optimal in the sense that it uses labels of logarith-mic length, runs in linear time, and answers any reachabil-ity query in constant time. Our approach is based on usingthe reachability labeling for the speciﬁcation as an eﬀectiveskeletonfor designing the reachability labeling for workﬂowruns. We also demonstrate empirically the eﬀectiveness ofour skeleton-based labeling approach.Categories and Subject DescriptorsH.2.8 [Database Management]: Database Applications—scientiﬁc databasesGeneral TermsAlgorithms, Performance1. INTRODUCTIONEﬃciently maintaining the provenance of data producedby scientiﬁc workﬂow systems is of great current interest.Using provenance information, scientists can examine thedata, parameter settings, and analysis tools that were usedin an “in-silico” experiment to produce a good data result,or determine which downstream data objects were aﬀectedby a bad data result. Tracking such provenance informationentails answering reachability queries on a directed acyclicgraph (DAG) which encodes the dependencies between dataand modules in a workﬂow execution.As observed in [8, 11], two immediate approaches to an-swering reachability queries – using graph traversals and pre-Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on the ﬁrst page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior speciﬁcpermission and/or a fee.SIGMOD’10,June 6–11, 2010, Indianapolis, Indiana, USA.Copyright 2010 ACM 978-1-4503-0032-2/10/06 ...$10.00.computing the transitive closure – are prohibitively expen-sive for large graphs. A better approach is to usereachabilitylabels,i.e., to assign each vertex in the graph a label suchthat by comparing only the labels of any two vertices, we candecide if one can reach the other. However, the eﬀectivenessof this approach crucially depends on the ability to developacompactandeﬃcientlabeling scheme, where compactnessrefers to the space used by the labels, and eﬃciency refers tothe time complexity of creating and comparing the labels.Motivated by applications in XML database systems, sev-eral compact and eﬃcient labeling schemes for trees havebeen developed; see, for example, [15, 13]. These label-ing schemes are generally considered to beoptimalin thesense that they use labels of logarithmic length, run in lin-ear time, and answer queries in constant time. In contrast,any labeling scheme for general DAGs may require labels oflinear length (in the number of vertices), even if arbitraryconstruction and query time are allowed.However, workﬂow executions are not arbitrary DAGs:Each execution (a.k.a.run) of a workﬂow follows the samebasic structure as the directed graph representing its speci-ﬁcation, but may become much larger due to fork (parallel)and loop executions. Building on this observation, we pro-pose askeleton-basedlabeling scheme, that uses the reacha-bility labeling for the speciﬁcation for designing the reacha-bility labeling for workﬂow runs. Our approach labels a runin two phases: (1) label its speciﬁcation using any labelingscheme for directed graphs, and (2) extend the reachabilitylabels on the speciﬁcation (called theskeleton labels) withadditional run-time information about the forking and loop-ing behavior to label the run.bcda(a) Workflow Specification (b) Workflow Runb1c1d1a1b3c3b2c2x1x2x3x4x5x8x6x7Figure 1:Workﬂow ExampleAs an example, consider the workﬂow speciﬁcation andone of its runs shown in Figure 1, in which vertices representmodules (a.k.a. tasks) and edges indicate potential data ﬂowbetween the modules. In the speciﬁcation, the dotted ovalaroundbandcindicates aforkand the dotted backarrowfromctobaloop. In the run, the fork is executed twice inparallel; in one fork copy, the loop is executed twice, whilein the other fork copy it is executed only once.1Edges inthe run are also annotated withxi’s, indicating data thatwas created by the ﬁrst module and passed to the second.1A realistic run may execute the fork and loop hundreds of times.711\n","\n","Data Provenance at Internet Scale:\n","Architecture, Experiences, and the Road Ahead\n","Ang Chen*\n","University of PennsylvaniaY ang Wu*\n","University of Pennsylvania\n","Andreas Haeberlen\n","University of PennsylvaniaBoon Thau Loo\n","University of PennsylvaniaWenchao Zhou\n","Georgetown University\n","ABSTRACT\n","Provenance is a way to answer “why” questions about computa-\n","tions. It has found a number of uses in the database community,\n","such as debugging query answers or tracing unexpected results to\n","database tuples. In fact, the ability to ask “why” can be useful\n","for a much broader range of applications. In this paper, we summa-\n","rize our experiences over the past few years in adapting provenance\n","for diagnostic and forensic uses in networks and distributed sys-\n","tems. Our work draws inspirations from database provenance, yet\n","the deployment scale, use cases, and distributed nature of networks\n","require a signiﬁcant re-design of traditional data provenance mod-\n","els. We review a number of use cases, ranging from investigating\n","intrusions to diagnosing (and even automatically ﬁxing) software-\n","deﬁned networks, and present a uniﬁed system architecture that we\n","have designed and implemented for provenance in distributed sys-\n","tems. We conclude with a discussion of open issues in this space.\n","1 Introduction\n","Provenance is, in essence, a way to answer “why” questions about\n","computations. It works by linking each effect, such as a computed\n","value, to its direct causes, such as the corresponding inputs and the\n","operation that was performed on them. Thus, it becomes possible\n","to recursively “explain” a particular result by showing its direct\n","causes, and then their own causes, and so on, until a set of basic\n","inputs is reached. This explanation is the provenance of the result:\n","it is a tree whose vertexes represent computations or data and whose\n","edges indicate direct causal connections.\n","Provenance originated in the database community [13], and it has\n","found a number of interesting uses, such as diagnosing query an-\n","swers [31], reverse data management [34], or tracking unexpected\n","results to speciﬁc tuples in the input data [32]. However, the con-\n","cept itself is not database-speciﬁc: it can potentially help in any\n","situation where a system has shown some unexpected behavior that\n","must now be investigated and tracked to a set of “root causes”. This\n","kind of situation is common in many areas of computer science.\n","Over the past several years, we have been working on network\n","provenance , which is a general class of solutions that adapt prove-\n","nance for diagnostic and forensic uses in computer networks and,\n","more generally, distributed systems. As with database applications,\n","*These authors contributed equally to this work.\n","This article is published under a Creative Commons Attribution License\n","(http://creativecommons.org/licenses/by/3.0/), which permits distribution\n","and reproduction in any medium as well allowing derivative works, pro-\n","vided that you attribute the original work to the author(s) and CIDR 2017.\n","8th Biennial Conference on Innovative Data Systems Research (CIDR ‘17)\n","January 8-11, 2017, Chaminade, California, USA.these systems frequently do something that their operators did not\n","expect. In the context of database applications, a SQL query may\n","have been written incorrectly, resulting in erroneous query results.\n","Likewise, in computer networks, a network operator may observe\n","unusual routes or dropped packets, which may be symptoms of er-\n","rors in network conﬁgurations, or worse still, bugs introduced by\n","intentional manipulation and even targeted attacks. And, just as\n","in the database world, the operators are faced with the challenging\n","problem of tracing the observed symptoms to a set of root causes.\n","One way to apply provenance to distributed systems is to essen-\n","tially model the distributed system as a giant database: the state of\n","each node can be stored in tables, and the programs can be mod-\n","eled as a set of declarative rules [29]. The provenance of each tuple\n","can then be tracked just like it would be in a database, and diag-\n","nostic queries (say, “Why is the network using route R to reach\n","host H?”) can be formulated as provenance queries (say, “What is\n","the provenance of route R?”), which then reveal the corresponding\n","root causes (say, a recent conﬁguration change).\n","Given the distributed nature of networks, we can partition and\n","distribute the state such that each node maintains only a portion\n","of the information required to reconstruct any tuple provenance,\n","rather than storing all the network state in a centralized database.\n","This was the approach we took in our ﬁrst solution [51], but it\n","quickly became clear that there were a number of additional chal-\n","lenges. One simple example is the fact that network state, unlike\n","tuples in traditional databases, tends to be short-lived: by the time\n","that the operator is notiﬁed of the problem with route R, that route\n","may already have been replaced by another. We solved this by\n","adding a temporal dimension to the provenance, so that questions\n","could be asked about past states [50]; however, this massively in-\n","creased the amount of metadata that needed to be kept, so we added\n","garbage collection and a range of strong compression techniques,\n","along with a cost model to choose the “right” technique for a given\n","system [50].\n","Other challenges were more fundamental, however. For instance,\n","one interesting application area is security. Given that the prove-\n","nance graph in our setting is stored in a distributed and open fash-\n","ion across administrative domains, the entire system is vulnerable\n","to attacks. The operator might wish to learn how an attacker “got\n","into” the system, or what changes she has made to it. However, if\n","the attacker has compromised the system, what prevents her from\n","tampering with the provenance and giving false or misleading re-\n","sponses? In another class of scenarios in network debugging, the\n","problem is not the presence of an unexpected event, but rather the\n","absence of an expected event; here, it is not immediately clear how\n","to even apply provenance, since there is no starting point for a pos-\n","sible explanation. Our solutions to both problems involve new data\n","structures, as well as substantial re-designs and reﬁnements of the\n","1\n","\n","Maximum Satisﬁability in Program Analysis:\n","Applications and Techniques\n","Mayur Naik1, Xujie Si1, Xin Zhang1, and Radu Grigore2\n","1University of Pennsylvania2University of Kent\n","Abstract. A central challenge in program analysis concerns balanc-\n","ing diﬀerent competing tradeoﬀs. To address this challenge, we propose\n","an approach based on the Maximum Satisﬁability (MaxSAT) problem,\n","an optimization extension of the Boolean Satisﬁability (SAT) problem.\n","We demonstrate the approach on three diverse applications that ad-\n","vance the state-of-the-art in balancing tradeoﬀs in program analysis.\n","Enabling these applications on real-world programs necessitates solv-\n","ing large MaxSAT instances comprising over 1030clauses in a sound and\n","optimal manner. We propose a general framework that scales to such\n","instances by iteratively expanding a subset of clauses while providing\n","soundness and optimality guarantees. We also present new techniques to\n","instantiate and optimize the framework.\n","Keywords: maximum satisﬁability, program analysis\n","\n","Incremental Offline/Online PIR (extended version)⋆\n","Yiping Ma§Ke Zhong§Tal Rabin§†Sebastian Angel§‡\n","§University of Pennsylvania†Algorand Foundation‡Microsoft Research\n","Abstract. Recent private information retrieval (PIR)\n","schemes preprocess the database with a query-independent\n","offline phase in order to achieve sublinear computation during\n","a query-specific online phase. These offline/online protocols\n","expand the set of applications that can profitably use PIR,\n","but they make a critical assumption: that the database is\n","immutable. In the presence of changes such as additions,\n","deletions, or updates, existing schemes must preprocess the\n","database from scratch, wasting prior effort. To address this,\n","we introduce incremental preprocessing for offline/online\n","PIR schemes, allowing the original preprocessing to continue\n","to be used after database changes, while incurring an update\n","cost proportional to the number of changes rather than\n","the size of the database. We adapt two offline/online PIR\n","schemes to use incremental preprocessing and show how it\n","significantly improves the throughput and reduces the latency\n","of applications where the database changes over time.\n","1 Introduction\n","Private information retrieval (PIR) [ 22] is an incredibly useful\n","cryptographic building block that allows a client to download\n","an object from a database without revealing which object\n","was fetched. PIR has many applications, including privacy-\n","preserving video streaming [ 3,35], password checking [ 4],\n","blocklists [ 40], ad delivery [ 34], friend discovery [ 12], sub-\n","scriptions [ 20], and anonymous messaging [ 6,42,47]. While\n","powerful, PIR is expensive: PIR imposes a computational\n","linear lower bound since the database must operate on all\n","objects in order to answer a query. After all, if even a single\n","object is omitted when answering a query this would leak that\n","this object is of no interest to the client.\n","The existence of this computational lower bound is unfor-\n","tunate, as it limits the scale of applications that can profitably\n","use PIR. But there is still hope! There is an active line of\n","work [ 5,8,15,18,23,45,50,51] that pursues the idea of\n","preprocessing the database to generate auxiliary information\n","orhints (which could be stored at the server or clients depend-\n","ing on the proposal) during an offline phase, and then use the\n","hints during an online phase to answer one or more queries\n","with sublinear computation and communication. This is an\n","exciting proposition, as it enables applications that require\n","quick answers for (online) queries to large databases, but that\n","can afford an expensive query-independent preprocessing.\n","⋆This is the full version of our paper [ 46]. It includes additional details,\n","experiments, and another PIR protocol (Appendices B–E).A major implicit assumption in all of the above works is\n","that the PIR database is immutable . That is, once it has been\n","preprocessed, no new items will be added, deleted, or updated.\n","Should the database change, under all existing schemes, it is\n","necessary to redo the expensive offline phase—defeating in\n","many cases the benefits of preprocessing. Meanwhile, many\n","of the proposed applications that use PIR naturally experi-\n","ence at least some moderate content churn. For example, Pop-\n","corn [ 35], which implements a private video service where\n","clients stream movies using PIR, must deal with movies being\n","added, deleted, or modified (e.g., to change codec) occasion-\n","ally. Similarly, anonymous messaging systems like Pung [ 6]\n","and Talek [ 20] have databases where new elements (messages)\n","are added every few minutes, while contact discovery services\n","like DP5 [12] have users creating and deleting accounts.\n","This paper extends offline/online PIR schemes to support\n","mutable databases where the content can change at any time\n","by introducing the notion of incremental preprocessing . The\n","result is an incremental offline/online PIR scheme where ad-\n","ditions, deletions, and edits to the database do not require a\n","complete preprocessing after every change. Instead, the hints\n","are updated at a cost that is proportional to the number of\n","changes. To demonstrate the feasibility and benefits of incre-\n","mental preprocessing, we extend two recent two-server of-\n","fline/online PIR protocols: (1) the Corrigan-Gibbs and Kogan\n","scheme (CK) [ 23], and (2) the Shi, Aqeel, Chandrasekaran,\n","and Maggs scheme (SACM) [ 51]. Our experimental evalua-\n","tion confirms that the savings of our approach are consider-\n","able. In a database with 1 million items, the computational\n","cost of updating the hints in our incremental CK scheme (iCK)\n","for a batch of 10,000 updates (additions, deletions, edits) is\n","56×cheaper than preprocessing from scratch; the savings are\n","even more pronounced when there are fewer updates. Fur-\n","thermore, an implementation of PIR-Tor [ 47] that uses our\n","iCK construction improves the throughput achieved by Tor\n","directory nodes by roughly 7 ×over an implementation of\n","PIR-Tor that uses a state-of-the-art 2-server PIR scheme [ 13].\n","Our extensions to make these schemes incremental, how-\n","ever, come at a modest cost. For iCK, the cost is a small\n","increase in storage at the client to keep additional auxiliary\n","material needed for the client to construct the right query\n","during the online phase, and a larger (though still sublinear\n","and concretely very efficient) queries during the online phase.\n","For our incremental version of SACM, called iSACM, the\n","cost is higher online communication (though still sublinear\n","in the size of the updated database), since our construction is\n","1\n","\n","Differential Provenance: Better Network Diagnostics\n","with Reference Events\n","Ang Chen\n","University of PennsylvaniaY ang Wu\n","University of Pennsylvania\n","Andreas Haeberlen\n","University of PennsylvaniaWenchao Zhou\n","Georgetown UniversityBoon Thau Loo\n","University of Pennsylvania\n","ABSTRACT\n","In this paper, we propose a new approach to diagnosing prob-\n","lems in complex networks. Our approach is based on the\n","insight that many of the trickiest problems are anomalies –\n","they affect only a small fraction of the trafﬁc (e.g., perhaps\n","a certain subnet), or they only manifest infrequently. Thus,\n","it is quite common for the network operator to have “exam-\n","ples” of both working and non-working trafﬁc readily avail-\n","able – perhaps a packet that was misrouted, and a similar\n","packet that was routed correctly. In this case, the cause of\n","the problem is likely to be wherever the two packets were\n","treated differently by the network.\n","We sketch the design of a network debugger that can lever-\n","age this information using a novel concept that we call dif-\n","ferential provenance . Like classical provenance, differen-\n","tial provenance tracks the causal connections between net-\n","work and conﬁguration states and the packets that were af-\n","fected by them; however, it can additionally reason about the\n","causes of any discrepancies between different provenances.\n","We have performed a case study in the context of software-\n","deﬁned networks, and our initial results are encouraging:\n","they suggest that differential provenance can often identify\n","the root cause of even very subtle network issues.\n","Categories and Subject Descriptors\n","D.2.5 [ Testing and debugging ]: Diagnostics\n","Keywords\n","Diagnostics, Debugging, Provenance\n","1. INTRODUCTION\n","Networks are not easy to get right. Despite the fact that re-\n","searchers have developed a wide range of tools for network\n","diagnostics [8, 16, 17, 11, 14, 15, 5], understanding the intri-\n","Permission to make digital or hard copies of all or part of this work for personal or\n","classroom use is granted without fee, provided that copies are not made or distributed\n","for proﬁt or commercial advantage and that copies bear this notice and the full citation\n","on the ﬁrst page. Copyrights for components of this work owned by others than the\n","author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n","republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\n","and/or a fee. Request permissions from Permissions@acm.org.\n","HotNets-XIV , November 16–17, 2015, Philadelphia, PA, USA.\n","Copyright is held by the owner/author(s). Publication rights licensed to ACM.\n","ACM 978-1-4503-4047-2/15/11 ...$15.00.\n","DOI: http://dx.doi.org/10.1145/2834050.2834111.cate relations between network events for a root-cause anal-\n","ysis is still challenging. This is perhaps especially true for\n","software-deﬁned networks (SDNs), which offers exceptional\n","ﬂexibility by introducing a programmable controller but also\n","further complicates reasoning about network faults.\n","Network provenance [21] is a promising candidate for un-\n","derstanding the details of network executions, as it provides\n","step-by-step explanations of network events. It encodes net-\n","work events as vertexes in a distributed database and their\n","causal relations as links; they form a provenance graph on\n","which an operator could issue queries that ask for an ex-\n","planation of a certain event. For example, an operator can\n","ask why a certain route exists in her router’s BGP table by\n","issuing a query on the vertex that represents this route in\n","the provenance graph; the answer to her query would be a\n","provenance tree rooted at that vertex, chronicling how the\n","route has been derived from other BGP events. A number of\n","provenance-based diagnostic tools have been developed re-\n","cently, including systems like ExSPAN [21], SNP [19], and\n","Y! [16]. Conceptually, these systems all provide variants of\n","the same capability: a way to ask for a comprehensive ex-\n","planation of a certain network event.\n","However, while a comprehensive explanation is certainly\n","useful for diagnosing a problem, it is not the same as ﬁnd-\n","ing the actual root causes . We illustrate the difference with\n","an analogy from everyday life: suppose Bob wants to know\n","why his bus arrives at 5:05pm, which is ﬁve minutes late. If\n","Bob had a provenance-based debugger, he could submit the\n","query “Why did my bus arrive at 5:05pm?”, and he would\n","get a comprehensive explanation, such as “The bus was dis-\n","patched at the terminal at 4:00pm, and arrived at stop A at\n","4:13pm; it departed from there at 4:15pm, and arrived at stop\n","B at 4:21pm; ... Finally, it departed from stop Z at 5:01pm,\n","and arrived at Bob’s platform at 5:05pm”. This is very dif-\n","ferent from what Bob really wanted to know; the actual root\n","cause might be something like “At stop G, the bus had to\n","wait for ﬁve minutes because of a trafﬁc jam”.\n","But suppose we allow Bob to instead ask about the differ-\n","ences between two events – perhaps “why did my bus arrive\n","at 5:05pm today and not at 5:00pm, like yesterday?”. Thus,\n","the debugger can omit those parts of the explanation that the\n","two events have in common, and instead focus on the (hope-\n","fully few) parts that caused the different outcomes. We argue\n","1\n","\n","Incremental Offline/Online PIR\n","Yiping Ma⋆Ke Zhong⋆Tal Rabin⋆†Sebastian Angel⋆‡\n","⋆University of Pennsylvania†Algorand Foundation‡Microsoft Research\n","Abstract. Recent private information retrieval (PIR)\n","schemes preprocess the database with a query-independent\n","offline phase in order to achieve sublinear computation during\n","a query-specific online phase. These offline/online protocols\n","expand the set of applications that can profitably use PIR,\n","but they make a critical assumption: that the database is\n","immutable. In the presence of changes such as additions,\n","deletions, or updates, existing schemes must preprocess the\n","database from scratch, wasting prior effort. To address this,\n","we introduce incremental preprocessing for offline/online\n","PIR schemes, allowing the original preprocessing to continue\n","to be used after database changes, while incurring an update\n","cost proportional to the number of changes rather than\n","the size of the database. We adapt two offline/online PIR\n","schemes to use incremental preprocessing and show how it\n","significantly improves the throughput and reduces the latency\n","of applications where the database changes over time.\n","1 Introduction\n","Private information retrieval (PIR) [ 22] is an incredibly useful\n","cryptographic building block that allows a client to download\n","an object from a database without revealing which object\n","was fetched. PIR has many applications, including privacy-\n","preserving video streaming [ 3,35], password checking [ 4],\n","blocklists [ 40], ad delivery [ 34], friend discovery [ 12], sub-\n","scriptions [ 20], and anonymous messaging [ 6,42,47]. While\n","powerful, PIR is expensive: PIR imposes a computational\n","linear lower bound since the database must operate on all\n","objects in order to answer a query. After all, if even a single\n","object is omitted when answering a query this would leak that\n","this object is of no interest to the client.\n","The existence of this computational lower bound is unfor-\n","tunate, as it limits the scale of applications that can profitably\n","use PIR. But there is still hope! There is an active line of\n","work [ 5,8,15,18,23,45,50,51] that pursues the idea of\n","preprocessing the database to generate auxiliary information\n","orhints (which could be stored at the server or clients depend-\n","ing on the proposal) during an offline phase, and then use the\n","hints during an online phase to answer one or more queries\n","with sublinear computation and communication. This is an\n","exciting proposition, as it enables applications that require\n","quick answers for (online) queries to large databases, but that\n","can afford an expensive query-independent preprocessing.\n","A major implicit assumption in all of the above works is\n","that the PIR database is immutable . That is, once it has been\n","preprocessed, no new items will be added, deleted, or updated.Should the database change, under all existing schemes, it is\n","necessary to redo the expensive offline phase—defeating in\n","many cases the benefits of preprocessing. Meanwhile, many\n","of the proposed applications that use PIR naturally experi-\n","ence at least some moderate content churn. For example, Pop-\n","corn [ 35], which implements a private video service where\n","clients stream movies using PIR, must deal with movies being\n","added, deleted, or modified (e.g., to change codec) occasion-\n","ally. Similarly, anonymous messaging systems like Pung [ 6]\n","and Talek [ 20] have databases where new elements (messages)\n","are added every few minutes, while contact discovery services\n","like DP5 [12] have users creating and deleting accounts.\n","This paper extends offline/online PIR schemes to support\n","mutable databases where the content can change at any time\n","by introducing the notion of incremental preprocessing . The\n","result is an incremental offline/online PIR scheme where ad-\n","ditions, deletions, and edits to the database do not require a\n","complete preprocessing after every change. Instead, the hints\n","are updated at a cost that is proportional to the number of\n","changes. To demonstrate the feasibility and benefits of incre-\n","mental preprocessing, we extend two recent two-server of-\n","fline/online PIR protocols: (1) the Corrigan-Gibbs and Kogan\n","scheme (CK) [ 23], and (2) the Shi, Aqeel, Chandrasekaran,\n","and Maggs scheme (SACM) [ 51]. Our experimental evalua-\n","tion confirms that the savings of our approach are consider-\n","able. In a database with 1 million items, the computational\n","cost of updating the hints in our incremental CK scheme (iCK)\n","for a batch of 10,000 updates (additions, deletions, edits) is\n","56×cheaper than preprocessing from scratch; the savings are\n","even more pronounced when there are fewer updates. Fur-\n","thermore, an implementation of PIR-Tor [ 47] that uses our\n","iCK construction improves the throughput achieved by Tor\n","directory nodes by roughly 7 ×over an implementation of\n","PIR-Tor that uses a state-of-the-art 2-server PIR scheme [ 13].\n","Our extensions to make these schemes incremental, how-\n","ever, come at a modest cost. For iCK, the cost is a small\n","increase in storage at the client to keep additional auxiliary\n","material needed for the client to construct the right query\n","during the online phase, and a larger (though still sublinear\n","and concretely very efficient) queries during the online phase.\n","For our incremental version of SACM, called iSACM, the\n","cost is higher online communication (though still sublinear\n","in the size of the updated database), since our construction is\n","not compatible with a specific cryptographic primitive used\n","in SACM (a private puncturable PRF [51]).\n","1\n","\n","SageDB: An Instance-Optimized Data Analytics System\n","Jialin Ding, Ryan Marcus∗, Andreas Kipf,\n","Vikram Nathan, Aniruddha Nrusimha, Kapil Vaidya, Alexander van Renen†, Tim Kraska\n","Massachusetts Institute of Technology∗University of Pennsylvania†Friedrich-Alexander-Universität Erlangen-Nürnberg\n","ABSTRACT\n","Moderndatasystemsaretypicallybothcomplexandgeneral-purpose.\n","They are complex because of the numerous internal knobs and pa-\n","rameters that users need to manually tune in order to achieve good\n","performance; they are general-purpose because they are designed to\n","handle diverse use cases, and therefore often do not achieve the best\n","possible performance for any specific use case. A recent trend aims\n","to tackle these pitfalls: instance-optimized systems are designed to\n","automatically self-adjust in order to achieve the best performance\n","for a specific use case, i.e., a dataset and query workload. Thus far,\n","the research community has focused on creating instance-optimized\n","database components, such as learned indexes and learned cardi-\n","nality estimators, which are evaluated in isolation. However, to the\n","best of our knowledge, there is no complete data system built with\n","instance-optimization as a foundational design principle.\n","In this paper, we present a progress report on SageDB, our effort\n","towards building the first instance-optimized data system. SageDB\n","synthesizes various instance-optimization techniques to automat-\n","ically specialize for a given use case, while simultaneously exposing\n","a simple user interface that places minimal technical burden on the\n","user. Our prototype outperforms a commercial cloud-based analytics\n","system by up to 3× on end-to-end query workloads and up to 250×\n","on individual queries. SageDB is an ongoing research effort, and we\n","highlight our lessons learned and key directions for future work.\n","PVLDB Reference Format:\n","JialinDing,RyanMarcus,AndreasKipf,VikramNathan,AniruddhaNrusimha,\n","Kapil Vaidya, Alexander van Renen, Tim Kraska. SageDB: An Instance-\n","Optimized Data Analytics System. PVLDB, 15(13): 4062 - 4078, 2022.\n","doi:10.14778/3565838.3565857\n","1 INTRODUCTION\n","Most modern data management systems fall on a spectrum be-\n","tween general-purpose and application-specific. For example, Post-\n","greSQL [ 8] is extremely general purpose, and powers a diverse range\n","of analytical and transactional workloads. Apache Spark is slightly\n","specialized towards analytic tasks, but can still handle a wide variety\n","of use cases (e.g., batch reporting, ad-hoc interactive queries, data\n","science, and ML) and low-level workloads (e.g., I/O-bound, CPU-\n","bound, in-memory, on-disk, in the cloud). On the other hand, systems\n","like Google’s Mesa [ 30] and Napa [ 10] were custom-built to power\n","Google Ads, and are not suitable for any other application. While\n","these systems improve efficiency, these bespoke systems require\n","This work is licensed under the Creative Commons BY-NC-ND 4.0 International\n","License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\n","this license. For any use beyond those covered by this license, obtain permission by\n","emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\n","licensed to the VLDB Endowment.\n","Proceedings of the VLDB Endowment, Vol. 15, No. 13 ISSN 2150-8097.\n","doi:10.14778/3565838.3565857years of intense engineering effort and are only achievable by large\n","corporations with significant resources.\n","Ideally, users should be able to have the efficiency of special-\n","ized systems along with the flexibility of general-purpose systems.\n","Tuning configuration options (\"knobs\") is easier than building an\n","entirely new system, and can bridge some of the performance gap.\n","However, experienced engineers and database administrators still\n","go through the time-consuming and error-prone tuning process for\n","each application. Recent research proposes techniques for automatic\n","knob tuning [15]; however, the performance impact of tuning such\n","knobs is still limited. For example, users can only adjust the size\n","of a data block, not how data is laid out on disk. Fundamentally,\n","general-purpose systems are designed to be task agnostic, so for\n","most tasks a tuned general-purpose system will perform worse than\n","a custom-tailored system.\n","Recent work has shown that existing system components can be\n","replaced with instance-optimized orlearned components, which are\n","able to automatically adjust to a specific use case and workload (see\n","[5] for an overview). For example, learned index structures [24, 37]\n","offer the same read functionality as traditional index structures (e.g.\n","B+ trees) while providing better performance in both latency and\n","space consumption. Instance-optimized data storage layouts [ 63]\n","are able to improve scan performance by skipping data with greater\n","effectiveness than traditional sorting-based partitioning techniques.\n","However,theseinstance-optimizedcomponentshavelargelybeen\n","designed and evaluated in isolation, and there have only been a few\n","efforts to integrate them into an end-to-end system. Bourbon [ 19]\n","replaces block indexes in an LSM-tree with learned indexes and\n","demonstrates latency improvements. Google integrated learned in-\n","dexes into BigTable [ 9] with similar findings, mainly due to a smaller\n","index footprint and fewer cache misses when traversing the index.\n","While these are useful initial studies, it is still unclear how multiple\n","instance-optimized components would work together in concert. In\n","fact, it is easy to imagine a number of learned components destruc-\n","tively interfering with each other. Is it possible to build a system that\n","autonomously custom-tailors its major components to the user’s\n","requirements, approaching the performance of a bespoke system\n","but with similar ease of use as a general-purpose system?\n","To the best of our knowledge, there is no end-to-end data system\n","built with instance-optimization as a foundational design principle.\n","We previously presented our vision and blueprint for such a system,\n","called SageDB [ 36]. In this paper, we present our first prototype of\n","SageDB, and show how two carefully selected components can work\n","together in practice. These instance-optimized components are (1)\n","(multi-dimensional) data layouts and data replication and (2) partial\n","materialized views. These techniques minimize I/O when scanning\n","data from disk and maximize computation reuse through intelligent\n","pre-materialization of partial results. While the ultimate goal is to\n","automatically trigger self-optimization whenever necessary, for the\n","4062\n","\n","Secure Network Provenance\n","Wenchao Zhou\n","University of PennsylvaniaQiong Fei\n","University of PennsylvaniaArjun Narayan\n","University of Pennsylvania\n","Andreas Haeberlen\n","University of PennsylvaniaBoon Thau Loo\n","University of PennsylvaniaMicah Sherr\n","Georgetown University\n","ABSTRACT\n","This paper introduces secure network provenance (SNP) , a\n","novel technique that enables networked systems to explain to\n","their operators whythey are in a certain state – e.g., why a\n","suspicious routing table entry is present on a certain router,\n","or where a given cache entry originated. SNP provides net-\n","work forensics capabilities by permitting operators to track\n","down faulty or misbehaving nodes, and to assess the damage\n","such nodes may have caused to the rest of the system. SNP\n","is designed for adversarial settings and is robust to manip-\n","ulation; its tamper-evident properties ensure that operators\n","can detect when compromised nodes lie or falsely implicate\n","correct nodes.\n","We also present the design of SNooPy , a general-purpose\n","SNP system. To demonstrate that SNooPy is practical,\n","we apply it to three example applications: the Quagga\n","BGP daemon, a declarative implementation of Chord, and\n","Hadoop MapReduce. Our results indicate that SNooPy can\n","eﬃciently explain state in an adversarial setting, that it can\n","be applied with minimal eﬀort, and that its costs are low\n","enough to be practical.\n","Categories and Subject Descriptors\n","C.2.4 [ Computer Systems Organization ]: Computer-\n","Communication Networks— Distributed Systems ; D.4.5\n","[Software ]: Operating Systems— Reliability\n","General Terms\n","Algorithms, Design, Reliability, Security\n","Keywords\n","Accountability, Byzantine faults, Distributed systems, Evi-\n","dence, Provenance, Security\n","1. INTRODUCTION\n","Operators of distributed systems often ﬁnd themselves need-\n","ing to answer a diagnostic or forensic question. Some part\n","of the system is found to be in an unexpected state – for\n","example, a suspicious routing table entry is discovered, or aproxy cache is found to contain an unusually large number of\n","advertisements. The operators must determine the causes of\n","this state before they can decide on an appropriate response.\n","On the one hand, there may be an innocent explanation: the\n","routing table entry could be the result of a misconﬁguration,\n","and the cache entries could have appeared due to a workload\n","change. On the other hand, the unexpected state may be the\n","symptom of an ongoing attack: the routing table entry could\n","be the result of route hijacking, and the cache entries could\n","be a side-eﬀect of a malware infection. If an attack or mis-\n","conﬁguration is discovered, the operators must determine its\n","eﬀects , such as corrupted state or conﬁguration changes on\n","other nodes, so that these nodes can be repaired and the\n","system brought back to a correct state.\n","In this paper, we consider forensics in an adversarial set-\n","ting, that is, we assume that a faulty node does not neces-\n","sarily crash but can also change its behavior and continue\n","operating. To be conservative, we assume that faults can\n","be Byzantine [ 24], i.e., a faulty node can behave arbitrar-\n","ily. This covers a wide range of faults and misbehavior, e.g.,\n","cases where a malicious adversary has compromised some of\n","the nodes, but also more benign faults, such as hardware\n","failures or misconﬁgurations. Getting correct answers to\n","forensic queries in an adversarial setting is diﬃcult because\n","the misbehaving nodes can lie to the querier. For example,\n","the adversary can attempt to conceal his actions by causing\n","his nodes to fabricate plausible (but incorrect) responses to\n","forensic queries, or he can attempt to frame correct nodes\n","by returning responses that blame his own misbehavior on\n","them. Thus, the adversary can gain valuable time by mis-\n","directing the operators and/or causing them to suspect a\n","problem with the forensic system itself.\n","Existing forensic systems are either designed for non-\n","adversarial settings [ 43,51] or require some trusted com-\n","ponents, e.g., a trusted virtual-machine monitor [ 3,21], a\n","trusted host-level monitor [ 27], a trusted OS [ 29], or trusted\n","hardware [ 7]. However, most components that are avail-\n","able today are not fully trustworthy; OSes and virtual ma-\n","chine monitors have bugs, which a powerful adversary could\n","exploit, and even trusted hardware is sometimes compro-\n","mised [ 20]. We argue that it is useful to have alternative\n","techniques available that do not require this type of trust.\n","We introduce secure network provenance (SNP) , a tech-\n","nique for building forensic systems that can operate in a\n","completely untrusted environment. We assume that the ad-\n","versary may have compromised an arbitrary subset of the\n","nodes, and that he may have complete control over these\n","nodes. On the one hand, this very conservative threat model\n","requires some compromises: an SNP system can only answer\n","\n","The Good, the Bad, and the Differences:\n","Better Network Diagnostics with\n","Differential Provenance\n","Ang Chen\n","University of PennsylvaniaY ang Wu\n","University of Pennsylvania\n","Andreas Haeberlen\n","University of PennsylvaniaWenchao Zhou\n","Georgetown UniversityBoon Thau Loo\n","University of Pennsylvania\n","ABSTRACT\n","In this paper, we propose a new approach to diagnosing prob-\n","lems in distributed systems. Our approach is based on the in-\n","sight that many of the trickiest problems are anomalies. For\n","instance, in a network, problems often affect only a small\n","fraction of the trafﬁc (perhaps a certain subnet), or they only\n","manifest infrequently. Thus, it is quite common for the op-\n","erator to have “examples” of both working and non-working\n","trafﬁc readily available – perhaps a packet that was mis-\n","routed, and a similar packet that was routed correctly. In\n","this case, the cause of the problem is likely to be wherever\n","the two packets were treated differently by the network.\n","We present the design of a debugger that can leverage this\n","information using a novel concept that we call differential\n","provenance . Differential provenance tracks the causal con-\n","nections between network states and state changes, just like\n","classical provenance, but it can additionally perform root-\n","cause analysis by reasoning about the differences between\n","two provenance trees. We have built a diagnostic tool that\n","is based on differential provenance, and we have used our\n","tool to debug a number of complex, realistic problems in\n","two scenarios: software-deﬁned networks and MapReduce\n","jobs. Our results show that differential provenance can de-\n","liver very concise diagnostic information; in many cases, it\n","can even identify the precise root cause of the problem.\n","CCS Concepts\n","•Networks→Network management; Network experimen-\n","tation;•Information systems →Data provenance;\n","Keywords\n","Network diagnostics, debugging, provenance\n","Permission to make digital or hard copies of all or part of this work for personal\n","or classroom use is granted without fee provided that copies are not made or\n","distributed for proﬁt or commercial advantage and that copies bear this notice\n","and the full citation on the ﬁrst page. Copyrights for components of this work\n","owned by others than the author(s) must be honored. Abstracting with credit is\n","permitted. To copy otherwise, or republish, to post on servers or to redistribute to\n","lists, requires prior speciﬁc permission and/or a fee. Request permissions from\n","permissions@acm.org.\n","SIGCOMM ’16, August 22 - 26, 2016, Florianópolis, Brazil\n","c⃝2016 Copyright held by the owner/author(s). Publication rights licensed to\n","ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00\n","DOI: http://dx.doi.org/10.1145/2934872.29349101. INTRODUCTION\n","Distributed systems are not easy to get right. Despite the fact\n","that researchers have developed a wide range of diagnostic\n","tools [16, 30, 31, 19, 27, 29, 10], understanding the intricate\n","relations between low-level events, which is needed for root-\n","cause analysis, is still challenging.\n","Recent work on data provenance [36] has provided a new\n","approach to understanding the details of distributed execu-\n","tions. Intuitively, a provenance system keeps track of the\n","causal connections between the states and events that a sys-\n","tem generates at runtime; for instance, when applied to a\n","software-deﬁned network (SDN), it might associate each ﬂow\n","entry with the parts of the controller program that were used\n","to compute it. Then, when the operator asks a diagnostic\n","question – say, why a certain packet was routed to a par-\n","ticular host – the system returns a comprehensive explana-\n","tion that recursively explains each relevant event in terms of\n","its direct causes. A number of provenance-based diagnostic\n","tools have been developed recently, including systems like\n","ExSPAN [36], SNP [34], and Y! [30].\n","However, while such a comprehensive explanation is use-\n","ful for diagnosing a problem, it is not the same as ﬁnding the\n","actual root causes . We illustrate the difference with an ana-\n","logy from everyday life: suppose Bob wants to know why\n","his bus arrived at 5:05pm, which is ﬁve minutes late. If\n","Bob had a provenance-based debugger, he could submit the\n","query “Why did my bus arrive at 5:05pm?”, and he would\n","get a comprehensive explanation, such as “The bus was dis-\n","patched at the terminal at 4:00pm, and arrived at stop A at\n","4:13pm; it departed from there at 4:15pm, and arrived at stop\n","B at 4:21pm; ... Finally, it departed from stop Z at 5:01pm,\n","and arrived at Bob’s platform at 5:05pm”. This is very dif-\n","ferent from what Bob really wanted to know: the actual root\n","cause might be something like “At stop G, the bus had to\n","wait for ﬁve minutes because of a trafﬁc jam”.\n","But suppose we allow Bob to instead ask about the dif-\n","ferences between two events – perhaps “Why did my bus\n","arrive at 5:05pm today, and not at 5:00pm like yesterday?”.\n","The debugger can then omit those parts of the explanation\n","that the two events have in common, and instead focus on\n","the (hopefully few) parts that caused the different outcomes.\n","We argue that a similar approach should work for diagnos-\n","1\n","\n","Putting Lipstick on Pig:\n","Enabling Database­style Workﬂow Provenance\n","Y ael Amsterdamer2, Susan B. Davidson1, Daniel Deutch3, Tova Milo2, Julia Stoyanovich1,\n","Val Tannen1\n","1University of Pennsylvania, USA2Tel Aviv University, Israel3Ben Gurion University, Israel\n","{susan, jstoy, val }@cis.upenn.edu {yaelamst, milo }@cs.tau.ac.il deutchd@cs.bgu.ac.il\n","ABSTRACT\n","Workﬂow provenance typically assumes that each module\n","is a “black-box”, so that each output depends on all in-\n","puts ( coarse-grained dependencies). Furthermore, it does\n","not model the internal state of a module, which can change\n","between repeated executions. In practice, however, an out-\n","put may depend on only a small subset of the inputs ( ﬁne-\n","grained dependencies) as well as on the internal state of\n","the module. We present a novel provenance framework that\n","marries database-style and workﬂow-style provenance, by\n","using Pig Latin to expose the functionality of modules, thus\n","capturing internal state and ﬁne-grained dependencies. A\n","critical ingredient in our solution is the use of a novel form of\n","provenance graph that models module invocations and yields\n","a compact representation of ﬁne-grained workﬂow prove-\n","nance. It also enables a number of novel graph transforma-\n","tion operations, allowing to choose the desired level of gra n-\n","ularity in provenance querying (ZoomIn and ZoomOut), and\n","supporting “what-if” workﬂow analytic queries. We imple-\n","mented our approach in the Lipstick system and developed\n","a benchmark in support of a systematic performance eval-\n","uation. Our results demonstrate the feasibility of trackin g\n","and querying ﬁne-grained workﬂow provenance.\n","1. INTRODUCTION\n","Data-intensive application domains such as science and\n","electronic commerce are increasingly using workﬂow system s\n","to design and manage the analysis of large datasets and to\n","track the provenance of intermediate and ﬁnal data prod-\n","ucts. Provenance is extremely important for veriﬁability\n","and repeatability of results, as well as for debugging and\n","trouble-shooting workﬂows [10, 11].\n","The standard assumption for workﬂow provenance is that\n","each module is a “black-box”, so that each output of the\n","module depends on all its inputs ( coarse-grained dependen-\n","cies). This model is problematic since it cannot account for\n","common situations in which an output item depends onlyon a small subset of the inputs ( ﬁne-grained dependencies).\n","For example, the module function may be mapped over an\n","input list, so that the ithelement of the output list depends\n","only on the ithelement of the input list (see Taverna [18,\n","29]). Furthermore, the model does not capture the internal\n","state of a module, which may be modiﬁed by inputs seen\n","in previous executions of the workﬂow (e.g., a learning al-\n","gorithm), and an output may depend on some (but not all)\n","of these previous inputs. Maintaining an “output depends\n","on all inputs” assumption quickly leads to a very coarse ap-\n","proximation of the actual data dependencies that exist in\n","an execution of the workﬂow; furthermore, it does not show\n","the way in which these dependencies arise.\n","For example, consider the car dealership workﬂow shown\n","in Figure 1. The execution starts with a buyer providing her\n","identiﬁer and the car model of interest to a bid request mod-\n","ule that distributes the request to several car dealer mod-\n","ules. Each dealer looks in its database for how many cars of\n","the requested model are available, how many sales of that\n","model have recently been made, and whether the buyer pre-\n","viously made a request for this model, and, based on this\n","information, generates a bid and records it in its database\n","state. Bids are directed to an aggregator module that cal-\n","culates the best (minimum) bid. The user then makes a\n","choice to accept or decline the bid; if the bid is accepted,\n","the relevant dealership is notiﬁed to ﬁnalize the purchase.\n","If the user declines the bid but requests the same car model\n","in a subsequent execution, each dealer will consult its bid\n","history and will generate a bid of the same or lower amount.\n","Coarse-grained provenance for this workﬂow would show\n","the information that was given by the user to the bid re-\n","quest module, the bids that were produced by each dealer\n","and given as input to the aggregator, the choice that the\n","user made, and which dealer made a sale (if any). However,\n","it would not show the dependence of the bid on the cars\n","that were available at the time of the request, on relevant\n","sale history, and on previous bids. Thus, queries such as\n","“Was the sale of this VW Jetta aﬀected by the presence of a\n","Honda Civic in the dealership’s lot?”,“Which cars aﬀected\n","the computation of this winning bid?”, and “Had this Toy-\n","ota Prius not been present, would its dealer still have made a\n","sale?” would not be supported. Coarse-grained provenance\n","would also not give detailed information about how the best\n","bid was calculated (a minimum aggregate).\n","Finer-grained provenance has been well-studied in databas e\n","research. In particular, a framework based on semiring an-\n","notations has been proposed [17], in which every tuple of\n","the database is annotated with an element of a provenance\n","Permission to make digital or hard copies of all or part of this work for\n","personal or classroom use is granted without fee provided that copies are\n","not made or distributed for proﬁt or commercial advantage and that copies\n","bear this notice and the full citation on the ﬁrst page. To copy otherwise, to\n","republish, to post on servers or to redistribute to lists, requires prior speciﬁc\n","permission and/or a fee. Articles from this volume were invited to present\n","their results at The 38th International Conference on Very Large Data Bases,\n","August 27th - 31st 2012, Istanbul, Turkey.\n","Proceedings of the VLDB Endowment, V ol. 5, No. 4\n","Copyright 2011 VLDB Endowment 2150-8097/11/12... $10.00.\n","346\n","arXiv:1201.0231v1  [cs.DB]  31 Dec 2011:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Swapneel Seth's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Sanjeev Khanna_SIGMOD10_Labeling.pdf\n","................................................................................\n","Content: \n","An Optimal Labeling Scheme for Workﬂow ProvenanceUsing Skeleton LabelsZhuowei Bao, Susan B. Davidson, Sanjeev Khanna,\n","Sudeepa RoyDepartment of Computer and Information Science, University of PennsylvaniaPhiladelphia, PA 19104,\n","USA{zhuowei, susan, sanjeev, sudeepa}@cis.upenn.eduABSTRACTWe develop a compact and eﬃcient reachability labelingscheme\n","for answering provenance queries on workﬂow runsthat conform to a given speciﬁcation. Even though a work-ﬂow run can be\n","structurally more complex and can be ar-bitrarily larger than the speciﬁcation due to fork (parallel)and loop\n","executions, we show that a compact reachabilitylabeling for a run can be eﬃciently computed using the factthat it\n","originates from a ﬁxed speciﬁcation. Our labelingscheme is optimal in the sense that it uses labels of logarith-mic\n","length, runs in linear time, and answers any reachabil-ity query in constant time. Our approach is based on usingthe\n","reachability labeling for the speciﬁcation as an eﬀectiveskeletonfor designing the reachability labeling for\n","workﬂowruns. We also demonstrate empirically the eﬀectiveness ofour skeleton-based labeling approach.Categories and\n","Subject DescriptorsH.2.8 [Database Management]: Database Applications—scientiﬁc databasesGeneral TermsAlgorithms,\n","Performance1. INTRODUCTIONEﬃciently maintaining the provenance of data producedby scientiﬁc workﬂow systems is of great\n","current interest.Using provenance information, scientists can examine thedata, parameter settings, and analysis tools\n","that were usedin an “in-silico” experiment to produce a good data result,or determine which downstream data objects were\n","aﬀectedby a bad data result. Tracking such provenance informationentails answering reachability queries on a directed\n","acyclicgraph (DAG) which encodes the dependencies between dataand modules in a workﬂow execution.As observed in [8, 11],\n","two immediate approaches to an-swering reachability queries – using graph traversals and pre-Permission to make digital\n","or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies\n","arenot made or distributed for proﬁt or commercial advantage and that copiesbear this notice and the full citation on\n","the ﬁrst page. To copy otherwise, torepublish, to post on servers or to redistribute to lists, requires prior\n","speciﬁcpermission and/or a fee.SIGMOD’10,June 6–11, 2010, Indianapolis, Indiana, USA.Copyright 2010 ACM\n","978-1-4503-0032-2/10/06 ...$10.00.computing the transitive closure – are prohibitively expen-sive for large graphs. A\n","better approach is to usereachabilitylabels,i.e., to assign each vertex in the graph a label suchthat by comparing only\n","the labels of any two vertices, we candecide if one can reach the other. However, the eﬀectivenessof this approach\n","crucially depends on the ability to developacompactandeﬃcientlabeling scheme, where compactnessrefers to the space used\n","by the labels, and eﬃciency refers tothe time complexity of creating and comparing the labels.Motivated by applications\n","in XML database systems, sev-eral compact and eﬃcient labeling schemes for trees havebeen developed; see, for example,\n","[15, 13]. These label-ing schemes are generally considered to beoptimalin thesense that they use labels of logarithmic\n","length, run in lin-ear time, and answer queries in constant time. In contrast,any labeling scheme for general DAGs may\n","require labels oflinear length (in the number of vertices), even if arbitraryconstruction and query time are\n","allowed.However, workﬂow executions are not arbitrary DAGs:Each execution (a.k.a.run) of a workﬂow follows the samebasic\n","structure as the directed graph representing its speci-ﬁcation, but may become much larger due to fork (parallel)and\n","loop executions. Building on this observation, we pro-pose askeleton-basedlabeling scheme, that uses the reacha-bility\n","labeling for the speciﬁcation for designing the reacha-bility labeling for workﬂow runs. Our approach labels a runin two\n","phases: (1) label its speciﬁcation using any labelingscheme for directed graphs, and (2) extend the reachabilitylabels\n","on the speciﬁcation (called theskeleton labels) withadditional run-time information about the forking and loop-ing\n","behavior to label the run.bcda(a) Workflow Specification (b) Workflow Runb1c1d1a1b3c3b2c2x1x2x3x4x5x8x6x7Figure\n","1:Workﬂow ExampleAs an example, consider the workﬂow speciﬁcation andone of its runs shown in Figure 1, in which\n","vertices representmodules (a.k.a. tasks) and edges indicate potential data ﬂowbetween the modules. In the speciﬁcation,\n","the dotted ovalaroundbandcindicates aforkand the dotted backarrowfromctobaloop. In the run, the fork is executed twice\n","inparallel; in one fork copy, the loop is executed twice, whilein the other fork copy it is executed only once.1Edges\n","inthe run are also annotated withxi’s, indicating data thatwas created by the ﬁrst module and passed to the second.1A\n","realistic run may execute the fork and loop hundreds of times.711\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Andreas Haeberlen_provenance-cidr2017.pdf\n","................................................................................\n","Content: \n","Data Provenance at Internet Scale: Architecture, Experiences, and the Road Ahead Ang Chen* University of PennsylvaniaY\n","ang Wu* University of Pennsylvania Andreas Haeberlen University of PennsylvaniaBoon Thau Loo University of\n","PennsylvaniaWenchao Zhou Georgetown University ABSTRACT Provenance is a way to answer “why” questions about computa-\n","tions. It has found a number of uses in the database community, such as debugging query answers or tracing unexpected\n","results to database tuples. In fact, the ability to ask “why” can be useful for a much broader range of applications. In\n","this paper, we summa- rize our experiences over the past few years in adapting provenance for diagnostic and forensic\n","uses in networks and distributed sys- tems. Our work draws inspirations from database provenance, yet the deployment\n","scale, use cases, and distributed nature of networks require a signiﬁcant re-design of traditional data provenance mod-\n","els. We review a number of use cases, ranging from investigating intrusions to diagnosing (and even automatically ﬁxing)\n","software- deﬁned networks, and present a uniﬁed system architecture that we have designed and implemented for provenance\n","in distributed sys- tems. We conclude with a discussion of open issues in this space. 1 Introduction Provenance is, in\n","essence, a way to answer “why” questions about computations. It works by linking each effect, such as a computed value,\n","to its direct causes, such as the corresponding inputs and the operation that was performed on them. Thus, it becomes\n","possible to recursively “explain” a particular result by showing its direct causes, and then their own causes, and so\n","on, until a set of basic inputs is reached. This explanation is the provenance of the result: it is a tree whose\n","vertexes represent computations or data and whose edges indicate direct causal connections. Provenance originated in the\n","database community [13], and it has found a number of interesting uses, such as diagnosing query an- swers [31], reverse\n","data management [34], or tracking unexpected results to speciﬁc tuples in the input data [32]. However, the con- cept\n","itself is not database-speciﬁc: it can potentially help in any situation where a system has shown some unexpected\n","behavior that must now be investigated and tracked to a set of “root causes”. This kind of situation is common in many\n","areas of computer science. Over the past several years, we have been working on network provenance , which is a general\n","class of solutions that adapt prove- nance for diagnostic and forensic uses in computer networks and, more generally,\n","distributed systems. As with database applications, *These authors contributed equally to this work. This article is\n","published under a Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits\n","distribution and reproduction in any medium as well allowing derivative works, pro- vided that you attribute the\n","original work to the author(s) and CIDR 2017. 8th Biennial Conference on Innovative Data Systems Research (CIDR ‘17)\n","January 8-11, 2017, Chaminade, California, USA.these systems frequently do something that their operators did not\n","expect. In the context of database applications, a SQL query may have been written incorrectly, resulting in erroneous\n","query results. Likewise, in computer networks, a network operator may observe unusual routes or dropped packets, which\n","may be symptoms of er- rors in network conﬁgurations, or worse still, bugs introduced by intentional manipulation and\n","even targeted attacks. And, just as in the database world, the operators are faced with the challenging problem of\n","tracing the observed symptoms to a set of root causes. One way to apply provenance to distributed systems is to essen-\n","tially model the distributed system as a giant database: the state of each node can be stored in tables, and the\n","programs can be mod- eled as a set of declarative rules [29]. The provenance of each tuple can then be tracked just like\n","it would be in a database, and diag- nostic queries (say, “Why is the network using route R to reach host H?”) can be\n","formulated as provenance queries (say, “What is the provenance of route R?”), which then reveal the corresponding root\n","causes (say, a recent conﬁguration change). Given the distributed nature of networks, we can partition and distribute\n","the state such that each node maintains only a portion of the information required to reconstruct any tuple provenance,\n","rather than storing all the network state in a centralized database. This was the approach we took in our ﬁrst solution\n","[51], but it quickly became clear that there were a number of additional chal- lenges. One simple example is the fact\n","that network state, unlike tuples in traditional databases, tends to be short-lived: by the time that the operator is\n","notiﬁed of the problem with route R, that route may already have been replaced by another. We solved this by adding a\n","temporal dimension to the provenance, so that questions could be asked about past states [50]; however, this massively\n","in- creased the amount of metadata that needed to be kept, so we added garbage collection and a range of strong\n","compression techniques, along with a cost model to choose the “right” technique for a given system [50]. Other\n","challenges were more fundamental, however. For instance, one interesting application area is security. Given that the\n","prove- nance graph in our setting is stored in a distributed and open fash- ion across administrative domains, the\n","entire system is vulnerable to attacks. The operator might wish to learn how an attacker “got into” the system, or what\n","changes she has made to it. However, if the attacker has compromised the system, what prevents her from tampering with\n","the provenance and giving false or misleading re- sponses? In another class of scenarios in network debugging, the\n","problem is not the presence of an unexpected event, but rather the absence of an expected event; here, it is not\n","immediately clear how to even apply provenance, since there is no starting point for a pos- sible explanation. Our\n","solutions to both problems involve new data structures, as well as substantial re-designs and reﬁnements of the 1\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Mayur Naik_vmcai18.pdf\n","................................................................................\n","Content: \n","Maximum Satisﬁability in Program Analysis: Applications and Techniques Mayur Naik1, Xujie Si1, Xin Zhang1, and Radu\n","Grigore2 1University of Pennsylvania2University of Kent Abstract. A central challenge in program analysis concerns\n","balanc- ing diﬀerent competing tradeoﬀs. To address this challenge, we propose an approach based on the Maximum\n","Satisﬁability (MaxSAT) problem, an optimization extension of the Boolean Satisﬁability (SAT) problem. We demonstrate the\n","approach on three diverse applications that ad- vance the state-of-the-art in balancing tradeoﬀs in program analysis.\n","Enabling these applications on real-world programs necessitates solv- ing large MaxSAT instances comprising over\n","1030clauses in a sound and optimal manner. We propose a general framework that scales to such instances by iteratively\n","expanding a subset of clauses while providing soundness and optimality guarantees. We also present new techniques to\n","instantiate and optimize the framework. Keywords: maximum satisﬁability, program analysis\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Tal Rabin_2021-1438.pdf\n","................................................................................\n","Content: \n","Incremental Offline/Online PIR (extended version)⋆ Yiping Ma§Ke Zhong§Tal Rabin§†Sebastian Angel§‡ §University of\n","Pennsylvania†Algorand Foundation‡Microsoft Research Abstract. Recent private information retrieval (PIR) schemes\n","preprocess the database with a query-independent offline phase in order to achieve sublinear computation during a query-\n","specific online phase. These offline/online protocols expand the set of applications that can profitably use PIR, but\n","they make a critical assumption: that the database is immutable. In the presence of changes such as additions,\n","deletions, or updates, existing schemes must preprocess the database from scratch, wasting prior effort. To address\n","this, we introduce incremental preprocessing for offline/online PIR schemes, allowing the original preprocessing to\n","continue to be used after database changes, while incurring an update cost proportional to the number of changes rather\n","than the size of the database. We adapt two offline/online PIR schemes to use incremental preprocessing and show how it\n","significantly improves the throughput and reduces the latency of applications where the database changes over time. 1\n","Introduction Private information retrieval (PIR) [ 22] is an incredibly useful cryptographic building block that allows\n","a client to download an object from a database without revealing which object was fetched. PIR has many applications,\n","including privacy- preserving video streaming [ 3,35], password checking [ 4], blocklists [ 40], ad delivery [ 34],\n","friend discovery [ 12], sub- scriptions [ 20], and anonymous messaging [ 6,42,47]. While powerful, PIR is expensive: PIR\n","imposes a computational linear lower bound since the database must operate on all objects in order to answer a query.\n","After all, if even a single object is omitted when answering a query this would leak that this object is of no interest\n","to the client. The existence of this computational lower bound is unfor- tunate, as it limits the scale of applications\n","that can profitably use PIR. But there is still hope! There is an active line of work [ 5,8,15,18,23,45,50,51] that\n","pursues the idea of preprocessing the database to generate auxiliary information orhints (which could be stored at the\n","server or clients depend- ing on the proposal) during an offline phase, and then use the hints during an online phase to\n","answer one or more queries with sublinear computation and communication. This is an exciting proposition, as it enables\n","applications that require quick answers for (online) queries to large databases, but that can afford an expensive query-\n","independent preprocessing. ⋆This is the full version of our paper [ 46]. It includes additional details, experiments,\n","and another PIR protocol (Appendices B–E).A major implicit assumption in all of the above works is that the PIR database\n","is immutable . That is, once it has been preprocessed, no new items will be added, deleted, or updated. Should the\n","database change, under all existing schemes, it is necessary to redo the expensive offline phase—defeating in many cases\n","the benefits of preprocessing. Meanwhile, many of the proposed applications that use PIR naturally experi- ence at least\n","some moderate content churn. For example, Pop- corn [ 35], which implements a private video service where clients stream\n","movies using PIR, must deal with movies being added, deleted, or modified (e.g., to change codec) occasion- ally.\n","Similarly, anonymous messaging systems like Pung [ 6] and Talek [ 20] have databases where new elements (messages) are\n","added every few minutes, while contact discovery services like DP5 [12] have users creating and deleting accounts. This\n","paper extends offline/online PIR schemes to support mutable databases where the content can change at any time by\n","introducing the notion of incremental preprocessing . The result is an incremental offline/online PIR scheme where ad-\n","ditions, deletions, and edits to the database do not require a complete preprocessing after every change. Instead, the\n","hints are updated at a cost that is proportional to the number of changes. To demonstrate the feasibility and benefits\n","of incre- mental preprocessing, we extend two recent two-server of- fline/online PIR protocols: (1) the Corrigan-Gibbs\n","and Kogan scheme (CK) [ 23], and (2) the Shi, Aqeel, Chandrasekaran, and Maggs scheme (SACM) [ 51]. Our experimental\n","evalua- tion confirms that the savings of our approach are consider- able. In a database with 1 million items, the\n","computational cost of updating the hints in our incremental CK scheme (iCK) for a batch of 10,000 updates (additions,\n","deletions, edits) is 56×cheaper than preprocessing from scratch; the savings are even more pronounced when there are\n","fewer updates. Fur- thermore, an implementation of PIR-Tor [ 47] that uses our iCK construction improves the throughput\n","achieved by Tor directory nodes by roughly 7 ×over an implementation of PIR-Tor that uses a state-of-the-art 2-server\n","PIR scheme [ 13]. Our extensions to make these schemes incremental, how- ever, come at a modest cost. For iCK, the cost\n","is a small increase in storage at the client to keep additional auxiliary material needed for the client to construct\n","the right query during the online phase, and a larger (though still sublinear and concretely very efficient) queries\n","during the online phase. For our incremental version of SACM, called iSACM, the cost is higher online communication\n","(though still sublinear in the size of the updated database), since our construction is 1\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Andreas Haeberlen_diffprov-hotnets2015.pdf\n","................................................................................\n","Content: \n","Differential Provenance: Better Network Diagnostics with Reference Events Ang Chen University of PennsylvaniaY ang Wu\n","University of Pennsylvania Andreas Haeberlen University of PennsylvaniaWenchao Zhou Georgetown UniversityBoon Thau Loo\n","University of Pennsylvania ABSTRACT In this paper, we propose a new approach to diagnosing prob- lems in complex\n","networks. Our approach is based on the insight that many of the trickiest problems are anomalies – they affect only a\n","small fraction of the trafﬁc (e.g., perhaps a certain subnet), or they only manifest infrequently. Thus, it is quite\n","common for the network operator to have “exam- ples” of both working and non-working trafﬁc readily avail- able –\n","perhaps a packet that was misrouted, and a similar packet that was routed correctly. In this case, the cause of the\n","problem is likely to be wherever the two packets were treated differently by the network. We sketch the design of a\n","network debugger that can lever- age this information using a novel concept that we call dif- ferential provenance .\n","Like classical provenance, differen- tial provenance tracks the causal connections between net- work and conﬁguration\n","states and the packets that were af- fected by them; however, it can additionally reason about the causes of any\n","discrepancies between different provenances. We have performed a case study in the context of software- deﬁned networks,\n","and our initial results are encouraging: they suggest that differential provenance can often identify the root cause of\n","even very subtle network issues. Categories and Subject Descriptors D.2.5 [ Testing and debugging ]: Diagnostics\n","Keywords Diagnostics, Debugging, Provenance 1. INTRODUCTION Networks are not easy to get right. Despite the fact that\n","re- searchers have developed a wide range of tools for network diagnostics [8, 16, 17, 11, 14, 15, 5], understanding the\n","intri- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted\n","without fee, provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear\n","this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than the\n","author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or\n","to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from Permissions@acm.org.\n","HotNets-XIV , November 16–17, 2015, Philadelphia, PA, USA. Copyright is held by the owner/author(s). Publication rights\n","licensed to ACM. ACM 978-1-4503-4047-2/15/11 ...$15.00. DOI: http://dx.doi.org/10.1145/2834050.2834111.cate relations\n","between network events for a root-cause anal- ysis is still challenging. This is perhaps especially true for software-\n","deﬁned networks (SDNs), which offers exceptional ﬂexibility by introducing a programmable controller but also further\n","complicates reasoning about network faults. Network provenance [21] is a promising candidate for un- derstanding the\n","details of network executions, as it provides step-by-step explanations of network events. It encodes net- work events\n","as vertexes in a distributed database and their causal relations as links; they form a provenance graph on which an\n","operator could issue queries that ask for an ex- planation of a certain event. For example, an operator can ask why a\n","certain route exists in her router’s BGP table by issuing a query on the vertex that represents this route in the\n","provenance graph; the answer to her query would be a provenance tree rooted at that vertex, chronicling how the route\n","has been derived from other BGP events. A number of provenance-based diagnostic tools have been developed re- cently,\n","including systems like ExSPAN [21], SNP [19], and Y! [16]. Conceptually, these systems all provide variants of the same\n","capability: a way to ask for a comprehensive ex- planation of a certain network event. However, while a comprehensive\n","explanation is certainly useful for diagnosing a problem, it is not the same as ﬁnd- ing the actual root causes . We\n","illustrate the difference with an analogy from everyday life: suppose Bob wants to know why his bus arrives at 5:05pm,\n","which is ﬁve minutes late. If Bob had a provenance-based debugger, he could submit the query “Why did my bus arrive at\n","5:05pm?”, and he would get a comprehensive explanation, such as “The bus was dis- patched at the terminal at 4:00pm, and\n","arrived at stop A at 4:13pm; it departed from there at 4:15pm, and arrived at stop B at 4:21pm; ... Finally, it departed\n","from stop Z at 5:01pm, and arrived at Bob’s platform at 5:05pm”. This is very dif- ferent from what Bob really wanted to\n","know; the actual root cause might be something like “At stop G, the bus had to wait for ﬁve minutes because of a trafﬁc\n","jam”. But suppose we allow Bob to instead ask about the differ- ences between two events – perhaps “why did my bus\n","arrive at 5:05pm today and not at 5:00pm, like yesterday?”. Thus, the debugger can omit those parts of the explanation\n","that the two events have in common, and instead focus on the (hope- fully few) parts that caused the different outcomes.\n","We argue 1\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Sebastian Angel_incpir-sec22.pdf\n","................................................................................\n","Content: \n","Incremental Offline/Online PIR Yiping Ma⋆Ke Zhong⋆Tal Rabin⋆†Sebastian Angel⋆‡ ⋆University of Pennsylvania†Algorand\n","Foundation‡Microsoft Research Abstract. Recent private information retrieval (PIR) schemes preprocess the database with\n","a query-independent offline phase in order to achieve sublinear computation during a query-specific online phase. These\n","offline/online protocols expand the set of applications that can profitably use PIR, but they make a critical\n","assumption: that the database is immutable. In the presence of changes such as additions, deletions, or updates,\n","existing schemes must preprocess the database from scratch, wasting prior effort. To address this, we introduce\n","incremental preprocessing for offline/online PIR schemes, allowing the original preprocessing to continue to be used\n","after database changes, while incurring an update cost proportional to the number of changes rather than the size of the\n","database. We adapt two offline/online PIR schemes to use incremental preprocessing and show how it significantly\n","improves the throughput and reduces the latency of applications where the database changes over time. 1 Introduction\n","Private information retrieval (PIR) [ 22] is an incredibly useful cryptographic building block that allows a client to\n","download an object from a database without revealing which object was fetched. PIR has many applications, including\n","privacy- preserving video streaming [ 3,35], password checking [ 4], blocklists [ 40], ad delivery [ 34], friend\n","discovery [ 12], sub- scriptions [ 20], and anonymous messaging [ 6,42,47]. While powerful, PIR is expensive: PIR\n","imposes a computational linear lower bound since the database must operate on all objects in order to answer a query.\n","After all, if even a single object is omitted when answering a query this would leak that this object is of no interest\n","to the client. The existence of this computational lower bound is unfor- tunate, as it limits the scale of applications\n","that can profitably use PIR. But there is still hope! There is an active line of work [ 5,8,15,18,23,45,50,51] that\n","pursues the idea of preprocessing the database to generate auxiliary information orhints (which could be stored at the\n","server or clients depend- ing on the proposal) during an offline phase, and then use the hints during an online phase to\n","answer one or more queries with sublinear computation and communication. This is an exciting proposition, as it enables\n","applications that require quick answers for (online) queries to large databases, but that can afford an expensive query-\n","independent preprocessing. A major implicit assumption in all of the above works is that the PIR database is immutable .\n","That is, once it has been preprocessed, no new items will be added, deleted, or updated.Should the database change,\n","under all existing schemes, it is necessary to redo the expensive offline phase—defeating in many cases the benefits of\n","preprocessing. Meanwhile, many of the proposed applications that use PIR naturally experi- ence at least some moderate\n","content churn. For example, Pop- corn [ 35], which implements a private video service where clients stream movies using\n","PIR, must deal with movies being added, deleted, or modified (e.g., to change codec) occasion- ally. Similarly,\n","anonymous messaging systems like Pung [ 6] and Talek [ 20] have databases where new elements (messages) are added every\n","few minutes, while contact discovery services like DP5 [12] have users creating and deleting accounts. This paper\n","extends offline/online PIR schemes to support mutable databases where the content can change at any time by introducing\n","the notion of incremental preprocessing . The result is an incremental offline/online PIR scheme where ad- ditions,\n","deletions, and edits to the database do not require a complete preprocessing after every change. Instead, the hints are\n","updated at a cost that is proportional to the number of changes. To demonstrate the feasibility and benefits of incre-\n","mental preprocessing, we extend two recent two-server of- fline/online PIR protocols: (1) the Corrigan-Gibbs and Kogan\n","scheme (CK) [ 23], and (2) the Shi, Aqeel, Chandrasekaran, and Maggs scheme (SACM) [ 51]. Our experimental evalua- tion\n","confirms that the savings of our approach are consider- able. In a database with 1 million items, the computational cost\n","of updating the hints in our incremental CK scheme (iCK) for a batch of 10,000 updates (additions, deletions, edits) is\n","56×cheaper than preprocessing from scratch; the savings are even more pronounced when there are fewer updates. Fur-\n","thermore, an implementation of PIR-Tor [ 47] that uses our iCK construction improves the throughput achieved by Tor\n","directory nodes by roughly 7 ×over an implementation of PIR-Tor that uses a state-of-the-art 2-server PIR scheme [ 13].\n","Our extensions to make these schemes incremental, how- ever, come at a modest cost. For iCK, the cost is a small\n","increase in storage at the client to keep additional auxiliary material needed for the client to construct the right\n","query during the online phase, and a larger (though still sublinear and concretely very efficient) queries during the\n","online phase. For our incremental version of SACM, called iSACM, the cost is higher online communication (though still\n","sublinear in the size of the updated database), since our construction is not compatible with a specific cryptographic\n","primitive used in SACM (a private puncturable PRF [51]). 1\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Ryan Marcus_Jialin Ding et al. - 2022 - SageDB An Instance-Optimized Data Analytics Syste.pdf\n","................................................................................\n","Content: \n","SageDB: An Instance-Optimized Data Analytics System Jialin Ding, Ryan Marcus∗, Andreas Kipf, Vikram Nathan, Aniruddha\n","Nrusimha, Kapil Vaidya, Alexander van Renen†, Tim Kraska Massachusetts Institute of Technology∗University of\n","Pennsylvania†Friedrich-Alexander-Universität Erlangen-Nürnberg ABSTRACT\n","Moderndatasystemsaretypicallybothcomplexandgeneral-purpose. They are complex because of the numerous internal knobs and\n","pa- rameters that users need to manually tune in order to achieve good performance; they are general-purpose because\n","they are designed to handle diverse use cases, and therefore often do not achieve the best possible performance for any\n","specific use case. A recent trend aims to tackle these pitfalls: instance-optimized systems are designed to\n","automatically self-adjust in order to achieve the best performance for a specific use case, i.e., a dataset and query\n","workload. Thus far, the research community has focused on creating instance-optimized database components, such as\n","learned indexes and learned cardi- nality estimators, which are evaluated in isolation. However, to the best of our\n","knowledge, there is no complete data system built with instance-optimization as a foundational design principle. In this\n","paper, we present a progress report on SageDB, our effort towards building the first instance-optimized data system.\n","SageDB synthesizes various instance-optimization techniques to automat- ically specialize for a given use case, while\n","simultaneously exposing a simple user interface that places minimal technical burden on the user. Our prototype\n","outperforms a commercial cloud-based analytics system by up to 3× on end-to-end query workloads and up to 250× on\n","individual queries. SageDB is an ongoing research effort, and we highlight our lessons learned and key directions for\n","future work. PVLDB Reference Format: JialinDing,RyanMarcus,AndreasKipf,VikramNathan,AniruddhaNrusimha, Kapil Vaidya,\n","Alexander van Renen, Tim Kraska. SageDB: An Instance- Optimized Data Analytics System. PVLDB, 15(13): 4062 - 4078, 2022.\n","doi:10.14778/3565838.3565857 1 INTRODUCTION Most modern data management systems fall on a spectrum be- tween general-\n","purpose and application-specific. For example, Post- greSQL [ 8] is extremely general purpose, and powers a diverse\n","range of analytical and transactional workloads. Apache Spark is slightly specialized towards analytic tasks, but can\n","still handle a wide variety of use cases (e.g., batch reporting, ad-hoc interactive queries, data science, and ML) and\n","low-level workloads (e.g., I/O-bound, CPU- bound, in-memory, on-disk, in the cloud). On the other hand, systems like\n","Google’s Mesa [ 30] and Napa [ 10] were custom-built to power Google Ads, and are not suitable for any other\n","application. While these systems improve efficiency, these bespoke systems require This work is licensed under the\n","Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a\n","copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org.\n","Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB\n","Endowment, Vol. 15, No. 13 ISSN 2150-8097. doi:10.14778/3565838.3565857years of intense engineering effort and are only\n","achievable by large corporations with significant resources. Ideally, users should be able to have the efficiency of\n","special- ized systems along with the flexibility of general-purpose systems. Tuning configuration options (\"knobs\") is\n","easier than building an entirely new system, and can bridge some of the performance gap. However, experienced engineers\n","and database administrators still go through the time-consuming and error-prone tuning process for each application.\n","Recent research proposes techniques for automatic knob tuning [15]; however, the performance impact of tuning such knobs\n","is still limited. For example, users can only adjust the size of a data block, not how data is laid out on disk.\n","Fundamentally, general-purpose systems are designed to be task agnostic, so for most tasks a tuned general-purpose\n","system will perform worse than a custom-tailored system. Recent work has shown that existing system components can be\n","replaced with instance-optimized orlearned components, which are able to automatically adjust to a specific use case and\n","workload (see [5] for an overview). For example, learned index structures [24, 37] offer the same read functionality as\n","traditional index structures (e.g. B+ trees) while providing better performance in both latency and space consumption.\n","Instance-optimized data storage layouts [ 63] are able to improve scan performance by skipping data with greater\n","effectiveness than traditional sorting-based partitioning techniques. However,theseinstance-\n","optimizedcomponentshavelargelybeen designed and evaluated in isolation, and there have only been a few efforts to\n","integrate them into an end-to-end system. Bourbon [ 19] replaces block indexes in an LSM-tree with learned indexes and\n","demonstrates latency improvements. Google integrated learned in- dexes into BigTable [ 9] with similar findings, mainly\n","due to a smaller index footprint and fewer cache misses when traversing the index. While these are useful initial\n","studies, it is still unclear how multiple instance-optimized components would work together in concert. In fact, it is\n","easy to imagine a number of learned components destruc- tively interfering with each other. Is it possible to build a\n","system that autonomously custom-tailors its major components to the user’s requirements, approaching the performance of\n","a bespoke system but with similar ease of use as a general-purpose system? To the best of our knowledge, there is no\n","end-to-end data system built with instance-optimization as a foundational design principle. We previously presented our\n","vision and blueprint for such a system, called SageDB [ 36]. In this paper, we present our first prototype of SageDB,\n","and show how two carefully selected components can work together in practice. These instance-optimized components are\n","(1) (multi-dimensional) data layouts and data replication and (2) partial materialized views. These techniques minimize\n","I/O when scanning data from disk and maximize computation reuse through intelligent pre-materialization of partial\n","results. While the ultimate goal is to automatically trigger self-optimization whenever necessary, for the 4062\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Andreas Haeberlen_snp-sosp2011.pdf\n","................................................................................\n","Content: \n","Secure Network Provenance Wenchao Zhou University of PennsylvaniaQiong Fei University of PennsylvaniaArjun Narayan\n","University of Pennsylvania Andreas Haeberlen University of PennsylvaniaBoon Thau Loo University of PennsylvaniaMicah\n","Sherr Georgetown University ABSTRACT This paper introduces secure network provenance (SNP) , a novel technique that\n","enables networked systems to explain to their operators whythey are in a certain state – e.g., why a suspicious routing\n","table entry is present on a certain router, or where a given cache entry originated. SNP provides net- work forensics\n","capabilities by permitting operators to track down faulty or misbehaving nodes, and to assess the damage such nodes may\n","have caused to the rest of the system. SNP is designed for adversarial settings and is robust to manip- ulation; its\n","tamper-evident properties ensure that operators can detect when compromised nodes lie or falsely implicate correct\n","nodes. We also present the design of SNooPy , a general-purpose SNP system. To demonstrate that SNooPy is practical, we\n","apply it to three example applications: the Quagga BGP daemon, a declarative implementation of Chord, and Hadoop\n","MapReduce. Our results indicate that SNooPy can eﬃciently explain state in an adversarial setting, that it can be\n","applied with minimal eﬀort, and that its costs are low enough to be practical. Categories and Subject Descriptors C.2.4\n","[ Computer Systems Organization ]: Computer- Communication Networks— Distributed Systems ; D.4.5 [Software ]: Operating\n","Systems— Reliability General Terms Algorithms, Design, Reliability, Security Keywords Accountability, Byzantine faults,\n","Distributed systems, Evi- dence, Provenance, Security 1. INTRODUCTION Operators of distributed systems often ﬁnd\n","themselves need- ing to answer a diagnostic or forensic question. Some part of the system is found to be in an\n","unexpected state – for example, a suspicious routing table entry is discovered, or aproxy cache is found to contain an\n","unusually large number of advertisements. The operators must determine the causes of this state before they can decide\n","on an appropriate response. On the one hand, there may be an innocent explanation: the routing table entry could be the\n","result of a misconﬁguration, and the cache entries could have appeared due to a workload change. On the other hand, the\n","unexpected state may be the symptom of an ongoing attack: the routing table entry could be the result of route\n","hijacking, and the cache entries could be a side-eﬀect of a malware infection. If an attack or mis- conﬁguration is\n","discovered, the operators must determine its eﬀects , such as corrupted state or conﬁguration changes on other nodes, so\n","that these nodes can be repaired and the system brought back to a correct state. In this paper, we consider forensics in\n","an adversarial set- ting, that is, we assume that a faulty node does not neces- sarily crash but can also change its\n","behavior and continue operating. To be conservative, we assume that faults can be Byzantine [ 24], i.e., a faulty node\n","can behave arbitrar- ily. This covers a wide range of faults and misbehavior, e.g., cases where a malicious adversary\n","has compromised some of the nodes, but also more benign faults, such as hardware failures or misconﬁgurations. Getting\n","correct answers to forensic queries in an adversarial setting is diﬃcult because the misbehaving nodes can lie to the\n","querier. For example, the adversary can attempt to conceal his actions by causing his nodes to fabricate plausible (but\n","incorrect) responses to forensic queries, or he can attempt to frame correct nodes by returning responses that blame his\n","own misbehavior on them. Thus, the adversary can gain valuable time by mis- directing the operators and/or causing them\n","to suspect a problem with the forensic system itself. Existing forensic systems are either designed for non- adversarial\n","settings [ 43,51] or require some trusted com- ponents, e.g., a trusted virtual-machine monitor [ 3,21], a trusted host-\n","level monitor [ 27], a trusted OS [ 29], or trusted hardware [ 7]. However, most components that are avail- able today\n","are not fully trustworthy; OSes and virtual ma- chine monitors have bugs, which a powerful adversary could exploit, and\n","even trusted hardware is sometimes compro- mised [ 20]. We argue that it is useful to have alternative techniques\n","available that do not require this type of trust. We introduce secure network provenance (SNP) , a tech- nique for\n","building forensic systems that can operate in a completely untrusted environment. We assume that the ad- versary may\n","have compromised an arbitrary subset of the nodes, and that he may have complete control over these nodes. On the one\n","hand, this very conservative threat model requires some compromises: an SNP system can only answer\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Andreas Haeberlen_diffprov-sigcomm2016.pdf\n","................................................................................\n","Content: \n","The Good, the Bad, and the Differences: Better Network Diagnostics with Differential Provenance Ang Chen University of\n","PennsylvaniaY ang Wu University of Pennsylvania Andreas Haeberlen University of PennsylvaniaWenchao Zhou Georgetown\n","UniversityBoon Thau Loo University of Pennsylvania ABSTRACT In this paper, we propose a new approach to diagnosing prob-\n","lems in distributed systems. Our approach is based on the in- sight that many of the trickiest problems are anomalies.\n","For instance, in a network, problems often affect only a small fraction of the trafﬁc (perhaps a certain subnet), or\n","they only manifest infrequently. Thus, it is quite common for the op- erator to have “examples” of both working and non-\n","working trafﬁc readily available – perhaps a packet that was mis- routed, and a similar packet that was routed\n","correctly. In this case, the cause of the problem is likely to be wherever the two packets were treated differently by\n","the network. We present the design of a debugger that can leverage this information using a novel concept that we call\n","differential provenance . Differential provenance tracks the causal con- nections between network states and state\n","changes, just like classical provenance, but it can additionally perform root- cause analysis by reasoning about the\n","differences between two provenance trees. We have built a diagnostic tool that is based on differential provenance, and\n","we have used our tool to debug a number of complex, realistic problems in two scenarios: software-deﬁned networks and\n","MapReduce jobs. Our results show that differential provenance can de- liver very concise diagnostic information; in many\n","cases, it can even identify the precise root cause of the problem. CCS Concepts •Networks→Network management; Network\n","experimen- tation;•Information systems →Data provenance; Keywords Network diagnostics, debugging, provenance Permission\n","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided\n","that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full\n","citation on the ﬁrst page. Copyrights for components of this work owned by others than the author(s) must be honored.\n","Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists,\n","requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. SIGCOMM ’16, August 22 -\n","26, 2016, Florianópolis, Brazil c⃝2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN\n","978-1-4503-4193-6/16/08. . . $15.00 DOI: http://dx.doi.org/10.1145/2934872.29349101. INTRODUCTION Distributed systems\n","are not easy to get right. Despite the fact that researchers have developed a wide range of diagnostic tools [16, 30,\n","31, 19, 27, 29, 10], understanding the intricate relations between low-level events, which is needed for root- cause\n","analysis, is still challenging. Recent work on data provenance [36] has provided a new approach to understanding the\n","details of distributed execu- tions. Intuitively, a provenance system keeps track of the causal connections between the\n","states and events that a sys- tem generates at runtime; for instance, when applied to a software-deﬁned network (SDN),\n","it might associate each ﬂow entry with the parts of the controller program that were used to compute it. Then, when the\n","operator asks a diagnostic question – say, why a certain packet was routed to a par- ticular host – the system returns a\n","comprehensive explana- tion that recursively explains each relevant event in terms of its direct causes. A number of\n","provenance-based diagnostic tools have been developed recently, including systems like ExSPAN [36], SNP [34], and Y!\n","[30]. However, while such a comprehensive explanation is use- ful for diagnosing a problem, it is not the same as ﬁnding\n","the actual root causes . We illustrate the difference with an ana- logy from everyday life: suppose Bob wants to know\n","why his bus arrived at 5:05pm, which is ﬁve minutes late. If Bob had a provenance-based debugger, he could submit the\n","query “Why did my bus arrive at 5:05pm?”, and he would get a comprehensive explanation, such as “The bus was dis-\n","patched at the terminal at 4:00pm, and arrived at stop A at 4:13pm; it departed from there at 4:15pm, and arrived at\n","stop B at 4:21pm; ... Finally, it departed from stop Z at 5:01pm, and arrived at Bob’s platform at 5:05pm”. This is very\n","dif- ferent from what Bob really wanted to know: the actual root cause might be something like “At stop G, the bus had\n","to wait for ﬁve minutes because of a trafﬁc jam”. But suppose we allow Bob to instead ask about the dif- ferences\n","between two events – perhaps “Why did my bus arrive at 5:05pm today, and not at 5:00pm like yesterday?”. The debugger\n","can then omit those parts of the explanation that the two events have in common, and instead focus on the (hopefully\n","few) parts that caused the different outcomes. We argue that a similar approach should work for diagnos- 1\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Susan Davidson_1201.0231.pdf\n","................................................................................\n","Content: \n","Putting Lipstick on Pig: Enabling Database­style Workﬂow Provenance Y ael Amsterdamer2, Susan B. Davidson1, Daniel\n","Deutch3, Tova Milo2, Julia Stoyanovich1, Val Tannen1 1University of Pennsylvania, USA2Tel Aviv University, Israel3Ben\n","Gurion University, Israel {susan, jstoy, val }@cis.upenn.edu {yaelamst, milo }@cs.tau.ac.il deutchd@cs.bgu.ac.il\n","ABSTRACT Workﬂow provenance typically assumes that each module is a “black-box”, so that each output depends on all in-\n","puts ( coarse-grained dependencies). Furthermore, it does not model the internal state of a module, which can change\n","between repeated executions. In practice, however, an out- put may depend on only a small subset of the inputs ( ﬁne-\n","grained dependencies) as well as on the internal state of the module. We present a novel provenance framework that\n","marries database-style and workﬂow-style provenance, by using Pig Latin to expose the functionality of modules, thus\n","capturing internal state and ﬁne-grained dependencies. A critical ingredient in our solution is the use of a novel form\n","of provenance graph that models module invocations and yields a compact representation of ﬁne-grained workﬂow prove-\n","nance. It also enables a number of novel graph transforma- tion operations, allowing to choose the desired level of gra\n","n- ularity in provenance querying (ZoomIn and ZoomOut), and supporting “what-if” workﬂow analytic queries. We imple-\n","mented our approach in the Lipstick system and developed a benchmark in support of a systematic performance eval-\n","uation. Our results demonstrate the feasibility of trackin g and querying ﬁne-grained workﬂow provenance. 1.\n","INTRODUCTION Data-intensive application domains such as science and electronic commerce are increasingly using workﬂow\n","system s to design and manage the analysis of large datasets and to track the provenance of intermediate and ﬁnal data\n","prod- ucts. Provenance is extremely important for veriﬁability and repeatability of results, as well as for debugging\n","and trouble-shooting workﬂows [10, 11]. The standard assumption for workﬂow provenance is that each module is a “black-\n","box”, so that each output of the module depends on all its inputs ( coarse-grained dependen- cies). This model is\n","problematic since it cannot account for common situations in which an output item depends onlyon a small subset of the\n","inputs ( ﬁne-grained dependencies). For example, the module function may be mapped over an input list, so that the\n","ithelement of the output list depends only on the ithelement of the input list (see Taverna [18, 29]). Furthermore, the\n","model does not capture the internal state of a module, which may be modiﬁed by inputs seen in previous executions of the\n","workﬂow (e.g., a learning al- gorithm), and an output may depend on some (but not all) of these previous inputs.\n","Maintaining an “output depends on all inputs” assumption quickly leads to a very coarse ap- proximation of the actual\n","data dependencies that exist in an execution of the workﬂow; furthermore, it does not show the way in which these\n","dependencies arise. For example, consider the car dealership workﬂow shown in Figure 1. The execution starts with a\n","buyer providing her identiﬁer and the car model of interest to a bid request mod- ule that distributes the request to\n","several car dealer mod- ules. Each dealer looks in its database for how many cars of the requested model are available,\n","how many sales of that model have recently been made, and whether the buyer pre- viously made a request for this model,\n","and, based on this information, generates a bid and records it in its database state. Bids are directed to an aggregator\n","module that cal- culates the best (minimum) bid. The user then makes a choice to accept or decline the bid; if the bid\n","is accepted, the relevant dealership is notiﬁed to ﬁnalize the purchase. If the user declines the bid but requests the\n","same car model in a subsequent execution, each dealer will consult its bid history and will generate a bid of the same\n","or lower amount. Coarse-grained provenance for this workﬂow would show the information that was given by the user to the\n","bid re- quest module, the bids that were produced by each dealer and given as input to the aggregator, the choice that\n","the user made, and which dealer made a sale (if any). However, it would not show the dependence of the bid on the cars\n","that were available at the time of the request, on relevant sale history, and on previous bids. Thus, queries such as\n","“Was the sale of this VW Jetta aﬀected by the presence of a Honda Civic in the dealership’s lot?”,“Which cars aﬀected\n","the computation of this winning bid?”, and “Had this Toy- ota Prius not been present, would its dealer still have made a\n","sale?” would not be supported. Coarse-grained provenance would also not give detailed information about how the best bid\n","was calculated (a minimum aggregate). Finer-grained provenance has been well-studied in databas e research. In\n","particular, a framework based on semiring an- notations has been proposed [17], in which every tuple of the database is\n","annotated with an element of a provenance Permission to make digital or hard copies of all or part of this work for\n","personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or\n","commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to\n","republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Articles from\n","this volume were invited to present their results at The 38th International Conference on Very Large Data Bases, August\n","27th - 31st 2012, Istanbul, Turkey. Proceedings of the VLDB Endowment, V ol. 5, No. 4 Copyright 2011 VLDB Endowment\n","2150-8097/11/12... $10.00. 346 arXiv:1201.0231v1  [cs.DB]  31 Dec 2011\n","................................................................................\n","Response: Concept: - Workﬂow provenance - Database-style and workﬂow-style provenance - Provenance framework - Pig Latin -\n","Provenance graph - ZoomIn and ZoomOut\n","................................................................................\n","What are key features of Weijie Su's work?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","Create a list of features, concepts, and related topics from each papers that can be used for machine learning\n","\n","Question: What are key features of Weijie Su's work?\n","\n","This is an example of what you have to do:\n","Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into humanreadable concepts. They allow people to easily understand\n","why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and\n","often under-perform their black box counterparts, preventing\n","their broad adoption. We address these shortcomings and\n","are first to show how to construct high-performance CBMs\n","without manual specification of similar accuracy to black\n","box models. Our approach, Language Guided Bottlenecks\n","(LaBo), leverages a language model, GPT-3, to define a\n","large space of possible bottlenecks. Given a problem domain,\n","LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches\n","possible bottlenecks through a novel submodular utility that\n","promotes the selection of discriminative and diverse information. Ultimately, GPT-3’s sentential concepts can be aligned\n","to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for\n","concepts important to visual recognition. In the evaluation\n","with 11 diverse datasets, LaBo bottlenecks excel at few-shot\n","classification: they are 11.7% more accurate than black\n","box linear probes at 1 shot and comparable with more data.\n","Overall, LaBo demonstrates that inherently interpretable\n","models can be widely applied at similar, or better, performance than black box approaches.: [Language Guided Bottlenecks, Concept Bottleneck Models, Machine learning, Data Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing],\n","\n","# We examine a Markovian model for the price evolution of a\n","# stock, in which the probability of local upward or downward movement\n","# is arbitrarily dependent on the current price itself (and perhaps some auxiliary state information). \n","# This model directly and considerably generalizes many of the most well-studied price evolution models in classical finance, including a variety of random walk, drift and diffusion models.\n","# Our main result is a universally profitable\" trading strategy | a single fixed strategy whose profitability competes with the optimal strategy\n","# (which knows all of the underlying parameters of the infinite and possibly\n","# nonstationary Markov process): [Trading, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models],\n","\n","# We present new algorithms for reinforcement learning and prove that they have polynomial bounds\n","# on the resources required to achieve near-optimal return in general Markov decision processes. After observing\n","# that the number of actions required to approach the optimal return is lower bounded by the mixing time T of\n","# the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give\n","# algorithms requiring a number of actions and total computation time that are only polynomial in T and the number\n","# of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is\n","# their explicit handling of the Exploration-Exploitation trade-off.: [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time],\n","\n","The Role of Visualization in Computer Science Education  \n"," \n"," \n","ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER  \n"," \n","Department of Computer Science  \n","Virginia Tech  \n","Blacksburg, VA 24061  \n"," \n"," \n","Computer Science core instruction attempts to provide a detailed understanding o f dy-\n","namic processes such as the working of an algorithm or the flow of information between \n","computing ent ities. Such dynamic processes are not well explained by static media such \n","as text and images, and are difficult to convey in le cture. We survey the hist ory of visual i-\n","zation in CS education, foc using on artifacts that have  a documented positive educational \n","assessment. We discuss how changes in computing technology have affected the deve l-\n","opment and uptake of such visualization artifacts in CS education, and how recent tech-\n","nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization, data structure visualization, program visualiz a-\n","tion, eTextbooks, hypertextbooks.  \n","  \n","(Draft of paper to appear in Computers in the Schools , 2012)\n","\n","Adaptively Secure Garbled Circuits\n","from One-Way Functions∗\n","Brett Hemenway†Zahra Jafargholi‡Rafail Ostrovsky§Alessandra Scafuro¶\n","Daniel Wichs∥\n","Abstract\n","A garbling scheme is used to garble a circuit Cand an input xin a way that reveals the output\n","C(x) but hides everything else. In many settings, the circuit can be garbled oﬀ-line without strict\n","eﬃciency constraints, but the input must be garbled very eﬃciently on-line , with much lower complexity\n","than evaluating the circuit. Yao’s scheme has essentially optimal on-line complexity, but only achieves\n","selective security , where the adversary must choose the input xprior to seeing the garbled circuit. It has\n","remained an open problem to achieve adaptive security , where the adversary can choose xafter seeing\n","the garbled circuit, while preserving on-line eﬃciency.\n","In this work, we modify Yao’s scheme in a way that allows us to prove adaptive security under one-way\n","functions. As our main instantiation, we get a scheme where the on-line complexity is only proportional\n","to the width wof the circuit, which corresponds to the space complexity of the computation, but is\n","independent of the circuit’s depth d. Alternately, we can also get an instantiation where the on-line\n","complexity is only proportional to the input/output size and the depth dof the circuit but independent\n","of its width w, albeit in this case we incur a 2O(d)security loss in our reduction. More broadly, we relate\n","the on-line complexity of adaptively secure garbling schemes in our framework to a certain type of pebble\n","complexity of the circuit.\n","As our main tool, of independent interest, we develop a new notion of somewhere equivocal encryption,\n","which allows us to eﬃciently equivocate on a small subset of the message bits.\n","∗This work was done in part while some of the authors were visiting the Simons Institute for the Theory of Computing,\n","supported by the Simons Foundation and by the DIMACS/Simons Collaboration in Cryptography through NSF grant CNS-\n","1523467.\n","†University of Pennsylvania. Department of Computer Science. fbrett@cis.upenn.edu\n","‡Northeastern University. Department of Computer Science. zahra@ccs.neu.edu\n","§University of California, Los Angeles. Department of Computer Science and Mathematics. rafail@cs.ucla.edu Research\n","supported in part by NSF grants 09165174, 1065276, 1118126 and 1136174, US-Israel BSF grant 2008411, OKAWA Foundation\n","Research Award, IBM Faculty Research Award, Xerox Faculty Research Award, B. John Garrick Foundation Award, Teradata\n","Research Award, and Lockheed-Martin Corporation Research Award. This material is based upon work supported in part by\n","DARPA Safeware program. The views expressed are those of the author and do not reﬂect the oﬃcial policy or position of the\n","Department of Defense or the U.S. Government.\n","¶Boston University and Northeastern University. Department of Computer Science. scafuro@bu.edu Research supported by\n","NSF grants 1012798.\n","∥Northeastern University. Department of Computer Science. wichs@ccs.neu.edu . Research supported by NSF grants\n","CNS-1347350, CNS-1314722, CNS-1413964.\n","\n","1 \n","Chapter 17. How Social Networks Shape Social Comparison  \n"," \n","Jingwen Zhang (University of California, Davis)  \n","Damon Centola (Annenberg School for Communication/University of Pennsylvania)  \n","Abstract: While social comparison research has focused on  the processes and consequences \n","of how the comparer gleans information from the comparison other (individual or group) , \n","recent research on social networks demonstrates how information and influence is \n","distributed across persons in a network. This chapter r eviews social influence  processes in \n","social networks . We first review recent research on social compar ison and its negative \n","consequences in online social networks. Then we delve into discussing  the social network \n","causes of biased social perceptions online and  how this can be remedied by building more \n","accurate perceptions through constructed online networks.  Lastly , we discuss findings from \n","recent experimental studies  that illustrate how construc ted online networks can harness \n","social comparison to induce significant changes in health behavior.\n","\n","QWIRE : A Core Language for Quantum Circuits\n","Jennifer Paykin Robert Rand Steve Zdancewic\n","University of Pennsylvania, USA\n","jpaykin@seas.upenn.edu, rrand@seas.upenn.edu, stevez@cis.upenn.edu\n","Abstract\n","This paper introduces QWIRE (“choir”), a language for deﬁning\n","quantum circuits and an interface for manipulating them inside of\n","an arbitrary classical host language. QWIRE is minimal—it con-\n","tains only a few primitives—and sound with respect to the phys-\n","ical properties entailed by quantum mechanics. At the same time,\n","QWIRE is expressive and highly modular due to its relationship\n","with the host language, mirroring the QRAM model of computation\n","that places a quantum computer (controlled by circuits) alongside\n","a classical computer (controlled by the host language).\n","We presentQWIRE along with its type system and operational\n","semantics, which we prove is safe and strongly normalizing when-\n","ever the host language is. We give circuits a denotational semantics\n","in terms of density matrices. Throughout, we investigate examples\n","that demonstrate the expressive power of QWIRE , including exten-\n","sions to the host language that (1) expose a general analysis frame-\n","work for circuits, and (2) provide dependent types.\n","Categories and Subject Descriptors D.3.1 [ Programming Lan-\n","guages ]: Formal Deﬁnitions and Theory; F.3.1 [ Logics and Mean-\n","ings of Programs ]: Specifying and Verifying and Reasoning about\n","Programs\n","Keywords quantum programming languages, quantum circuits,\n","linear types, denotational semantics\n","1. Introduction\n","The standard architecture for quantum computers follows the quan-\n","tum circuit model , which presents quantum computations as se-\n","quences of gates over qubits (the quantum analogue of bits). As\n","with classical circuits, quantum circuits exist at a very low level\n","of abstraction, and yet in spite of this, researchers and industry\n","professionals write complex quantum algorithms in state-of-the-art\n","quantum circuit languages like Quipper (Green et al. 2013a) and\n","LIQUi|⟩(Wecker and Svore 2014).\n","Why is the quantum circuit model so successful? In part, it is\n","due to the fact that quantum data like qubits are extremely unintu-\n","itive from a classical perspective. Research into simple operations\n","on quantum data, such as qubit-controlled conditionals and recur-\n","sion, is still in its infancy (Ying 2014; Badescu and Panangaden\n","2015), so programmers cannot be sure that their algorithms using\n","such abstractions are valid quantum-mechanically.Although circuits manipulate quantum data, they themselves are\n","classical data—a circuit is just a sequence of instructions describ-\n","ing how to apply gates to wires. In practice this means that circuits\n","can be used to build up layers of abstractions hiding the low-level\n","details. The QRAM model of quantum computing (Knill 1996) for-\n","malizes this intuition by describing how a quantum computer could\n","work in tandem with a classical computer. In the QRAM model the\n","classical computer handles the majority of ordinary tasks, while\n","the quantum computer performs specialized quantum operations.\n","To communicate, the classical computer sends instructions to the\n","quantum machine in the form of quantum circuits. Over the course\n","of execution, the quantum computer sends measurement results\n","back to the classical computer as needed.\n","Classical\n","ComputerQuantum\n","Computercircuits\n","measurement results\n","Embedded languages like Quipper, LIQ Ui|⟩, the Q language (Bet-\n","telli et al. 2003), and the quantum IO monad (Altenkirch and Green\n","2010) can be thought of as instantiations of this model. They exe-\n","cute by running host language programs on the classical computer,\n","making specialized calls to the (hypothetical) quantum machine.\n","The classical host languages allow programmers to easily build up\n","high-level abstractions out of low-level quantum operations.\n","However, such abstractions are only worthwhile if the circuits\n","they produce are safe—if they do not cause errors when executed\n","on a quantum computer. Unfortunately, proving that an embedded\n","language produces well-formed circuits is hard because it means\n","reasoning about the entirety of the classical host language. This is\n","frustrating when we care most about the correctness of quantum\n","programs, which we expect to be both more expensive and error-\n","prone than the embedded language’s classical programs.\n","One way of ensuring the safety of circuits is via a strong type\n","system. Type safety for a quantum programming language means\n","that well-formed circuits will not get stuck or “go wrong” when\n","executed on a quantum machine. A subtlety is that this deﬁnition\n","implies that the quantum program is even implementable on a quan-\n","tum computer—that the high-level operational view of the language\n","is compatible with quantum physics. One way of ensuring that the\n","language is implementable is to give a denotational semantics for\n","programs in terms of quantum mechanics.\n","Several quantum programming languages have been proposed\n","with an emphasis on type safety, including Selinger’s QPL lan-\n","guage (Selinger 2004), the quantum lambda calculus (Selinger and\n","Valiron 2009), QML (Altenkirch and Grattage 2005), and Proto-\n","Quipper (Ross 2015). However, these are toy languages, not de-\n","signed for implementation in a conventional programming environ-\n","ment.\n","Permission to make digital or hard copies of all or part of this work for personal or\n","classroom use is granted without fee provided that copies are not made or distributed\n","for proﬁt or commercial advantage and that copies bear this notice and the full citation\n","on the ﬁrst page. Copyrights for components of this work owned by others than ACM\n","must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n","to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a\n","fee. Request permissions from Permissions@acm.org.\n","Copyright is held by the owner/author(s). Publication rights licensed to ACM.\n","POPL’17 , January 15–21, 2017, Paris, France\n","ACM. 978-1-4503-4660-3/17/01...$15.00\n","http://dx.doi.org/10.1145/3009837.3009894\n","846\n","\n","Tackling the Qubit Mapping Problem for NISQ-Era\n","Quantum Devices\n","Gushu Li\n","University of California\n","Santa Barbara, CA\n","gushuli@ece.ucsb.eduYufei Ding\n","University of California\n","Santa Barbara, CA\n","yufeiding@cs.ucsb.eduYuan Xie\n","University of California\n","Santa Barbara, CA\n","yuanxie@ece.ucsb.edu\n","Abstract\n","Due to little consideration in the hardware constraints, e.g.,\n","limited connections between physical qubits to enable two-\n","qubit gates, most quantum algorithms cannot be directly\n","executed on the Noisy Intermediate-Scale Quantum (NISQ)\n","devices. Dynamically remapping logical qubits to physical\n","qubits in the compiler is needed to enable the two-qubit\n","gates in the algorithm, which introduces additional oper-\n","ations and inevitably reduces the fidelity of the algorithm.\n","Previous solutions in finding such remapping suffer from\n","high complexity, poor initial mapping quality, and limited\n","flexibility and controllability.\n","To address these drawbacks mentioned above, this paper\n","proposes a SWAP-based BidiREctional heuristic search al-\n","gorithm (SABRE), which is applicable to NISQ devices with\n","arbitrary connections between qubits. By optimizing every\n","search attempt, globally optimizing the initial mapping using\n","a novel reverse traversal technique, introducing the decay\n","effect to enable the trade-off between the depth and the num-\n","ber of gates of the entire algorithm, SABRE outperforms\n","the best known algorithm with exponential speedup and\n","comparable or better results on various benchmarks.\n","CCS Concepts •Computer systems organization →\n","Quantum computing ;•Hardware →Quantum compu-\n","tation; Emerging languages and compilers .\n","Keywords Quantum Computing; Qubit Mapping; NISQ\n","ACM Reference Format:\n","Gushu Li, Yufei Ding, and Yuan Xie. 2019. Tackling the Qubit Map-\n","ping Problem for NISQ-Era Quantum Devices. In 2019 Architectural\n","Support for Programming Languages and Operating Systems (ASP-\n","LOS ’19), April 13–17, 2019, Providence, RI, USA. ACM, New York,\n","NY, USA, 14pages. https://doi.org/10.1145/3297858.3304023\n","Permission to make digital or hard copies of all or part of this work for\n","personal or classroom use is granted without fee provided that copies are not\n","made or distributed for profit or commercial advantage and that copies bear\n","this notice and the full citation on the first page. Copyrights for components\n","of this work owned by others than ACM must be honored. Abstracting with\n","credit is permitted. To copy otherwise, or republish, to post on servers or to\n","redistribute to lists, requires prior specific permission and/or a fee. Request\n","permissions from permissions@acm.org.\n","ASPLOS ’19, April 13–17, 2019, Providence, RI, USA\n","©2019 Association for Computing Machinery.\n","ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00\n","https://doi.org/10.1145/3297858.33040231 Introduction\n","Quantum Computing (QC) has been rapidly growing in the\n","last few decades because of its potential in various important\n","applications, including integer factorization [ 47], database\n","search [ 15], quantum simulation [ 36], etc. Recently, IBM,\n","Intel, and Google released their QC devices with 50, 49, and\n","72 qubits respectively [22, 23,56]. IBM and Rigetti also pro-\n","vide cloud QC services [ 18,40], allowing more people to\n","study real quantum hardware. We are expected to enter the\n","Noisy Intermediate-Scale Quantum (NISQ) era in the next\n","few years [ 39], when QC devices with dozens to hundreds of\n","qubits will be available. Though the number of qubits is insuf-\n","ficient for Quantum Error Correction (QEC), .it is expected\n","that these devices will be used to solve real-world problems\n","beyond the capability of available classical computers [ 5,38].\n","However, there exists a gap between quantum software\n","and hardware due to technology constraints in the NISQ\n","era. When designing a quantum program based on the most\n","popular circuit model, it is always assumed that qubits and\n","quantum operations are perfect and any quantum-physics-\n","allowed operations can be applied. But on NISQ hardware,\n","the qubits have limited coherence time, and quantum op-\n","erations are not perfect. Furthermore, only a subset of the-\n","oretically possible quantum operations can be directly im-\n","plemented, which calls for a modification in the quantum\n","program to fit the target platform.\n","In this paper, we will focus on the qubit mapping problem\n","caused by limited two-qubit coupling on NISQ devices. Two-\n","qubit gates are one important type of quantum operations\n","applied on two qubits. They can create quantum entangle-\n","ment, an advantage that does not exist in classical computing.\n","Two-qubit gates can be applied to arbitrary two logical qubits\n","in a quantum algorithm but this assumption does not hold\n","with NISQ devices. When running a quantum program, the\n","logical qubits need to be mapped to the physical qubits (an\n","analogy in classical computation is register allocation). But\n","for the physical qubits on NISQ devices, one qubit can only\n","couple with its neighbor qubits directly. So that for a specific\n","mapping, two-qubit gates can only be applied to limited log-\n","ical qubit pairs, whose corresponding physical qubit pairs\n","support direct coupling. This makes a quantum circuit not\n","directly executable on NISQ devices.\n","As a result, circuit transformation is required to make\n","the circuit compatible with NISQ device during compilation.\n","Session: Quantum Computing\n","ASPLOS’19, April 13–17, 2019, Providence, RI, USA\n","1001\n","\n","TOWARDS EFFICIENT ANSATZ ARCHITECTURE FOR\n","VARIATIONAL QUANTUM ALGORITHMS\n","A P REPRINT\n","Anbang Wu\n","Department of Computer Science\n","University of California, Santa Barbara\n","anbang@ucsb.eduGushu Li\n","Department of Electrical & Computer Engineering\n","University of California, Santa Barbara\n","gushuli@ece.ucsb.edu\n","Yuke Wang\n","Department of Computer Science\n","University of California, Santa Barbara\n","yuke_wang@cs.ucsb.edu\n","Boyuan Feng\n","Department of Computer Science\n","University of California, Santa Barbara\n","boyuan@cs.ucsb.edu\n","Yufei Ding\n","Department of Computer Science\n","University of California, Santa Barbara\n","yufeiding@cs.ucsb.edu\n","Yuan Xie\n","Department of Electrical & Computer Engineering\n","University of California, Santa Barbara\n","yuanxie@ucsb.edu\n","November 30, 2021\n","ABSTRACT\n","Variational quantum algorithms are the quantum analogy of the successful neural network in classical\n","machine learning. The ansatz architecture design, which is similar to classical neural network\n","architecture design, is critical to the performance of a variational quantum algorithm. In this paper,\n","we explore how to design efﬁcient ansatz and investigate several common design options in today’s\n","quantum software frameworks. In particular, we study the number of effective parameters in different\n","ansatz architectures and theoretically prove that an ansatz with parameterized RX and RZ gates\n","and alternating two-qubit gate entanglement layers would be more efﬁcient (i.e., more effective\n","parameters per two-qubit gate). Such ansatzes are expected to have stronger expressive power and\n","obtain better solutions. Numerical experimental results show that our efﬁcient ansatz architecture\n","outperforms other ansatzes with smaller ansatz size and better optimization results.arXiv:2111.13730v1  [quant-ph]  26 Nov 2021\n","\n","Article\n","Interpretable Forward and Inverse Design of\n","Particle Spectral Emissivity Using CommonMachine-Learning Models\n","Inverse design is usually done by expensive, slow, iterative optimization. Elzouka\n","et al. show how a simple machine-learning model (decision trees) can efﬁcientlyperform inverse design in one shot, while recovering design rules understandableby humans. Inverse design of the spectral emissivity of particles is used as an\n","example.\n","Mahmoud Elzouka, Charles\n","Yang, Adrian Albert, Ravi S.Prasher, Sean D. Lubner\n","rsprasher@lbl.gov (R.S.P.)\n","slubner@lbl.gov (S.D.L.)\n","HIGHLIGHTS\n","Radiative particles are ubiquitous\n","in\n","nature, and energy and\n","biomedical technologies\n","Particle spectral emissivity is\n","modeled and investigated usingsimple decision trees\n","The same trained model can solve\n","both the forward and inverseproblems\n","The highly interpretable models\n","return explicit design rules for theinverse problem\n","Elzouka et al., Cell Reports Physical Science 1,\n","100259\n","December 23, 2020 ª2020 The Author(s).\n","https://doi.org/10.1016/j.xcrp.2020.100259\n","ll\n","OPEN ACCESS\n","\n","Bringing Decentralized Search to Decentralized Services\n","Mingyu Li, Jinhao Zhu, Tianxu Zhang, Cheng Tan†, Yubin Xia, Sebastian Angel⋆, Haibo Chen\n","Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University\n","Shanghai AI Laboratory\n","Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China\n","†Northeastern University⋆University of Pennsylvania\n","Abstract\n","This paper addresses a key missing piece in the current ecosys-\n","tem of decentralized services and blockchain apps: the lack of\n","decentralized, verifiable, and private search. Existing decen-\n","tralized systems like Steemit, OpenBazaar, and the growing\n","number of blockchain apps provide alternatives to existing\n","services. And yet, they continue to rely on centralized search\n","engines and indexers to help users access the content they\n","seek and navigate the apps. Such centralized engines are in a\n","perfect position to censor content and violate users’ privacy,\n","undermining some of the key tenets behind decentralization.\n","To remedy this, we introduce DESEARCH , the first decen-\n","tralized search engine that guarantees the integrity and privacy\n","of search results for decentralized services and blockchain\n","apps. DESEARCH uses trusted hardware to build a network\n","of workers that execute a pipeline of small search engine\n","tasks (crawl, index, aggregate, rank, query). DESEARCH then\n","introduces a witness mechanism to make sure the completed\n","tasks can be reused across different pipelines, and to make\n","the final search results verifiable by end users. We implement\n","DESEARCH for two existing decentralized services that han-\n","dle over 80 million records and 240 GBs of data, and show\n","thatDESEARCH can scale horizontally with the number of\n","workers and can process 128 million search queries per day.\n","1 Introduction\n","Most of today’s online services—including search, social net-\n","works, and e-commerce—are centralized for reasons such\n","as economies of scale, compatible monetization strategies,\n","network effects, legal requirements, and technical limitations.\n","Yet, since the birth of the Internet, there have been periods of\n","intense interest in decentralization, including the peer-to-peer\n","systems bonanza of the early and mid 2000s [ 57,90,107] and\n","the current blockchain boom [ 39,94,113]. A rich set of de-\n","centralized services have appeared and are able to offer most\n","of the functionalities that common centralized online services\n","provide, as listed in Figure 1. Proponents of decentralization\n","argue that centralized services often employ anti-consumer\n","practices owing to their monopolistic positions [ 18,27], and\n","the mismatch between users’ expectations and operators’ in-\n","centives [ 44]. Further, centralized services are particularly sus-\n","ceptible to censorship [ 4,41] (either self-imposed or coerced\n","through technical or legal means) and collect vast amounts of\n","user information [13, 14].Service Centralized Decentralized\n","Currency U.S. Dollars Bitcoin [94]\n","Online Marketplace eBay OpenBazaar [28]\n","Social Media Twitter Steemit [40]\n","Video Sharing Youtube DTube [8]\n","Social Network Facebook Mastodon [16]\n","Public Storage DropBox IPFS [59]\n","Messaging Slack Matrix [25]\n","Video Conference Zoom Zipcall [50]\n","Website Hosting WiX [47] ZeroNet [49]\n","Financial Betting Etoro [12] Augur [1]\n","Supercomputing Titan [45] Golem [17]\n","Document Collaboration Google Docs Graphite [21]\n","FIGURE 1—Centralized services and decentralized alternatives.\n","While the idea of building fully decentralized services is\n","alluring, developers must currently make a significant com-\n","promise: they must defer search functionality to a central-\n","ized service. For example, OpenBazaar [ 28] makes a strong\n","case for a decentralized marketplace, but users must use a\n","centralized search engine such as Bazaar Dog [ 46] or Duo\n","Search [ 9] to discover what items are for sale in the first\n","place. A similar compromise is made by other popular ser-\n","vices [ 8,16,28,40,49]. This state of affairs is problematic be-\n","cause search is not an optional feature but rather a core compo-\n","nent of these systems. Without decentralized search, the pur-\n","ported goals of anti-censorship is hard to attain: the search en-\n","gine could trivially track users’ queries, and opaquely censor\n","particular content [ 3,4,7,32,41]. For example, Steemit [ 40]\n","is a decentralized social media service where posts are stored\n","on the public Steem blockchain [ 39], but Steemit developers\n","have been known to prevent users’ posts from appearing on\n","the front end site [41].\n","Prior proposals. Several search engines [ 29,81] propose\n","reaching consensus amongst replicas to ensure the correctness\n","of search indexes. However, these engines rely on a central\n","website hosted at the third party to answer queries. As a result,\n","an end-user who visits this website has no way to validate\n","the integrity of the displayed results, or to determine whether\n","there are missing entries. As an alternative, peer-to-peer-based\n","search engines [ 24,48] allow shared indexes between peers\n","and queries can be issued to any peer (essentially implement-\n","ing a distributed hash table). However, these engines do not\n","support verifiable indexes, and allow peers to monitor clients’\n","1\n","\n","Towards E￿cient Superconducting Quantum\n","Processor Architecture Design\n","Gushu Li\n","University of California\n","Santa Barbara, USA\n","gushuli@ece.ucsb.eduYufei Ding\n","University of California\n","Santa Barbara, USA\n","yufeiding@cs.ucsb.eduYuan Xie\n","University of California\n","Santa Barbara, USA\n","yuanxie@ece.ucsb.edu\n","Abstract\n","More computational resources (i.e., more physical qubits and\n","qubit connections) on a superconducting quantum processor\n","not only improve the performance but also result in more\n","complex chip architecture with lower yield rate. Optimizing\n","both of them simultaneously is a di￿cult problem due to\n","their intrinsic trade-o￿. Inspired by the application-speci￿c\n","design principle, this paper proposes an automatic design\n","￿ow to generate simpli￿ed superconducting quantum pro-\n","cessor architecture with negligible performance loss for dif-\n","ferent quantum programs. Our architecture-design-oriented\n","pro￿ling method identi￿es program components and pat-\n","terns critical to both the performance and the yield rate. A\n","follow-up hardware design￿ ow decomposes the complicated\n","design procedure into three subroutines, each of which fo-\n","cuses on di￿erent hardware components and cooperates with\n","corresponding pro￿ling results and physical constraints. Ex-\n","perimental results show that our design methodology could\n","outperform IBM’s general-purpose design schemes with bet-\n","ter Pareto-optimal results.CCS Concepts. •Hardware !Quantum computation\n",";•\n","Computer systems organization !Quantum comput-\n","ing.Keywords.\n","quantum computing; superconducting quantum\n","circuit; architecture design; application-speci￿c architecture\n","ACM Reference Format:\n","Gushu Li, Yufei Ding, and Yuan Xie. 2020. Towards E￿cient Super-\n","conducting Quantum Processor Architecture Design. In Proceedings\n","of the Twenty-Fifth International Conference on Architectural Support\n","for Programming Languages and Operating Systems (ASPLOS ’20),\n","March 16–20, 2020, Lausanne, Switzerland. ACM, New York, NY,\n","USA, 15pages. h￿ps://doi.org/10.1145/3373376.3378500\n","Permission to make digital or hard copies of all or part of this work for\n","personal or classroom use is granted without fee provided that copies are not\n","made or distributed for pro￿t or commercial advantage and that copies bearthis notice and the full citation on the￿ rst page. Copyrights for components\n","of this work owned by others than ACM must be honored. Abstracting withcredit is permitted. To copy otherwise, or republish, to post on servers or toredistribute to lists, requires prior speci￿c permission and/or a fee. Request\n","permissions from permissions@acm.org.\n","ASPLOS ’20, March 16–20, 2020, Lausanne, Switzerland\n","©2020 Association for Computing Machinery.\n","ACM ISBN 978-1-4503-7102-5/20/03. . . $15.00\n","h￿ps://doi.org/10.1145/3373376.33785001 Introduction\n","As a promising computation paradigm, Quantum Comput-\n","ing (QC) has been rapidly growing in the last two decades and\n","found its strong potential in many important areas, includingmachine learning [\n","14,20], chemistry simulation [ 32,39], etc.\n","In particular, the superconducting quantum circuit [ 13] has\n","become one of the most promising technique candidates for\n","building QC systems [ 5,9,37] due to the ever-increasing\n","qubit coherence time, individual qubit addressability, fabri-\n","cation technology scalability, etc. Towards e￿cient super-\n","conducting quantum circuit based QC systems, signi￿cant\n","research has recently been conducted, ranging from com-piler optimization [\n","34,47] to periphery control hardware\n","support [16, 50] and device innovation [27, 33].\n","Despite these system optimizations, the performance of a\n","superconducting quantum processor is still highly limited bythe amount of computation resource on it. Researchers have\n","been trying to integrate more qubits and qubit connections\n","on one superconducting quantum processor substrate. For\n","example, IBM’s￿ rst superconducting quantum chip on the\n","cloud has 5 qubits with 6 qubit connections, while its latest\n","published chip has 20 qubits with 37 qubit connections [ 10].\n","Increasing the number of physical qubits on a superconduct-\n","ing quantum processor allows programs with more logical\n","qubits to be executed. Denser qubit connections can increase\n","the overall chip performance by reducing the overhead of\n","qubit mapping and routing [28, 35,48,56].\n","Nevertheless, more qubits and qubit connections will, un-\n","fortunately, increase the probability of defect occurrence on\n","a chip, leading to lower yield rate and blocking future devel-\n","opment of larger-scale superconducting quantum processor.\n","For example, the yield rate of a 17-qubit chip can be lower\n","than 1% under IBM’s state-of-the-art technology [43]. Such\n","a low yield rate comes from frequency collision, a unique\n","defect on superconducting quantum processors [ 6,30]. The\n","frequencies of physically connected qubits may ‘collide’ witheach other when their values satisfy some speci￿c conditions.More qubit connections naturally increase the probability of\n","frequency collision and lower the yield rate.\n","To optimize both the yield rate and performance would\n","be desirable, but it is di￿cult in general due to the inherent\n","trade-o￿ between these two objectives. Most previous e￿orts\n","on them are direct device-level improvement [ 26,27,33,44],\n","Session 11B: Quantum computing — Who says \n","you can't watch two talks at once?\n"," \n","ASPLOS’20, March 16–20, 2020, Lausanne, Switzerland \n","1031\n","\n","TANGENT BUNDLE FILTERS AND NEURAL NETWORKS:\n","FROM MANIFOLDS TO CELLULAR SHEA VES AND BACK\n","C. Battiloro1,2, Z. Wang1, H. Riess3, P . Di Lorenzo2, A. Ribeiro1\n","1ESE Department, University of Pennsylvania, Philadelphia, USA\n","2DIET Department, Sapienza University of Rome, Rome, Italy\n","3ECE Department, Duke University, Durham, USA\n","E-mail: claudio.battiloro@uniroma1.it, zhiyangw@seas.upenn.edu\n","ABSTRACT\n","In this work we introduce a convolution operation over the tangent\n","bundle of Riemannian manifolds exploiting the Connection Lapla-\n","cian operator. We use the convolution to deﬁne tangent bundle ﬁl-\n","ters and tangent bundle neural networks (TNNs), novel continuous\n","architectures operating on tangent bundle signals, i.e. vector ﬁelds\n","over manifolds. We discretize TNNs both in space and time do-\n","mains, showing that their discrete counterpart is a principled vari-\n","ant of the recently introduced Sheaf Neural Networks. We formally\n","prove that this discrete architecture converges to the underlying con-\n","tinuous TNN. We numerically evaluate the effectiveness of the pro-\n","posed architecture on a denoising task of a tangent vector ﬁeld over\n","the unit 2-sphere.\n","Index Terms —Geometric Deep Learning, Tangent Bundle Sig-\n","nal Processing, Tangent Bundle Neural Networks, Cellular Sheaves\n","1. INTRODUCTION\n","The success of deep learning is mostly the success of Convolutional\n","Neural Networks (CNNs) [1]. CNNs have achieved impressive per-\n","formance in a wide range of applications showing good generaliza-\n","tion ability. Based on shift operators in the space domain, one (but\n","not the only one) key attribute is that the convolutional ﬁlters sat-\n","isfy the property of shift equivariance. Nowadays, data deﬁned on\n","irregular (non-Euclidean) domains are pervasive, with applications\n","ranging from detection and recommendation in social networks pro-\n","cessing [2], to resource allocations over wireless networks [3], or\n","point clouds for shape segmentation [4], just to name a few. For\n","this reason, the notions of shifts in CNNs have been adapted to\n","convolutional architectures on graphs (GNNs) [5, 6] as well as a\n","plethora of other structures, e.g. simplicial complexes [7–10], cell\n","complexes [11,12], and manifolds [13]. In [14], a framework for al-\n","gebraic neural networks has been proposed exploiting commutative\n","algebras. In this work we focus on tangent bundles, a formal tool for\n","describing and processing vector ﬁelds on manifolds, which are key\n","elements in tasks such as robot navigation or ﬂocking modeling.\n","Related Works. The renowned manifold assumption states that high\n","dimensional data examples are sampled from a low-dimensional\n","Riemannian manifold. This assumption is the fundamental block\n","of manifold learning, a class of methods for non-linear dimension-\n","ality reduction. Some of these methods approximate manifolds\n","with k-NN or geometric graphs via sampling points, i.e., for a ﬁne\n","enough sampling resolution, the graph Laplacian of the approxi-\n","mating graph “converges” to the Laplace-Beltrami operator of the\n","manifold [15]. These techniques rely on the eigenvalues and eigen-\n","vectors of the graph Laplacian [16], and they give rise to a novelperspective on manifold learning. In particular, the above approx-\n","imation leads to important transferability results of graph neural\n","networks (GNNs) [17,18], as well as to the introduction of Graphon\n","and Manifold Neural Networks, continuous architectures shown to\n","be limit objects of GNNs [19, 20]. However, most of the previ-\n","ous works focus on scalar signals, e.g. one or more scalar values\n","attached to each node of graphs or point of manifolds; recent devel-\n","opments [21] show that processing vector data deﬁned on tangent\n","bundles of manifolds or discrete vector bundles [22, 23] comes with\n","a series of beneﬁts. Moreover, the work in [24] proves that it is\n","possible to approximate both manifolds and their tangent bundles\n","with certain cellular sheaves obtained from a point cloud via k-NN\n","and Local PCA, such that, for a ﬁne enough sampling resolution,\n","the Sheaf Laplacian of the approximating sheaf “converges” to the\n","Connection Laplacian operator. Finally, the work in [25] generalizes\n","the result of [24] by proving the spectral convergence of a large class\n","of Laplacian operators via the Principal Bundle set up.\n","Contributions. In this work we deﬁne a convolution operation over\n","the tangent bundles of Riemannian manifolds with the Connection\n","Laplacian operator. Our deﬁnition is consistent, i.e. it reduces to\n","manifold convolution [19] in the one-dimensional bundle case, and\n","to the standard convolution if the manifold is the real line. We intro-\n","duce tangent bundle convolutional ﬁlters to process tangent bundle\n","signals (i.e. vector ﬁelds over manifolds), we deﬁne a frequency\n","representation for them and, by cascading layers consisting of tan-\n","gent bundle ﬁlters banks and nonlinearities, we introduce Tangent\n","Bundle Neural Networks (TNNs). We then discretize the TNNs in\n","the space domain by sampling points on the manifold and build-\n","ing a cellular sheaf [26] representing a legit approximation of both\n","the manifold and its tangent bundle [24]. We formally prove that\n","the discretized architecture over the cellular sheaf converges to the\n","underlying TNN as the number of sampled points increases. More-\n","over, we further discretize the architecture in the time domain by\n","sampling the ﬁlter impulse function in discrete and ﬁnite time steps,\n","showing that space-time discretized TNNs are a principled variant\n","of the very recently introduced Sheaf Neural Networks [23, 27, 28],\n","discrete architectures operating on cellular sheaves and generalizing\n","graph neural networks. Finally, we numerically evaluate the perfor-\n","mance of TNNs on a denoising task of a tangent vector ﬁeld of the\n","unit 2-sphere.\n","Paper Outline. The paper is organized as follows. We start with\n","some preliminary concepts in Section 2. We deﬁne the tangent bun-\n","dle convolution and ﬁlters in Section 3, and Tangent Bundle Neural\n","Networks (TNNs) in Section 4. In Section 5, we discretize TNNs in\n","space and time domains, showing that discretized TNNs are Sheaf\n","Neural Networks and proving the convergence result. Numerical re-\n","sults are in Section 6 and conclusions are in Section 7.arXiv:2210.15058v3  [eess.SP]  18 Nov 2022:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","Query: What are key features of Weijie Su's work?\n","................................................................................\n","--------------------------------------------------------------------------------\n","REFERENCE #0\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Eric Fouh_AVpedagogypost.pdf\n","................................................................................\n","Content: \n","The Role of Visualization in Computer Science Education       ERIC FOUH, MONIKA AKBAR, AND CLIFFORD A. SHAFFER\n","Department of Computer Science   Virginia Tech   Blacksburg, VA 24061       Computer Science core instruction attempts\n","to provide a detailed understanding o f dy- namic processes such as the working of an algorithm or the flow of\n","information between  computing ent ities. Such dynamic processes are not well explained by static media such  as text\n","and images, and are difficult to convey in le cture. We survey the hist ory of visual i- zation in CS education, foc\n","using on artifacts that have  a documented positive educational  assessment. We discuss how changes in computing\n","technology have affected the deve l- opment and uptake of such visualization artifacts in CS education, and how recent\n","tech- nology changes are leading to progress in developing online hypertextbooks.  KEYWORDS: Algorithm visualization,\n","data structure visualization, program visualiz a- tion, eTextbooks, hypertextbooks.      (Draft of paper to appear in\n","Computers in the Schools , 2012)\n","--------------------------------------------------------------------------------\n","REFERENCE #1\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Brett Hemenway _2015-1250.pdf\n","................................................................................\n","Content: \n","Adaptively Secure Garbled Circuits from One-Way Functions∗ Brett Hemenway†Zahra Jafargholi‡Rafail Ostrovsky§Alessandra\n","Scafuro¶ Daniel Wichs∥ Abstract A garbling scheme is used to garble a circuit Cand an input xin a way that reveals the\n","output C(x) but hides everything else. In many settings, the circuit can be garbled oﬀ-line without strict eﬃciency\n","constraints, but the input must be garbled very eﬃciently on-line , with much lower complexity than evaluating the\n","circuit. Yao’s scheme has essentially optimal on-line complexity, but only achieves selective security , where the\n","adversary must choose the input xprior to seeing the garbled circuit. It has remained an open problem to achieve\n","adaptive security , where the adversary can choose xafter seeing the garbled circuit, while preserving on-line eﬃciency.\n","In this work, we modify Yao’s scheme in a way that allows us to prove adaptive security under one-way functions. As our\n","main instantiation, we get a scheme where the on-line complexity is only proportional to the width wof the circuit,\n","which corresponds to the space complexity of the computation, but is independent of the circuit’s depth d. Alternately,\n","we can also get an instantiation where the on-line complexity is only proportional to the input/output size and the\n","depth dof the circuit but independent of its width w, albeit in this case we incur a 2O(d)security loss in our\n","reduction. More broadly, we relate the on-line complexity of adaptively secure garbling schemes in our framework to a\n","certain type of pebble complexity of the circuit. As our main tool, of independent interest, we develop a new notion of\n","somewhere equivocal encryption, which allows us to eﬃciently equivocate on a small subset of the message bits. ∗This\n","work was done in part while some of the authors were visiting the Simons Institute for the Theory of Computing,\n","supported by the Simons Foundation and by the DIMACS/Simons Collaboration in Cryptography through NSF grant CNS-\n","1523467. †University of Pennsylvania. Department of Computer Science. fbrett@cis.upenn.edu ‡Northeastern University.\n","Department of Computer Science. zahra@ccs.neu.edu §University of California, Los Angeles. Department of Computer Science\n","and Mathematics. rafail@cs.ucla.edu Research supported in part by NSF grants 09165174, 1065276, 1118126 and 1136174, US-\n","Israel BSF grant 2008411, OKAWA Foundation Research Award, IBM Faculty Research Award, Xerox Faculty Research Award, B.\n","John Garrick Foundation Award, Teradata Research Award, and Lockheed-Martin Corporation Research Award. This material is\n","based upon work supported in part by DARPA Safeware program. The views expressed are those of the author and do not\n","reﬂect the oﬃcial policy or position of the Department of Defense or the U.S. Government. ¶Boston University and\n","Northeastern University. Department of Computer Science. scafuro@bu.edu Research supported by NSF grants 1012798.\n","∥Northeastern University. Department of Computer Science. wichs@ccs.neu.edu . Research supported by NSF grants\n","CNS-1347350, CNS-1314722, CNS-1413964.\n","--------------------------------------------------------------------------------\n","REFERENCE #2\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Damon Centola_Chapter-17-social-comparison.pdf\n","................................................................................\n","Content: \n","1  Chapter 17. How Social Networks Shape Social Comparison     Jingwen Zhang (University of California, Davis)   Damon\n","Centola (Annenberg School for Communication/University of Pennsylvania)   Abstract: While social comparison research has\n","focused on  the processes and consequences  of how the comparer gleans information from the comparison other (individual\n","or group) ,  recent research on social networks demonstrates how information and influence is  distributed across\n","persons in a network. This chapter r eviews social influence  processes in  social networks . We first review recent\n","research on social compar ison and its negative  consequences in online social networks. Then we delve into discussing\n","the social network  causes of biased social perceptions online and  how this can be remedied by building more  accurate\n","perceptions through constructed online networks.  Lastly , we discuss findings from  recent experimental studies  that\n","illustrate how construc ted online networks can harness  social comparison to induce significant changes in health\n","behavior.\n","--------------------------------------------------------------------------------\n","REFERENCE #3\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Steven Zdancemic_3093333.3009894.pdf\n","................................................................................\n","Content: \n","QWIRE : A Core Language for Quantum Circuits Jennifer Paykin Robert Rand Steve Zdancewic University of Pennsylvania, USA\n","jpaykin@seas.upenn.edu, rrand@seas.upenn.edu, stevez@cis.upenn.edu Abstract This paper introduces QWIRE (“choir”), a\n","language for deﬁning quantum circuits and an interface for manipulating them inside of an arbitrary classical host\n","language. QWIRE is minimal—it con- tains only a few primitives—and sound with respect to the phys- ical properties\n","entailed by quantum mechanics. At the same time, QWIRE is expressive and highly modular due to its relationship with the\n","host language, mirroring the QRAM model of computation that places a quantum computer (controlled by circuits) alongside\n","a classical computer (controlled by the host language). We presentQWIRE along with its type system and operational\n","semantics, which we prove is safe and strongly normalizing when- ever the host language is. We give circuits a\n","denotational semantics in terms of density matrices. Throughout, we investigate examples that demonstrate the expressive\n","power of QWIRE , including exten- sions to the host language that (1) expose a general analysis frame- work for\n","circuits, and (2) provide dependent types. Categories and Subject Descriptors D.3.1 [ Programming Lan- guages ]: Formal\n","Deﬁnitions and Theory; F.3.1 [ Logics and Mean- ings of Programs ]: Specifying and Verifying and Reasoning about\n","Programs Keywords quantum programming languages, quantum circuits, linear types, denotational semantics 1. Introduction\n","The standard architecture for quantum computers follows the quan- tum circuit model , which presents quantum\n","computations as se- quences of gates over qubits (the quantum analogue of bits). As with classical circuits, quantum\n","circuits exist at a very low level of abstraction, and yet in spite of this, researchers and industry professionals\n","write complex quantum algorithms in state-of-the-art quantum circuit languages like Quipper (Green et al. 2013a) and\n","LIQUi|⟩(Wecker and Svore 2014). Why is the quantum circuit model so successful? In part, it is due to the fact that\n","quantum data like qubits are extremely unintu- itive from a classical perspective. Research into simple operations on\n","quantum data, such as qubit-controlled conditionals and recur- sion, is still in its infancy (Ying 2014; Badescu and\n","Panangaden 2015), so programmers cannot be sure that their algorithms using such abstractions are valid quantum-\n","mechanically.Although circuits manipulate quantum data, they themselves are classical data—a circuit is just a sequence\n","of instructions describ- ing how to apply gates to wires. In practice this means that circuits can be used to build up\n","layers of abstractions hiding the low-level details. The QRAM model of quantum computing (Knill 1996) for- malizes this\n","intuition by describing how a quantum computer could work in tandem with a classical computer. In the QRAM model the\n","classical computer handles the majority of ordinary tasks, while the quantum computer performs specialized quantum\n","operations. To communicate, the classical computer sends instructions to the quantum machine in the form of quantum\n","circuits. Over the course of execution, the quantum computer sends measurement results back to the classical computer as\n","needed. Classical ComputerQuantum Computercircuits measurement results Embedded languages like Quipper, LIQ Ui|⟩, the Q\n","language (Bet- telli et al. 2003), and the quantum IO monad (Altenkirch and Green 2010) can be thought of as\n","instantiations of this model. They exe- cute by running host language programs on the classical computer, making\n","specialized calls to the (hypothetical) quantum machine. The classical host languages allow programmers to easily build\n","up high-level abstractions out of low-level quantum operations. However, such abstractions are only worthwhile if the\n","circuits they produce are safe—if they do not cause errors when executed on a quantum computer. Unfortunately, proving\n","that an embedded language produces well-formed circuits is hard because it means reasoning about the entirety of the\n","classical host language. This is frustrating when we care most about the correctness of quantum programs, which we\n","expect to be both more expensive and error- prone than the embedded language’s classical programs. One way of ensuring\n","the safety of circuits is via a strong type system. Type safety for a quantum programming language means that well-\n","formed circuits will not get stuck or “go wrong” when executed on a quantum machine. A subtlety is that this deﬁnition\n","implies that the quantum program is even implementable on a quan- tum computer—that the high-level operational view of\n","the language is compatible with quantum physics. One way of ensuring that the language is implementable is to give a\n","denotational semantics for programs in terms of quantum mechanics. Several quantum programming languages have been\n","proposed with an emphasis on type safety, including Selinger’s QPL lan- guage (Selinger 2004), the quantum lambda\n","calculus (Selinger and Valiron 2009), QML (Altenkirch and Grattage 2005), and Proto- Quipper (Ross 2015). However, these\n","are toy languages, not de- signed for implementation in a conventional programming environ- ment. Permission to make\n","digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\n","copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full\n","citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting\n","with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\n","prior speciﬁc permission and/or a fee. Request permissions from Permissions@acm.org. Copyright is held by the\n","owner/author(s). Publication rights licensed to ACM. POPL’17 , January 15–21, 2017, Paris, France ACM.\n","978-1-4503-4660-3/17/01...$15.00 http://dx.doi.org/10.1145/3009837.3009894 846\n","--------------------------------------------------------------------------------\n","REFERENCE #4\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Gushu Li_3297858.3304023.pdf\n","................................................................................\n","Content: \n","Tackling the Qubit Mapping Problem for NISQ-Era Quantum Devices Gushu Li University of California Santa Barbara, CA\n","gushuli@ece.ucsb.eduYufei Ding University of California Santa Barbara, CA yufeiding@cs.ucsb.eduYuan Xie University of\n","California Santa Barbara, CA yuanxie@ece.ucsb.edu Abstract Due to little consideration in the hardware constraints,\n","e.g., limited connections between physical qubits to enable two- qubit gates, most quantum algorithms cannot be directly\n","executed on the Noisy Intermediate-Scale Quantum (NISQ) devices. Dynamically remapping logical qubits to physical qubits\n","in the compiler is needed to enable the two-qubit gates in the algorithm, which introduces additional oper- ations and\n","inevitably reduces the fidelity of the algorithm. Previous solutions in finding such remapping suffer from high\n","complexity, poor initial mapping quality, and limited flexibility and controllability. To address these drawbacks\n","mentioned above, this paper proposes a SWAP-based BidiREctional heuristic search al- gorithm (SABRE), which is\n","applicable to NISQ devices with arbitrary connections between qubits. By optimizing every search attempt, globally\n","optimizing the initial mapping using a novel reverse traversal technique, introducing the decay effect to enable the\n","trade-off between the depth and the num- ber of gates of the entire algorithm, SABRE outperforms the best known\n","algorithm with exponential speedup and comparable or better results on various benchmarks. CCS Concepts •Computer\n","systems organization → Quantum computing ;•Hardware →Quantum compu- tation; Emerging languages and compilers . Keywords\n","Quantum Computing; Qubit Mapping; NISQ ACM Reference Format: Gushu Li, Yufei Ding, and Yuan Xie. 2019. Tackling the\n","Qubit Map- ping Problem for NISQ-Era Quantum Devices. In 2019 Architectural Support for Programming Languages and\n","Operating Systems (ASP- LOS ’19), April 13–17, 2019, Providence, RI, USA. ACM, New York, NY, USA, 14pages.\n","https://doi.org/10.1145/3297858.3304023 Permission to make digital or hard copies of all or part of this work for\n","personal or classroom use is granted without fee provided that copies are not made or distributed for profit or\n","commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\n","of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or\n","republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request\n","permissions from permissions@acm.org. ASPLOS ’19, April 13–17, 2019, Providence, RI, USA ©2019 Association for Computing\n","Machinery. ACM ISBN 978-1-4503-6240-5/19/04. . . $15.00 https://doi.org/10.1145/3297858.33040231 Introduction Quantum\n","Computing (QC) has been rapidly growing in the last few decades because of its potential in various important\n","applications, including integer factorization [ 47], database search [ 15], quantum simulation [ 36], etc. Recently,\n","IBM, Intel, and Google released their QC devices with 50, 49, and 72 qubits respectively [22, 23,56]. IBM and Rigetti\n","also pro- vide cloud QC services [ 18,40], allowing more people to study real quantum hardware. We are expected to enter\n","the Noisy Intermediate-Scale Quantum (NISQ) era in the next few years [ 39], when QC devices with dozens to hundreds of\n","qubits will be available. Though the number of qubits is insuf- ficient for Quantum Error Correction (QEC), .it is\n","expected that these devices will be used to solve real-world problems beyond the capability of available classical\n","computers [ 5,38]. However, there exists a gap between quantum software and hardware due to technology constraints in\n","the NISQ era. When designing a quantum program based on the most popular circuit model, it is always assumed that qubits\n","and quantum operations are perfect and any quantum-physics- allowed operations can be applied. But on NISQ hardware, the\n","qubits have limited coherence time, and quantum op- erations are not perfect. Furthermore, only a subset of the-\n","oretically possible quantum operations can be directly im- plemented, which calls for a modification in the quantum\n","program to fit the target platform. In this paper, we will focus on the qubit mapping problem caused by limited two-\n","qubit coupling on NISQ devices. Two- qubit gates are one important type of quantum operations applied on two qubits.\n","They can create quantum entangle- ment, an advantage that does not exist in classical computing. Two-qubit gates can be\n","applied to arbitrary two logical qubits in a quantum algorithm but this assumption does not hold with NISQ devices. When\n","running a quantum program, the logical qubits need to be mapped to the physical qubits (an analogy in classical\n","computation is register allocation). But for the physical qubits on NISQ devices, one qubit can only couple with its\n","neighbor qubits directly. So that for a specific mapping, two-qubit gates can only be applied to limited log- ical qubit\n","pairs, whose corresponding physical qubit pairs support direct coupling. This makes a quantum circuit not directly\n","executable on NISQ devices. As a result, circuit transformation is required to make the circuit compatible with NISQ\n","device during compilation. Session: Quantum Computing ASPLOS’19, April 13–17, 2019, Providence, RI, USA 1001\n","--------------------------------------------------------------------------------\n","REFERENCE #5\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Gushu Li_2111.13730.pdf\n","................................................................................\n","Content: \n","TOWARDS EFFICIENT ANSATZ ARCHITECTURE FOR VARIATIONAL QUANTUM ALGORITHMS A P REPRINT Anbang Wu Department of Computer\n","Science University of California, Santa Barbara anbang@ucsb.eduGushu Li Department of Electrical & Computer Engineering\n","University of California, Santa Barbara gushuli@ece.ucsb.edu Yuke Wang Department of Computer Science University of\n","California, Santa Barbara yuke_wang@cs.ucsb.edu Boyuan Feng Department of Computer Science University of California,\n","Santa Barbara boyuan@cs.ucsb.edu Yufei Ding Department of Computer Science University of California, Santa Barbara\n","yufeiding@cs.ucsb.edu Yuan Xie Department of Electrical & Computer Engineering University of California, Santa Barbara\n","yuanxie@ucsb.edu November 30, 2021 ABSTRACT Variational quantum algorithms are the quantum analogy of the successful\n","neural network in classical machine learning. The ansatz architecture design, which is similar to classical neural\n","network architecture design, is critical to the performance of a variational quantum algorithm. In this paper, we\n","explore how to design efﬁcient ansatz and investigate several common design options in today’s quantum software\n","frameworks. In particular, we study the number of effective parameters in different ansatz architectures and\n","theoretically prove that an ansatz with parameterized RX and RZ gates and alternating two-qubit gate entanglement layers\n","would be more efﬁcient (i.e., more effective parameters per two-qubit gate). Such ansatzes are expected to have stronger\n","expressive power and obtain better solutions. Numerical experimental results show that our efﬁcient ansatz architecture\n","outperforms other ansatzes with smaller ansatz size and better optimization results.arXiv:2111.13730v1  [quant-ph]  26\n","Nov 2021\n","--------------------------------------------------------------------------------\n","REFERENCE #6\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Charles Yang_PIIS2666386420302812.pdf\n","................................................................................\n","Content: \n","Article Interpretable Forward and Inverse Design of Particle Spectral Emissivity Using CommonMachine-Learning Models\n","Inverse design is usually done by expensive, slow, iterative optimization. Elzouka et al. show how a simple machine-\n","learning model (decision trees) can efﬁcientlyperform inverse design in one shot, while recovering design rules\n","understandableby humans. Inverse design of the spectral emissivity of particles is used as an example. Mahmoud Elzouka,\n","Charles Yang, Adrian Albert, Ravi S.Prasher, Sean D. Lubner rsprasher@lbl.gov (R.S.P.) slubner@lbl.gov (S.D.L.)\n","HIGHLIGHTS Radiative particles are ubiquitous in nature, and energy and biomedical technologies Particle spectral\n","emissivity is modeled and investigated usingsimple decision trees The same trained model can solve both the forward and\n","inverseproblems The highly interpretable models return explicit design rules for theinverse problem Elzouka et al., Cell\n","Reports Physical Science 1, 100259 December 23, 2020 ª2020 The Author(s). https://doi.org/10.1016/j.xcrp.2020.100259 ll\n","OPEN ACCESS\n","--------------------------------------------------------------------------------\n","REFERENCE #7\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Sebastian Angel_desearch-osdi21.pdf\n","................................................................................\n","Content: \n","Bringing Decentralized Search to Decentralized Services Mingyu Li, Jinhao Zhu, Tianxu Zhang, Cheng Tan†, Yubin Xia,\n","Sebastian Angel⋆, Haibo Chen Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University Shanghai AI\n","Laboratory Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China †Northeastern\n","University⋆University of Pennsylvania Abstract This paper addresses a key missing piece in the current ecosys- tem of\n","decentralized services and blockchain apps: the lack of decentralized, verifiable, and private search. Existing decen-\n","tralized systems like Steemit, OpenBazaar, and the growing number of blockchain apps provide alternatives to existing\n","services. And yet, they continue to rely on centralized search engines and indexers to help users access the content\n","they seek and navigate the apps. Such centralized engines are in a perfect position to censor content and violate users’\n","privacy, undermining some of the key tenets behind decentralization. To remedy this, we introduce DESEARCH , the first\n","decen- tralized search engine that guarantees the integrity and privacy of search results for decentralized services and\n","blockchain apps. DESEARCH uses trusted hardware to build a network of workers that execute a pipeline of small search\n","engine tasks (crawl, index, aggregate, rank, query). DESEARCH then introduces a witness mechanism to make sure the\n","completed tasks can be reused across different pipelines, and to make the final search results verifiable by end users.\n","We implement DESEARCH for two existing decentralized services that han- dle over 80 million records and 240 GBs of data,\n","and show thatDESEARCH can scale horizontally with the number of workers and can process 128 million search queries per\n","day. 1 Introduction Most of today’s online services—including search, social net- works, and e-commerce—are centralized\n","for reasons such as economies of scale, compatible monetization strategies, network effects, legal requirements, and\n","technical limitations. Yet, since the birth of the Internet, there have been periods of intense interest in\n","decentralization, including the peer-to-peer systems bonanza of the early and mid 2000s [ 57,90,107] and the current\n","blockchain boom [ 39,94,113]. A rich set of de- centralized services have appeared and are able to offer most of the\n","functionalities that common centralized online services provide, as listed in Figure 1. Proponents of decentralization\n","argue that centralized services often employ anti-consumer practices owing to their monopolistic positions [ 18,27], and\n","the mismatch between users’ expectations and operators’ in- centives [ 44]. Further, centralized services are\n","particularly sus- ceptible to censorship [ 4,41] (either self-imposed or coerced through technical or legal means) and\n","collect vast amounts of user information [13, 14].Service Centralized Decentralized Currency U.S. Dollars Bitcoin [94]\n","Online Marketplace eBay OpenBazaar [28] Social Media Twitter Steemit [40] Video Sharing Youtube DTube [8] Social Network\n","Facebook Mastodon [16] Public Storage DropBox IPFS [59] Messaging Slack Matrix [25] Video Conference Zoom Zipcall [50]\n","Website Hosting WiX [47] ZeroNet [49] Financial Betting Etoro [12] Augur [1] Supercomputing Titan [45] Golem [17]\n","Document Collaboration Google Docs Graphite [21] FIGURE 1—Centralized services and decentralized alternatives. While the\n","idea of building fully decentralized services is alluring, developers must currently make a significant com- promise:\n","they must defer search functionality to a central- ized service. For example, OpenBazaar [ 28] makes a strong case for a\n","decentralized marketplace, but users must use a centralized search engine such as Bazaar Dog [ 46] or Duo Search [ 9] to\n","discover what items are for sale in the first place. A similar compromise is made by other popular ser- vices [\n","8,16,28,40,49]. This state of affairs is problematic be- cause search is not an optional feature but rather a core\n","compo- nent of these systems. Without decentralized search, the pur- ported goals of anti-censorship is hard to attain:\n","the search en- gine could trivially track users’ queries, and opaquely censor particular content [ 3,4,7,32,41]. For\n","example, Steemit [ 40] is a decentralized social media service where posts are stored on the public Steem blockchain [\n","39], but Steemit developers have been known to prevent users’ posts from appearing on the front end site [41]. Prior\n","proposals. Several search engines [ 29,81] propose reaching consensus amongst replicas to ensure the correctness of\n","search indexes. However, these engines rely on a central website hosted at the third party to answer queries. As a\n","result, an end-user who visits this website has no way to validate the integrity of the displayed results, or to\n","determine whether there are missing entries. As an alternative, peer-to-peer-based search engines [ 24,48] allow shared\n","indexes between peers and queries can be issued to any peer (essentially implement- ing a distributed hash table).\n","However, these engines do not support verifiable indexes, and allow peers to monitor clients’ 1\n","--------------------------------------------------------------------------------\n","REFERENCE #8\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Gushu Li_3373376.3378500.pdf\n","................................................................................\n","Content: \n","Towards E￿cient Superconducting Quantum Processor Architecture Design Gushu Li University of California Santa Barbara,\n","USA gushuli@ece.ucsb.eduYufei Ding University of California Santa Barbara, USA yufeiding@cs.ucsb.eduYuan Xie University\n","of California Santa Barbara, USA yuanxie@ece.ucsb.edu Abstract More computational resources (i.e., more physical qubits\n","and qubit connections) on a superconducting quantum processor not only improve the performance but also result in more\n","complex chip architecture with lower yield rate. Optimizing both of them simultaneously is a di￿cult problem due to\n","their intrinsic trade-o￿. Inspired by the application-speci￿c design principle, this paper proposes an automatic design\n","￿ow to generate simpli￿ed superconducting quantum pro- cessor architecture with negligible performance loss for dif-\n","ferent quantum programs. Our architecture-design-oriented pro￿ling method identi￿es program components and pat- terns\n","critical to both the performance and the yield rate. A follow-up hardware design￿ ow decomposes the complicated design\n","procedure into three subroutines, each of which fo- cuses on di￿erent hardware components and cooperates with\n","corresponding pro￿ling results and physical constraints. Ex- perimental results show that our design methodology could\n","outperform IBM’s general-purpose design schemes with bet- ter Pareto-optimal results.CCS Concepts. •Hardware !Quantum\n","computation ;• Computer systems organization !Quantum comput- ing.Keywords. quantum computing; superconducting quantum\n","circuit; architecture design; application-speci￿c architecture ACM Reference Format: Gushu Li, Yufei Ding, and Yuan Xie.\n","2020. Towards E￿cient Super- conducting Quantum Processor Architecture Design. In Proceedings of the Twenty-Fifth\n","International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’20), March\n","16–20, 2020, Lausanne, Switzerland. ACM, New York, NY, USA, 15pages. h￿ps://doi.org/10.1145/3373376.3378500 Permission\n","to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided\n","that copies are not made or distributed for pro￿t or commercial advantage and that copies bearthis notice and the full\n","citation on the￿ rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting\n","withcredit is permitted. To copy otherwise, or republish, to post on servers or toredistribute to lists, requires prior\n","speci￿c permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS ’20, March 16–20, 2020, Lausanne,\n","Switzerland ©2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7102-5/20/03. . . $15.00\n","h￿ps://doi.org/10.1145/3373376.33785001 Introduction As a promising computation paradigm, Quantum Comput- ing (QC) has\n","been rapidly growing in the last two decades and found its strong potential in many important areas, includingmachine\n","learning [ 14,20], chemistry simulation [ 32,39], etc. In particular, the superconducting quantum circuit [ 13] has\n","become one of the most promising technique candidates for building QC systems [ 5,9,37] due to the ever-increasing qubit\n","coherence time, individual qubit addressability, fabri- cation technology scalability, etc. Towards e￿cient super-\n","conducting quantum circuit based QC systems, signi￿cant research has recently been conducted, ranging from com-piler\n","optimization [ 34,47] to periphery control hardware support [16, 50] and device innovation [27, 33]. Despite these\n","system optimizations, the performance of a superconducting quantum processor is still highly limited bythe amount of\n","computation resource on it. Researchers have been trying to integrate more qubits and qubit connections on one\n","superconducting quantum processor substrate. For example, IBM’s￿ rst superconducting quantum chip on the cloud has 5\n","qubits with 6 qubit connections, while its latest published chip has 20 qubits with 37 qubit connections [ 10].\n","Increasing the number of physical qubits on a superconduct- ing quantum processor allows programs with more logical\n","qubits to be executed. Denser qubit connections can increase the overall chip performance by reducing the overhead of\n","qubit mapping and routing [28, 35,48,56]. Nevertheless, more qubits and qubit connections will, un- fortunately,\n","increase the probability of defect occurrence on a chip, leading to lower yield rate and blocking future devel- opment\n","of larger-scale superconducting quantum processor. For example, the yield rate of a 17-qubit chip can be lower than 1%\n","under IBM’s state-of-the-art technology [43]. Such a low yield rate comes from frequency collision, a unique defect on\n","superconducting quantum processors [ 6,30]. The frequencies of physically connected qubits may ‘collide’ witheach other\n","when their values satisfy some speci￿c conditions.More qubit connections naturally increase the probability of frequency\n","collision and lower the yield rate. To optimize both the yield rate and performance would be desirable, but it is\n","di￿cult in general due to the inherent trade-o￿ between these two objectives. Most previous e￿orts on them are direct\n","device-level improvement [ 26,27,33,44], Session 11B: Quantum computing — Who says  you can't watch two talks at once?\n","ASPLOS’20, March 16–20, 2020, Lausanne, Switzerland  1031\n","--------------------------------------------------------------------------------\n","REFERENCE #9\n","--------------------------------------------------------------------------------\n","Document Source: TrainingDataSmall/Alejandro Ribeiro_2210.15058.pdf\n","................................................................................\n","Content: \n","TANGENT BUNDLE FILTERS AND NEURAL NETWORKS: FROM MANIFOLDS TO CELLULAR SHEA VES AND BACK C. Battiloro1,2, Z. Wang1, H.\n","Riess3, P . Di Lorenzo2, A. Ribeiro1 1ESE Department, University of Pennsylvania, Philadelphia, USA 2DIET Department,\n","Sapienza University of Rome, Rome, Italy 3ECE Department, Duke University, Durham, USA E-mail:\n","claudio.battiloro@uniroma1.it, zhiyangw@seas.upenn.edu ABSTRACT In this work we introduce a convolution operation over\n","the tangent bundle of Riemannian manifolds exploiting the Connection Lapla- cian operator. We use the convolution to\n","deﬁne tangent bundle ﬁl- ters and tangent bundle neural networks (TNNs), novel continuous architectures operating on\n","tangent bundle signals, i.e. vector ﬁelds over manifolds. We discretize TNNs both in space and time do- mains, showing\n","that their discrete counterpart is a principled vari- ant of the recently introduced Sheaf Neural Networks. We formally\n","prove that this discrete architecture converges to the underlying con- tinuous TNN. We numerically evaluate the\n","effectiveness of the pro- posed architecture on a denoising task of a tangent vector ﬁeld over the unit 2-sphere. Index\n","Terms —Geometric Deep Learning, Tangent Bundle Sig- nal Processing, Tangent Bundle Neural Networks, Cellular Sheaves 1.\n","INTRODUCTION The success of deep learning is mostly the success of Convolutional Neural Networks (CNNs) [1]. CNNs have\n","achieved impressive per- formance in a wide range of applications showing good generaliza- tion ability. Based on shift\n","operators in the space domain, one (but not the only one) key attribute is that the convolutional ﬁlters sat- isfy the\n","property of shift equivariance. Nowadays, data deﬁned on irregular (non-Euclidean) domains are pervasive, with\n","applications ranging from detection and recommendation in social networks pro- cessing [2], to resource allocations over\n","wireless networks [3], or point clouds for shape segmentation [4], just to name a few. For this reason, the notions of\n","shifts in CNNs have been adapted to convolutional architectures on graphs (GNNs) [5, 6] as well as a plethora of other\n","structures, e.g. simplicial complexes [7–10], cell complexes [11,12], and manifolds [13]. In [14], a framework for al-\n","gebraic neural networks has been proposed exploiting commutative algebras. In this work we focus on tangent bundles, a\n","formal tool for describing and processing vector ﬁelds on manifolds, which are key elements in tasks such as robot\n","navigation or ﬂocking modeling. Related Works. The renowned manifold assumption states that high dimensional data\n","examples are sampled from a low-dimensional Riemannian manifold. This assumption is the fundamental block of manifold\n","learning, a class of methods for non-linear dimension- ality reduction. Some of these methods approximate manifolds with\n","k-NN or geometric graphs via sampling points, i.e., for a ﬁne enough sampling resolution, the graph Laplacian of the\n","approxi- mating graph “converges” to the Laplace-Beltrami operator of the manifold [15]. These techniques rely on the\n","eigenvalues and eigen- vectors of the graph Laplacian [16], and they give rise to a novelperspective on manifold\n","learning. In particular, the above approx- imation leads to important transferability results of graph neural networks\n","(GNNs) [17,18], as well as to the introduction of Graphon and Manifold Neural Networks, continuous architectures shown\n","to be limit objects of GNNs [19, 20]. However, most of the previ- ous works focus on scalar signals, e.g. one or more\n","scalar values attached to each node of graphs or point of manifolds; recent devel- opments [21] show that processing\n","vector data deﬁned on tangent bundles of manifolds or discrete vector bundles [22, 23] comes with a series of beneﬁts.\n","Moreover, the work in [24] proves that it is possible to approximate both manifolds and their tangent bundles with\n","certain cellular sheaves obtained from a point cloud via k-NN and Local PCA, such that, for a ﬁne enough sampling\n","resolution, the Sheaf Laplacian of the approximating sheaf “converges” to the Connection Laplacian operator. Finally,\n","the work in [25] generalizes the result of [24] by proving the spectral convergence of a large class of Laplacian\n","operators via the Principal Bundle set up. Contributions. In this work we deﬁne a convolution operation over the tangent\n","bundles of Riemannian manifolds with the Connection Laplacian operator. Our deﬁnition is consistent, i.e. it reduces to\n","manifold convolution [19] in the one-dimensional bundle case, and to the standard convolution if the manifold is the\n","real line. We intro- duce tangent bundle convolutional ﬁlters to process tangent bundle signals (i.e. vector ﬁelds over\n","manifolds), we deﬁne a frequency representation for them and, by cascading layers consisting of tan- gent bundle ﬁlters\n","banks and nonlinearities, we introduce Tangent Bundle Neural Networks (TNNs). We then discretize the TNNs in the space\n","domain by sampling points on the manifold and build- ing a cellular sheaf [26] representing a legit approximation of\n","both the manifold and its tangent bundle [24]. We formally prove that the discretized architecture over the cellular\n","sheaf converges to the underlying TNN as the number of sampled points increases. More- over, we further discretize the\n","architecture in the time domain by sampling the ﬁlter impulse function in discrete and ﬁnite time steps, showing that\n","space-time discretized TNNs are a principled variant of the very recently introduced Sheaf Neural Networks [23, 27, 28],\n","discrete architectures operating on cellular sheaves and generalizing graph neural networks. Finally, we numerically\n","evaluate the perfor- mance of TNNs on a denoising task of a tangent vector ﬁeld of the unit 2-sphere. Paper Outline. The\n","paper is organized as follows. We start with some preliminary concepts in Section 2. We deﬁne the tangent bun- dle\n","convolution and ﬁlters in Section 3, and Tangent Bundle Neural Networks (TNNs) in Section 4. In Section 5, we discretize\n","TNNs in space and time domains, showing that discretized TNNs are Sheaf Neural Networks and proving the convergence\n","result. Numerical re- sults are in Section 6 and conclusions are in Section 7.arXiv:2210.15058v3  [eess.SP]  18 Nov 2022\n","................................................................................\n","Response: Key features of Weijie Su's work include: - Language Guided Bottlenecks (LaBo): This approach leverages a language\n","model, GPT-3, to define a large space of possible bottlenecks for Concept Bottleneck Models (CBMs). It uses GPT-3 to\n","produce factual sentences about categories and searches for bottlenecks through a novel submodular utility. - Concept\n","Bottleneck Models (CBMs): CBMs are inherently interpretable models that factor model decisions into human-readable\n","concepts. They allow for easy understanding of why a model is failing, which is important for high-stakes applications.\n","- High-performance CBMs: Su's work shows how to construct high-performance CBMs without manual specification, achieving\n","similar accuracy to black box models. This addresses the under-performance issue of CBMs and promotes their broader\n","adoption. - Application to visual recognition: Su demonstrates that LaBo bottlenecks excel at few-shot classification in\n","visual recognition. They are more accurate than black box linear probes at 1 shot and comparable with more data. - Wide\n","applicability of interpretable models: LaBo shows that inherently interpretable models like CBMs can be widely applied\n","with similar or better performance than black box approaches.\n","................................................................................\n","['In this paper, the authors discuss the concept of auditing algorithms and understanding algorithmic systems from an\\nexternal perspective. They argue that as algorithms are increasingly being used in various domains, it is important to\\nhave a way to assess their fairness, transparency, and accountability. The authors propose a framework for auditing\\nalgorithms that includes four components: data access, model transparency, decision-making process, and system\\nresilience. They also discuss various challenges and considerations in auditing algorithms, such as data privacy, model\\ncomplexity, and potential biases. Overall, the paper provides insights into the key features and considerations in\\nauditing algorithms and understanding how they impact individuals and society.', \"Key features of Junhyong Kim's work: - Machine learning - Data Science - Language Guided Bottlenecks (LaBo) - Concept\\nBottleneck Models (CBM) - GPT-3 (language model) - LLMs (large language models) - Natural Language Processing - Computer\\nVision - Image Processing - Reinforcement Learning - Markov Decision Process - Artificial Intelligence - Algorithmic\\nRun-time\", '- Adaptive Query Processing - Transdiagnostic Dimensional Anxiety - Maximum Satisﬁability in Program Analysis:\\nApplications and Techniques - The Role of Visualization in Computer Science Education', '- Database Provenance - Diagnostic and Forensic Uses - Distributed Systems - Network Provenance - Distributed System\\nArchitecture - Intrusion Investigation - Software-Defined Networks - Open Issues in Provenance', '- Large Language Models (LLMs) - ChatGPT - Data Science - Education - Data cleaning - Model building - Interpretation -\\nReport writing - Responsibilities of data scientists - Automation in data science - Evolution of roles in data science -\\nSoftware engineer to product manager transition - Strategic planning - Coordinating resources - Overall product life\\ncycle - Pedagogy in data science education - Cultivating diverse skillsets - LLM-informed creativity - Critical thinking\\n- AI-guided programming - Interdisciplinary knowledge - Interactive teaching and learning tools - Personalized education\\n- Enriched learning experiences - Opportunities and challenges of integrating LLMs in education - Balancing human\\nexpertise and LLMs - Complementary human intelligence and creativity - Transformative period in data science education', \"Key features of Adam David Mally's work include:  1. Language Guided Bottlenecks (LaBo): Adam Mally developed a novel\\napproach called LaBo that leverages GPT-3, a language model, to define a large space of possible bottlenecks for Concept\\nBottleneck Models (CBMs) in machine learning.  2. Concept Bottleneck Models (CBMs): Adam Mally's work focuses on CBMs,\\nwhich are inherently interpretable models that factor model decisions into human-readable concepts. He aims to address\\nthe shortcomings of CBMs and improve their performance compared to black box models.  3. GPT-3: Mally's approach, LaBo,\\nutilizes GPT-3, a language model, to generate factual sentences about categories in order to form candidate concepts for\\nCBMs.  4. High-performance CBMs: Mally's research demonstrates how to construct high-performance CBMs without manual\\nspecification of concepts. His approach achieves similar accuracy to black box models.  5. Visual Recognition: Mally's\\nwork shows that LaBo bottlenecks excel at few-shot classification in visual recognition tasks. They are 11.7% more\\naccurate than black box linear probes at 1 shot and comparable with more data.  6. Machine Learning: Mally's work falls\\nwithin the broader field of machine learning, specifically in the area of interpretable models and concept-based\\napproaches.  7. Data Science: Mally's research has implications for data science, particularly in terms of understanding\\nand interpreting model decisions.  8. GPT-3's Sentential Concepts: Mally aligns GPT-3's sentential concepts to images\\nusing CLIP to form a bottleneck layer in CBMs.  9. Markovian Models: Mally explores a Markovian model for price\\nevolution in quantitative finance, focusing on trading strategies and pricing models.  10. Reinforcement Learning: Mally\\npresents new algorithms for reinforcement learning and demonstrates that they have polynomial bounds on resources\\nrequired for near-optimal return in general Markov decision processes.  11. Algorithmic Run-time: Mally's work discusses\\nalgorithms for reinforcement learning that have polynomial bounds on computation time, addressing the exploration-\\nexploitation trade-off.  12. Large Language Models (LLMs): Mally's research discusses the rapid advances of LLMs, such\\nas ChatGPT, and their impact on data science and statistics education.  13. Data Science Education: Mally argues for a\\nmeaningful evolution in data science education to account for the transforming role of LLMs in data analysis and\\ninterpretation. He emphasizes the need for diverse skillsets and interdisciplinary knowledge among data science\\nstudents.  14. Visualization in Computer Science Education: Mally's work acknowledges the importance of visualization in\\ncomputer science education and discusses its history and impact on teaching and learning.  15. Adaptive Query\\nProcessing: Mally explores adaptive query processing and its applications in database systems.  16. Inductive Biases and\\nVariable Creation in Self-Attention Mechanisms: Mally analyzes the inductive biases of self-attention modules in deep\\nlearning and identifies the ability of bounded-norm self-attention heads to learn sparse functions of input sequences.\\n17. Synthetic Experiments: Mally conducts synthetic experiments to probe the sample complexity and computational\\ncapabilities of self-attention models in learning sparse interactions and Boolean functions.\", '[Complex networked systems, Engineering infrastructures, Social networks, Technological infrastructure, Biological\\nsystems, Modeling, Analysis, Control, Structure, Dynamics, Critical societal functions, Lecture Notes in Computer\\nScience]', '[Program Analysis, Maximum Satisﬁability, Optimization, Boolean Satisﬁability, Tradeoffs, Clauses, Soundness, Iterative\\nExpansion]', 'Adaptively Secure Garbled Circuits, One-Way Functions, Garbling Scheme, Security, Circuit Evaluation, Cryptography,\\nEncryption, Computer Science.', \"Key features of Rahul Mangharam's work include: - Intersection of formal methods, machine learning, and controls for\\nmedical devices, energy efficient buildings, and autonomous systems - Research at the PRECISE Center and direction of\\nthe Safe Autonomous Systems Lab at the University of Pennsylvania - Recognition and awards, including the 2016 US\\nPresidential Early Career Award, 2014 IEEE Benjamin Franklin Key Award, and 2013 NSF CAREER Award - Development of\\nLanguage Guided Bottlenecks (LaBo) approach, which leverages a language model (GPT-3) for constructing high-performance\\nConcept Bottleneck Models (CBMs) for visual recognition - Application of machine learning to problems in autonomous\\nracing, energy systems, clinical trials, building energy optimization, and more\", '- Maximum Satisfiability (MaxSAT) - Program analysis - Balance of competing tradeoffs - Boolean Satisfiability (SAT)\\nproblem - Optimization - Optimization extension - Diagnosing query answers - Balancing tradeoffs - Program optimization\\n- Large MaxSAT instances - General framework - Solving large instances - New techniques - Reinforcement learning -\\nMarkov decision process - Artificial intelligence - Algorithmic run-time - Trading - Markovian model - High frequency\\ntrading - Pricing models - Quantitative finance - Classical finance - Random walk - Statistics - Mathematical finance -\\nProbabilistic models - Language Guided Bottlenecks - Concept Bottleneck Models - Machine learning - Data science - GPT-3\\n- LLMs (Language Learning Models) - Natural language processing - Computer vision - Image processing - Debugging -\\nStrongly-compartmentalized distributed systems - Data provenance - Architecture - Experiences - Road ahead - Internet\\nscale - Provenance - Debugging query answers - Tracing unexpected results - Reasons for computations - Network\\nprovenance - Intrusions investigation - Diagnosing software-defined networks - Open issues - Phonetics research - Forced\\nalignment - Speech recognition technology - Phonetic segmentation - Digital speech - Speech corpora - Acoustic-phonetic\\nfeatures - Distribution of speech and silence segments - Fundamental frequency - Phonetic segmentation and transcription\\n- Automatic phonetic segmentation - Forced alignment technique - Automatic speech recognition systems - Acoustic units\\nextraction', '- Replication and Partitioning for building secure distributed systems - Visualization in Computer Science Education -\\nAlgorithm visualization - Data structure visualization - Program visualization - eTextbooks and hypertextbooks - Control\\nfor societal-scale challenges - Maximum Satisfiability in Program Analysis - Compact Encodings and Enumerations -\\nMarkovian models - High-frequency trading - Pricing models - Quantitative finance - Reinforcement learning - Markov\\ndecision processes - Artificial intelligence - Algorithmic run-time', '- Suprasegmentals in speech - Automatic recognition of syllables, tones, and pitch accents - Wav2vec 2.0 model -\\nConnectionist Temporal Classification (CTC) loss - Mandarin tone recognition - Utilizing segmental information in\\nsuprasegmental recognition - Language models in recognition units - Combining Mandarin tone recognition with English\\nphoneme recognition - Deep learning and end-to-end models in speech recognition - Feature engineering in supervised and\\nunsupervised learning - Prosodic and spectral features in suprasegmental recognition - Downstream tasks in speech-to-\\ntext systems - Low-resource language recognition - Speaker recognition - Phone recognition', '- Replication and Partitioning for Secure Distributed Systems - Modeling, analysis, and control of complex networked\\nsystems and engineering infrastructures - Role of visualization in computer science education - Control for societal-\\nscale challenges - Data intimacy, machine learning, and consumer privacy - Language patterns in depression and anxiety', 'Complex networked systems Modeling and analysis of complex networks Control of complex networks Engineering\\ninfrastructure Social networks Technological infrastructure Biological systems', \"The key features of Nikolai Matni's work include language guided bottlenecks, concept bottleneck models, machine\\nlearning, data science, GPT-3, LLMs, natural language processing, computer vision, image processing, trading, Markovian\\nmodel, high frequency trading, pricing models, quantitative finance, classical finance, random walk, statistics,\\nmathematical finance, probabilistic models, reinforcement learning, Markov decision process, artificial intelligence,\\nalgorithmic run-time, ROS-NetSim, robotic and network simulators, multi-agent systems, networked robots, robot system\\ndesign, software architecture for robotics and automation, QWIRE, QASM based quantum transpiler framework, quantum\\nprogramming languages, robotic grasping and contact, modular self-reconfigurable robots, generative type abstraction and\\ntype-level computation, and learning and leveraging features in flow-like environments to improve situational awareness.\", \"Key features of Harvey Rubin's work: - Infectious Diseases: Harvey Rubin is a Professor of Medicine specializing in\\ninfectious diseases. - Research Funding: His research in infectious diseases has been funded by organizations such as\\nthe NIH, NSF, DARPA, the Global Alliance for TB Drug Discovery, and the Department of Defense. - Mathematical Modeling:\\nHe has extended his research in infectious diseases to mathematical modeling of complex biological systems. - Peer-\\nReviewed Publications: His research has resulted in more than 100 peer-reviewed papers, chapters, or reviews. -\\nScientific Review Panels: He has served on national and international scientific review panels, including the NIH, NSF,\\nNASA Intelligent Systems Program, DARPA, and The Medical Research Council, South Africa. - Interdisciplinary Expertise:\\nIn addition to his work in Medicine, he also holds secondary appointments as Professor in the Department of Biochemistry\\nand Biophysics, and as Professor of Computer and Information Sciences at the University of Pennsylvania School of\\nEngineering and Applied Sciences.\", \"Some key features of Mayur Naik's work include: - Language Guided Bottlenecks (LaBo) approach for constructing high-\\nperformance Concept Bottleneck Models (CBMs) without manual specification - Leveraging GPT-3 language model for defining\\na large space of possible bottlenecks and producing factual sentences about categories - Using CLIP to align GPT-3's\\nsentential concepts to images - Achieving high accuracy in few-shot classification with LaBo bottlenecks - Addressing\\nthe challenge of balancing different competing trade-offs in program analysis using Maximum Satisfiability (MaxSAT)\\nproblem - Developing a Differentiable Symbolic Reasoning (DSR-LM) framework to improve logical reasoning abilities of\\npre-trained language models - Integrating symbolic reasoning with pre-trained language models to support extensive\\nsymbolic programming and draw logical conclusions - Automatically inducing and optimizing rules in DSR-LM for deductive\\nreasoning - Applying semantic loss and logical integrity constraints as prior knowledge in DSR-LM\", '[Adaptive Query Processing, Database Management, Query Optimization, Data Processing, Query Execution, Query Planning,\\nMachine Learning, Artificial Intelligence]  Asynchronous Stochastic Optimization Full text available at:\\nhttp://dx.doi.org/10.1561/1900000002: [Asynchronous Stochastic Optimization, Machine Learning, Optimization Algorithms,\\nStochastic Gradient Descent, Parallel Computing, Distributed Systems, Convergence Analysis, Asynchronous Algorithms]\\nDeep Learning: Methods and Applications Full text available at: http://dx.doi.org/10.1561/1900000003: [Deep Learning,\\nArtificial Neural Networks, Machine Learning, Deep Neural Networks, Deep Reinforcement Learning, Convolutional Neural\\nNetworks, Recurrent Neural Networks, Generative Adversarial Networks, Natural Language Processing]  Structured\\nPrediction and Learning in Computer Vision Full text available at: http://dx.doi.org/10.1561/1900000008: [Structured\\nPrediction, Machine Learning, Computer Vision, Image Classification, Object Detection, Semantic Segmentation, Visual\\nRelationship Detection, Deep Learning, Convolutional Neural Networks]  These are just a few examples of the key\\nfeatures, concepts, and related topics from each paper that can be used for machine learning. There may be more topics\\ncovered in each paper, and additional papers by Joshua B. Plotkin may cover different areas of research and provide\\ndifferent insights.', \"Key features of Dan Roth's work: - Concept Bottleneck Models (CBM) for interpretable and explainable machine learning -\\nLanguage Guided Bottlenecks (LaBo) approach using GPT-3 for generating concepts - Alignment of GPT-3's sentential\\nconcepts to images using CLIP for visual recognition - High-performance CBMs without manual specification -\\nPrioritization of discriminative and diverse information for bottleneck layer selection - Improved few-shot\\nclassification accuracy with LaBo bottlenecks - Application of interpretable models in high-stakes domains -\\nContributions to natural language processing and machine learning\", '- Reinforcement Learning - Markov Decision Process - Adversarial Robust Control - Linear-Quadratic Control - Worst-case\\nperturbations - H∞methods - Stochastic disturbances - Dynamical systems - Control systems - Complex networked systems -\\nComputer systems - Algorithm visualization - Data structure visualization - Program visualization - eTextbooks -\\nHypertextbooks', '[Visualization, Computer Science Education, Algorithm Visualization, Data Structure Visualization, Program\\nVisualization, eTextbooks, Hypertextbooks, Depression, Anxiety, Language Patterns, Clinical Interview, Psychology,\\nComputer and Information Science, Artificial Intelligence, Stanford University, National Institute on Drug Abuse,\\nResearch Program]', '- Robust and reliable machine learning - Understanding, debugging, and guaranteeing the behavior of machine learning\\nmodels - Foundations of machine learning - Optimization - Scalability and practicality in real-world settings -\\nCosmology and surgery - Modeling, analysis, and control of complex networked systems and engineering infrastructures -\\nSocial networks, technological infrastructure, and biological systems - Data sources and practices at large consumer-\\nfacing technology companies - Data intimacy and its consequences for consumer privacy - Policy and regulation\\nimplications - Role of visualization in computer science education - Dynamic processes in computer science - Algorithm\\nvisualization - Data structure visualization - Program visualization - Online hypertextbooks', '- Garbled circuits - Adaptive security - One-way functions - Circuit evaluation - Input/output size - Selective security\\n- On-line complexity - Depth of circuit - Width of circuit - Space complexity', '- Hybrid control barrier functions - Safe control laws - System dynamics - Learning from data - Optimization-based\\nframework - Feasibility of optimization problem - Safe behavior - Hybrid systems - Imitation learning - Nonsmooth\\nbarrier functions - Reciprocal CBFs - Zeroing CBFs - Convex quadratic programs - Sum-of-squares programming', \"Key features of Victor M. Preciado's work include:  1. Modeling and analysis of complex networked systems and\\nengineering infrastructures. 2. Applications in social networks, technological infrastructure, and biological systems.\\n3. Research interests in the relationship between the structure and dynamics of complex networks. 4. Affiliation with\\nthe Networked & Social Systems Engineering program at the University of Pennsylvania. 5. Focus on using innovative\\nmathematical and computational approaches to model and control complex, high-dimensional dynamical systems. 6.\\nApplications of research in epidemic modeling and control, information spreading over socio-technical networks,\\nresilience of networked infrastructure, and brain dynamics. 7. Research on transferable hyper-graph neural networks,\\nsynthesis-based robust model predictive control, learning Lyapunov functions for piecewise affine systems, and more. 8.\\nPublication in various journals and conferences, including Automatica, IEEE Control Systems Letters, IEEE Transactions\\non Automatic Control, and International Symposium in Nonlinear Theory and its Applications (NOLTA).\", \"Based on the provided information, the key features of Benjamin C. Pierce's work include:  1. Specification,\\nVerification, and Testing 2. Security and Privacy 3. Bidirectional Programming (Lenses) 4. File and Data Synchronization\\n5. Static Types for XML Processing 6. Concurrent and Distributed Systems 7. Types and Surveys 8. Contracts 9. Modular\\nType Systems 10. Type Inference 11. Object-Oriented Programming 12. Subtyping 13. Intersection and Union Types 14.\\nExtensible Records 15. Dynamic Types 16. Semantics 17. Climate Change  Note: The list is not exhaustive and may not\\ninclude all the features covered in Benjamin C. Pierce's work.\", '[Self-Attention Mechanisms, Inductive Biases, Variable Creation, Transformer Networks, Natural Language Processing, Deep\\nLearning, Generative Sequence Modeling, Unsupervised Representation Learning, Recurrent Architectures, Convolutional\\nArchitectures, Statistical Foundations, Statistical Analysis, Attention Mechanisms, Generalization Bounds, Sparse\\nInteractions, Boolean Functions, Sample Complexity, Data Intimacy, Consumer Privacy, Machine Learning, Artificial\\nIntelligence, Policy and Regulation]', \"Key features of Pratyush Mishra's work: - Language Guided Bottlenecks (LaBo) - Concept Bottleneck Models (CBM) - GPT-3\\n(Generative Pre-trained Transformer 3) - Inherently interpretable models - High-performance CBMs - Black box models -\\nSubmodular utility - CLIP (Contrastive Language-Image Pre-training) - Visual recognition - Few-shot classification -\\nLinear probes - Image Processing - Markovian Model - High Frequency Trading - Pricing Models - Quantitative Finance -\\nClassical Finance - Random Walk - Statistics - Mathematical Finance - Probabilistic Models - Reinforcement Learning -\\nMarkov Decision Process - Artificial Intelligence - Algorithmic Run-time - Bayesian Persuasion - Reduced Form Approach -\\nVisualization in Computer Science Education - Algorithm visualization - Data structure visualization - Program\\nvisualization - eTextbooks - Hypertextbooks - Adaptive Query Processing - Control Systems Initiative - Self-Attention\\nMechanisms - Inductive Biases - Variable Creation - The American Statistician - Balance testing in experimental design -\\nObservational versus experimental data - Secure Distributed Systems - Replication and Partitioning\", '[Maximum Satisfiability, Program Analysis, Boolean Satisfiability, Optimization, Trade-offs, Soundness, Optimality,\\nLarge-scale Instances, Algorithms, Balancing, Solving, Clauses, Framework, Techniques, Algorithmic Systems, Auditing\\nAlgorithms, Adaptive Query Processing, Visualization, Computer Science Education, Dynamic Processes, Algorithm Working,\\nFlow of Information, Computing Entities, Static Media, History of Visualization, Educational Assessment, Computing\\nTechnology, Hypertextbooks, eTextbooks]', '- Distributed data management systems - Internet-scale query processing - Data-centric techniques - Formal methods -\\nNetworked systems - Program analysis - Maximum satisfiability - Machine learning for market microstructure and high-\\nfrequency trading - Trade execution optimization - Reinforcement learning for trade execution', '[Threshold Cryptography, Multiserver Model, YOSO Model, Control Systems, Societal-Scale Challenges, Roadmap 2030,\\nCybersecurity, Cryptography, Distributed Systems, Computer Science, Privacy, Security]', '[Adaptive Query Processing, Machine Learning, Neural Networks, Social Networks, Social Comparison, Non Commutative\\nConvolutional Signal Models, Stability, Provenance, Data Science Education, Large Language Models, ChatGPT, Data\\nCleaning, Model Building, Interpretation, Report Writing, Pedagogy, Personalized Education, Open Learning Analytics,\\nIntegrated & Modularized Platform, Labeling Scheme, Workflow Provenance, Social Influence, Constructed Online Networks,\\nSmall Deformations, Algebraic Signal Models, Convolutional Neural Networks, Social Perception, Experimental Studies,\\nHealth Behavior, Social Comparisons, Online Social Networks, Optimization, Graph Traversals, Databases, Information\\nDistribution, Markovian Model, Pricing Models, Random Walk, Drift and Diffusion Models, Reinforcement Learning, Markov\\nDecision Processes, Artificial Intelligence, Algorithmic Run-time, Maximum Satisfiability, Program Analysis, Provenance\\nQueries, Data Provenance, Internet Scale, Architecture, Experiences, Road Ahead, Security, Forensic Uses, Network\\nDebugging, Data Management, Distributed Systems, Network State, Network Provenance, Network Operator, Distributed\\nSystem, Data Analysis, Data Objects, Distributed Systems, Qeury Answers, Unexpected Behavior, Specification,\\nDeformations, Stability, Non Commutative Filters, Declarative Rules, Node, Model, Optimization, Non Commutative\\nArchitectures, Convolutional Architectures, Optimization Techniques, Algorithmic Complexity]', '- Algebraic signal models (ASMs) - Convolutional neural networks (CNNs) - Stability to small deformations - Photometric\\nstereo - Inverse rendering - Neural surface representation - Shading-based refinement - Non-commutative convolutional\\nfilters - Algebraic neural networks (AlgNNs) - Non-commutative algebras - Non-commutative operators - Maximum\\nsatisfiability (MaxSAT) - Program analysis - Maximum satisﬁability (MaxSAT) problem - Shape-from-GAN - GAN2X method -\\nSpecularity-aware neural surface representation - Self-supervised learning - Multigraph neural networks - Lie group\\nneural networks - Quaternion neural networks - Quiver neural networks - Convolutional signal models - Stability to\\ndeformations - Non-Lambertian inverse rendering - Image GANs - Linear program - Image reconstruction - Adversarial\\nexamples - Robustness metrics - Markovian model - Stock price evolution - Reinforcement learning - Markov decision\\nprocesses (MDPs) - Artificial intelligence - Exploration-exploitation trade-off - Network science - Collective\\nintelligence - Wisdom of the crowd - Joint modeling - Resource allocation - Social networks - Social comparison -\\ngroupthink - Social dynamics', '- Concept Bottleneck Models - Language Guided Bottlenecks (LaBo) - GPT-3 (Language Model) - CLIP (Language and Image\\nModel) - High Frequency Trading - Pricing Models - Quantitative Finance - Classical Finance - Random Walk - Statistics -\\nMathematical Finance - Probabilistic Models - Reinforcement Learning - Markov Decision Process - Artificial Intelligence\\n- Algorithmic Run-time - Visualization - Computer Science Education - Dynamic Processes - Computer Vision - Image\\nProcessing - Trading - Markovian Model - Data Science - Natural Language Processing - Social Networks - Social\\nComparison - Network Provenance - Adversarial Robust Control - Estimation - Large Language Models (LLMs) - Data Science\\nEducation', '- Maximum Satisﬁability in Program Analysis: Applications and Techniques: Trading, Markovian Model, High Frequency\\nTrading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance,\\nProbabilistic Models, Algorithmic Run-time - Language Guided Bottlenecks, Concept Bottleneck Models: Machine learning,\\nData Science, GPT-3, LLMs, Natural Language Processing, Computer Vision, Image Processing - The Role of Visualization in\\nComputer Science Education: Algorithm visualization, data structure visualization, program visualization, eTextbooks,\\nhypertextbooks - Data Provenance at Internet Scale: Architecture, Experiences, and the Road Ahead: Provenance, Database\\nManagement, Diagnostic and forensic uses, computer networks, distributed systems - An Optimal Labeling Scheme for\\nWorkflow Provenance Using Skeleton Labels: Database Applications, Provenance, Workflows, XML database systems, labeling\\nschemes, directed graphs - Adaptive Query Processing: Query Processing, Algorithms, Data Management Systems - Auditing\\nAlgorithms: Algorithmic Systems, Auditing, Understanding Algorithmic Systems', \"Features, concepts, and related topics from each paper that can be used for machine learning:  1. Gushu Li's work: -\\nLanguage Guided Bottlenecks - Concept Bottleneck Models - Machine learning - Data Science - GPT-3 - LLMs (Large Language\\nModels) - Natural Language Processing - Computer Vision - Image Processing  2. Xinming Tu, James Zou, Weijie J. Su,\\nLinjun Zhang's work: - Large Language Models - ChatGPT - Data Science - Education - AI-guided programming -\\nInterdisciplinary knowledge - Pedagogy - Personalized education - Enriched learning experiences - Complementary human\\nexpertise  3. Stephen Chong's work: - Replication and Partitioning - Secure Distributed Systems - Security - Privacy -\\nComputer Science - Algorithms - Computational complexity  4. Victor M. Preciado's work: - Complex networked systems -\\nEngineering infrastructures - Social networks - Technological infrastructure - Biological systems - Modeling - Analysis\\n- Control - Structure - Dynamics  5. Michael Kearns and Yuriy Nevmyvaka's work: - Machine Learning for Market\\nMicrostructure - High Frequency Trading - Predictive models - Statistical models - Algorithms - Computational complexity\\n- Trade execution - Alpha generation - Feature selection - Feature engineering  6. Eric Fouh, Monika Akbar, and Clifford\\nA. Shaffer's work: - Visualization in Computer Science Education - Dynamic processes - Algorithm visualization - Data\\nstructure visualization - Program visualization - eTextbooks - Hypertextbooks - Technology changes - Online education\", 'Complex networked systems, Engineering infrastructures, Social networks, Technological infrastructure, Biological\\nsystems, Modeling, Analysis, Control', \"Key features of Insup Lee's work:  - Concept Bottleneck Models (CBM): These are inherently interpretable models that\\nfactor model decisions into human-readable concepts. They allow for easy understanding of why a model is failing, which\\nis important for high-stakes applications.  - Language Guided Bottlenecks (LaBo): This approach leverages a language\\nmodel, GPT-3, to define a large space of possible bottlenecks. LaBo uses GPT-3 to produce factual sentences about\\ncategories to form candidate concepts. It efficiently searches possible bottlenecks through a novel submodular utility\\nto select discriminative and diverse information.  - High-performance CBMs without manual specification: Lee's work\\nshows how to construct high-performance CBMs without the need for manual specification. These CBMs achieve similar\\naccuracy to black box models in terms of performance.  - Application to visual recognition: Lee's work demonstrates that\\nLaBo bottlenecks excel at few-shot classification in visual recognition tasks. They are more accurate than black box\\nlinear probes at 1 shot and comparable with more data.  - Machine learning: Lee's work is related to machine learning,\\nas CBMs and LaBo are techniques used in the field of machine learning.  - Natural Language Processing (NLP): Lee's work\\nutilizes GPT-3, a language model, for generating factual sentences and leveraging NLP techniques in constructing\\nbottlenecks.  - Computer Vision and Image Processing: The application of LaBo bottlenecks is primarily focused on visual\\nrecognition tasks, indicating a connection to computer vision and image processing.  Overall, Insup Lee's work explores\\nthe development of interpretable models, automatic concept generation with the help of language models, and their\\napplication in visual recognition tasks. This research contributes to the advancement of machine learning techniques and\\ntheir potential for wider adoption in various domains.\", \"Key features of Benjamin Lee's work include:  - Language Guided Bottlenecks (LaBo): A method that leverages a language\\nmodel, GPT-3, to define a large space of possible bottlenecks for Concept Bottleneck Models (CBM) in machine learning.\\nLaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. - Concept Bottleneck Models\\n(CBM): Inherently interpretable models that factor model decisions into human-readable concepts. CBMs allow for easy\\nunderstanding of why a model is failing, which is critical for high-stakes applications. LaBo aims to improve the\\nperformance of CBMs without manual specification. - GPT-3 (Generative Pre-trained Transformer 3): A powerful language\\nmodel used in LaBo to generate factual sentences about categories and form candidate concepts for CBMs. - CLIP\\n(Contrastive Language-Image Pre-training): A model used to align GPT-3's sentential concepts to images and form a\\nbottleneck layer in LaBo. - Visual recognition and classification: LaBo bottlenecks demonstrate high effectiveness in\\nfew-shot classification tasks, outperforming black box linear probes in accuracy. - Markovian models: Analyzing the\\nprice evolution of stocks using Markovian models that generalize classical finance models, including random walk, drift,\\nand diffusion models. - Trading strategies: Developing universally profitable trading strategies for stocks based on the\\nanalysis of Markovian models. - Reinforcement learning: Developing algorithms for reinforcement learning in Markov\\ndecision processes that have polynomial bounds on required resources and achieve near-optimal return. These algorithms\\nexplicitly handle the exploration-exploitation trade-off. - Control systems: Contributing to a roadmap for control\\nsystems addressing societal-scale challenges in 2030.\", '- Machine learning - Concept Bottleneck Models - Language Guided Bottlenecks (LaBo) - GPT-3 - LLMs (Language Model) -\\nNatural Language Processing - Computer Vision - Image Processing - Trading - Markovian Model - High Frequency Trading -\\nPricing Models - Quantitative Finance - Classical Finance - Random Walk - Statistics - Mathematical Finance -\\nProbabilistic Models - Reinforcement Learning - Markov Decision Process - Artificial Intelligence - Algorithmic Run-time\\n- Visualization - Computer Science Education - Algorithm visualization - Data structure visualization - Program\\nvisualization - eTextbooks - Hypertextbooks - Market Microstructure - Trade Execution - Feature selection - Feature\\nengineering - Reinforcement Learning', '- Concept Bottleneck Models (CBM) - Language Guided Bottlenecks (LaBo) - GPT-3 (Generative Pre-training Transformer 3) -\\nLLMs (Language Model and Language Models) - Natural Language Processing (NLP) - Computer Vision - Image Processing -\\nMachine Learning - Data Science - Trading - Markovian Model - High Frequency Trading - Pricing Models - Quantitative\\nFinance - Classical Finance - Random Walk - Statistics - Mathematical Finance - Probabilistic Models - Reinforcement\\nLearning - Markov Decision Process - Artificial Intelligence - Algorithmic Run-time - Visualization - Computer Science\\nEducation - Algorithm visualization - Data structure visualization - Program visualization - eTextbooks - Hypertextbooks\\n- Adaptive Query Processing.', '- Suprasegmentals in speech - Automatic recognition of suprasegmentals - Wav2vec 2.0 and CTC - Recognition of syllables,\\ntones, and pitch accents - Improving Mandarin tone recognition using segmental information - Language models for tonal\\nsyllables - Combining English phoneme recognition and Mandarin tone recognition', \"Key features of Joe Devietti's work:  - Language Guided Bottlenecks (LaBo): This approach leverages a language model,\\nGPT-3, to define a large space of possible bottlenecks in concept bottleneck models (CBMs) used for machine learning.\\nLaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts, which can then be aligned to\\nimages using CLIP to form a bottleneck layer.  - Concept Bottleneck Models (CBM): CBMs are inherently interpretable\\nmodels that factor model decisions into human-readable concepts. They allow easy understanding of why a model is\\nfailing, which is critical for high-stakes applications. CBMs typically require manual specification of concepts, but\\nJoe Devietti's work shows how to construct high-performance CBMs without manual specification.  - GPT-3: GPT-3 is a\\nlanguage model used in Joe Devietti's work to generate factual sentences for the formation of candidate concepts in\\nLaBo.  - CLIP: CLIP is another model used in Joe Devietti's work to align GPT-3's sentential concepts to images, forming\\na bottleneck layer.  - Visualization: Joe Devietti's work focuses on developing inherently interpretable models that can\\nbe widely applied in machine learning, with similar or better performance than black box approaches. This involves\\nvisualizing concepts and their alignment to images for better understanding and interpretability.  - Computer Vision:\\nJoe Devietti's work involves the application of CBMs, LaBo, and CLIP in the field of computer vision, specifically for\\nvisual recognition tasks.  - Few-shot Classification: Joe Devietti's work demonstrates that LaBo bottlenecks excel at\\nfew-shot classification, achieving higher accuracy compared to black box linear probes with only 1 shot. This highlights\\nthe effectiveness of Joe Devietti's approach in handling limited training data scenarios.\", '[Threshold Cryptography, Multiserver Model, YOSO Model, Cryptography, Security, Computer Science, Cryptographic\\nProtocols, Distributed Systems, Privacy]  [Auditing Algorithms, Algorithmic Systems, Data Science, Machine Learning,\\nArtificial Intelligence, Fairness, Accountability, Transparency, Ethical AI]  [Visualization, Computer Science\\nEducation, Algorithm Visualization, Data Structure Visualization, Program Visualization, eTextbooks, Hypertextbooks]\\n[Adaptive Query Processing, Maximum Satisfiability, Program Analysis, Optimization, Algorithmic Techniques, Statistical\\nAnalysis]  [Reinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time]  [Instance-\\nOptimized Data Analytics, Database Systems, Performance Optimization, Learned Indexes, Materialized Views, SageDB, Data\\nStorage, Query Workloads]', '[Damon Centola, Complex Contagions, Markovian Model, High Frequency Trading, Pricing Models, Quantitative Finance,\\nClassical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models, Reinforcement Learning, Markov\\nDecision Process, Artificial Intelligence, Algorithmic Run-time, Influencers, Backfire Effects, Personal Networks,\\nSocial Networks, Behavioral Change, Zeros of a Random Analytic Function, Perfect Spacing, Differentiation, Analytic\\nFunctions, Poisson Process, Convex Hull, Gauss-Lucas Theorem, Pólya-Schur Operators, Riemann Hypothesis]', '[Quasi-Static and Dynamic Mismatch, Door Opening, Stair Climbing, Legged Robot, Robotics, Robot Fitness, Quadrupedal\\nPlatform, Geometric Constraints, Torque Density, Energy Density, Power Density, Human-Scale Environments, Task-\\nEnvironment Interaction, Kinematic Workspace, High Degree of Freedom Manipulators, Inverse Kinematic Problem, Dynamic\\nProblem]', '- Maximum Satisﬁability (MaxSAT) - Program analysis - Trade-offs in program analysis - Boolean Satisﬁability (SAT)\\nproblem - Optimization - Soundness and optimality guarantees - Visualization in computer science education - Dynamic\\nprocesses in computer science - Algorithm visualization - Data structure visualization - Program visualization -\\neTextbooks and hypertextbooks - Modeling, analysis, and control of complex networked systems - Engineering\\ninfrastructures - Social networks - Technological infrastructure - Biological systems - Data intimacy - Machine learning\\n- Consumer privacy - Data sources and practices at large consumer-facing technology companies - Google - Facebook -\\nPredictions and inferences from machine learning - Consumer privacy implications - Policy and regulation in the context\\nof data intimacy and machine learning.', \"Key features of Osbert Bastani's work: - Concept Bottleneck Models - Language Guided Bottlenecks (LaBo) - GPT-3\\n(language model) - CLIP (image alignment) - Inherently interpretable models - High-performance CBMs - Black box models -\\nVisual recognition - Few-shot classification - Linear probes - Machine learning - Data Science - Natural Language\\nProcessing - Computer Vision - Image Processing\", \"The key features of Yoseph Barash's work include the following:  1. Language Guided Bottlenecks (LaBo): This is an\\napproach that leverages a language model, GPT-3, to define a large space of possible bottlenecks in concept bottleneck\\nmodels (CBM). LaBo efficiently searches for bottlenecks through a submodular utility that promotes the selection of\\ndiscriminative and diverse information. The goal of LaBo is to construct high-performance CBMs without manual\\nspecification.  2. Concept Bottleneck Models (CBM): CBMs are inherently interpretable models that factor model decisions\\ninto human-readable concepts. Barash's work aims to address the shortcomings of CBMs and show how to construct high-\\nperformance CBMs without manual specification, achieving similar or better performance than black box models.  3. High-\\nperformance CBMs: Barash's work demonstrates that LaBo can be applied to a wide range of diverse datasets and achieve\\nhighly effective concepts important for visual recognition. The bottlenecks generated by LaBo are shown to excel at few-\\nshot classification and perform at similar or better accuracy than black box linear probes.  4. Natural Language\\nProcessing (NLP) and Computer Vision (CV) integration: Barash's work combines language models (GPT-3) with language-\\nguided bottlenecks and aligns them to images using CLIP, forming a bottleneck layer. This integration of NLP and CV\\nallows for interpretability and performance in visual recognition tasks.  5. Machine Learning and Data Science: Barash's\\nwork falls within the realm of machine learning and data science, focusing on the development of interpretable models\\nand addressing the challenges of model performance and understanding in high-stakes applications.\", '- Language Guided Bottlenecks - Concept Bottleneck Models - GPT-3 - LLMs (Language Models) - Natural Language Processing\\n- Computer Vision - Image Processing - Machine Learning - Data Science - High Frequency Trading - Pricing Models -\\nQuantitative Finance - Classical Finance - Random Walk - Statistics - Mathematical Finance - Probabilistic Models -\\nReinforcement Learning - Markov Decision Process - Artificial Intelligence - Algorithmic Run-time - Visualization in\\nComputer Science Education - Debugging - Satisfiability - Phonetics Research - Forced Alignment - Corpus Phonetics -\\nInformation and influence distribution in social networks - Online social networks', '- Language Guided Bottlenecks - Concept Bottleneck Models - Machine learning - Data Science - GPT-3 - LLMs (Language\\nModel Models) - Natural Language Processing - Computer Vision - Image Processing - Trading - Markovian Model - High\\nFrequency Trading - Pricing Models - Quantitative Finance - Classical Finance - Random Walk - Statistics - Mathematical\\nFinance - Probabilistic Models - Reinforcement Learning - Markov Decision Process - Artificial Intelligence -\\nAlgorithmic Run-time - Adaptive Query Processing - Modeling - Analysis - Control of complex networked systems -\\nEngineering infrastructures - Social networks - Technological infrastructure - Biological systems - Visualization in\\nComputer Science Education - Algorithm visualization - Data structure visualization - Program visualization - eTextbooks\\n- Hypertextbooks', \"Some key features of Daniel Hashimoto's work include: 1. Language Guided Bottlenecks (LaBo) approach for constructing\\nConcept Bottleneck Models (CBMs) without manual specification 2. Use of GPT-3, a language model, to define a large space\\nof possible bottlenecks 3. Efficient search algorithm for selecting discriminative and diverse information for\\nbottlenecks 4. Alignment of GPT-3's sentential concepts to images using CLIP for forming a bottleneck layer 5. High-\\nperformance CBMs that are comparable in accuracy to black box models 6. Application of CBMs to visual recognition tasks\\nand achieving similar or better performance compared to black box approaches.\", '[Maximum Satisﬁability, Program Analysis, Computer Science Education, Visualization, Algorithm Visualization, Data\\nStructure Visualization, eTextbooks, Hypertextbooks, Permissioned Blockchains, Supply Chain Management, Byzantine Fault\\nTolerance, Proof-of-Authority, Distributed Systems, Replication, Partitioning, Secure Distributed Systems, Threshold\\nCryptography, Adaptive Query Processing, Auditing Algorithms]', \"Key features of Andrew Head's work:  1. Language Guided Bottlenecks (LaBo): A novel approach that leverages a language\\nmodel, GPT-3, to define a large space of possible bottlenecks in Concept Bottleneck Models (CBM). This allows for the\\nconstruction of high-performance CBMs without manual specification.  2. Concept Bottleneck Models (CBM): Inherently\\ninterpretable models that factor model decisions into human-readable concepts. CBMs are important for high-stakes\\napplications as they allow people to easily understand why a model is failing.  3. GPT-3: A language model used in the\\nLaBo approach to generate factual sentences about categories and form candidate concepts for bottlenecks.  4. CLIP: A\\nmethod used to align GPT-3's sentential concepts to images and form a bottleneck layer important for visual recognition.\\n5. High-performance CBMs: The LaBo approach demonstrates that inherently interpretable models can be widely applied at\\nsimilar or better performance than black box approaches.  6. Few-shot classification: LaBo bottlenecks excel at few-shot\\nclassification, being 11.7% more accurate than black box linear probes at 1 shot and comparable with more data.  7.\\nInterpretable models: Andrew Head's work focuses on developing and promoting inherently interpretable models in machine\\nlearning.  Key features of other papers mentioned:  1. Markovian Model: A model for the price evolution of a stock that\\ngeneralizes many well-studied price evolution models in classical finance. It considers the probability of local upward\\nor downward movement dependent on the current price itself.  2. Reinforcement Learning: The paper presents new\\nalgorithms for reinforcement learning in Markov decision processes with polynomial bounds on resources required to\\nachieve near-optimal return.  3. Algorithmic Run-time: The algorithms presented in the paper explicitly handle the\\nExploration-Exploitation trade-off in reinforcement learning.\", 'Forced Alignment, Phonetic Research, Speech Recognition, Audio Transcription, Phonetics, Acoustic-Phonetic Features,\\nPronouncing Dictionary, Grapheme to Phoneme Rules, Hidden Markov Model (HMM), Gaussian Mixture Model', \"- Key Features of Daniel J Hopkins's work:  1. Concept Bottleneck Models (CBM)  2. Language Guided Bottlenecks (LaBo)\\n3. GPT-3 (Generative Pre-trained Transformer)  4. Black box models  5. Linear probes  6. Visual recognition  7. High-\\nstakes applications  8. Machine learning  9. Data science  10. Natural Language Processing (NLP)  11. Computer vision\\n12. Image processing  13. Markovian models  14. High-frequency trading  15. Pricing models  16. Quantitative finance\\n17. Classical finance  18. Random walk  19. Statistics  20. Mathematical finance  21. Probabilistic models  22.\\nReinforcement learning  23. Markov decision process  24. Artificial intelligence  25. Algorithmic run-time  26.\\nReplication and partitioning  27. Secure distributed systems  28. Networking and information technology  29.\\nSuprasegmentals in speech  30. Wav2vec 2.0  31. Connectionist Temporal Classification (CTC)  32. Syllables, tones, and\\npitch accents  33. Automatic recognition of suprasegmentals  34. Multitask learning\", \"M. Ani Hsieh's key features: Concept Bottleneck Models, Language Guided Bottlenecks, GPT-3, CLIP, Visual Recognition,\\nInterpretable Models, Black Box Models, Few-shot Classification\", '[CSS Grid Layout, Web Development, Layout Design, Compact Encodings, Data Structures, Website Building, HTML, CSS, User\\nInterface Design, Web Design],  # We propose an approach based on the Maximum Satisﬁability (MaxSAT) problem, an\\noptimization extension of the Boolean Satisﬁability (SAT) problem. We demonstrate the approach on three diverse\\napplications that advance the state-of-the-art in balancing tradeoﬀs in program analysis. Enabling these applications on\\nreal-world programs necessitates solving large MaxSAT instances comprising over 1030 clauses in a sound and optimal\\nmanner. We propose a general framework that scales to such instances by iteratively expanding a subset of clauses while\\nproviding soundness and optimality guarantees. We also present new techniques to instantiate and optimize the\\nframework.: [Optimization, Boolean Satisﬁability, Program Analysis, MaxSAT, Trade-offs, Algorithms, Problem Solving],  #\\nWe discuss and analyze the data sources and practices at large consumer-facing technology companies such as Google and\\nFacebook, and examine the central role of machine learning and artificial intelligence at such companies. We focus in\\nparticular on the notion of data intimacy - the fact that machine learning enables companies to routinely draw accurate\\npredictions and inferences about users that go far deeper than what is merely on the \"surface\" of the data collected. We\\ndiscuss the consequences for consumer privacy, and briefly discuss broad implications for policy and regulation.: [Data\\nPrivacy, Consumer Privacy, Data Sources, Predictive Analytics, Machine Learning, Artificial Intelligence, Policy and\\nRegulation],  # This project involves an R package called Palmer Penguins, which provides a dataset and functions for\\nexploring the data and modeling the size dimensions of three species of penguins. It aims to demonstrate the\\ncapabilities of the CSS Grid Layout, a CSS module created specifically to solve layout problems in website design. The\\nproject discusses the challenges of traditional website layouts using floats and tables, and highlights the benefits of\\nusing CSS Grid Layout for responsive and flexible design.: [Dataset, R Package, Data Exploration, Data Modeling, CSS\\nGrid Layout, Web Design, Responsive Design, Website Layout]', '- SNP enables networked systems to explain why they are in a certain state - SNP provides network forensics capabilities\\n- SNP is designed for adversarial settings and is robust to manipulation - SNP can efficiently explain state in an\\nadversarial setting - SNP does not require trusted components', '- Concept Bottleneck Models (CBM) - GPT-3 (language model) - Language Guided Bottlenecks (LaBo) - Computer Vision -\\nImage Processing - Reinforcement Learning - Markov Decision Process - Artificial Intelligence - Algorithmic Run-time -\\nTrading - Pricing Models - Quantitative Finance - Classical Finance - Random Walk - Statistics - Mathematical Finance -\\nProbabilistic Models - uncertainty estimation - visualization in computer science education - social networks - health\\nbehavior changes - data provenance at Internet scale', 'Time-aware provenance (TAP) is a model that captures the time, distribution, and causality of updates in distributed\\nsystems. It addresses the challenges of transient and inconsistent state, providing explanations for state changes, and\\nensuring security without trusted nodes. TAP enables system administrators to perform analysis tasks, such as\\ndiagnostics, forensics, and profiling, by efficiently and securely querying provenance data. The research agenda for TAP\\nincludes developing new provenance models and maintenance strategies, optimizing query processing, and designing query\\nlanguages that support time and change specifications.', '[Data Mining, Education, Machine Learning, Algorithms, Education Technology, Educational Data Science, Predictive\\nModeling, Student Performance, Educational Research]', \"Key features of Yasmin Kafai's work: - Language Guided Bottlenecks - Concept Bottleneck Models - Machine learning - Data\\nScience - GPT-3 - LLMs - Natural Language Processing - Computer Vision - Image Processing\", \"- Reachability analysis: One-shot reachability analysis is a framework used for analyzing the reachability of neural\\nnetwork dynamical systems. The goal is to verify the safety and stability of these systems. - Safety verification:\\nSafety verification is the process of ensuring that a system or component operates within defined safety parameters and\\ndoes not pose a risk to its environment or the individuals using it. - Neural network dynamical systems (NNDS): NNDS are\\nsystems that incorporate neural networks as controllers, planners, or perception modules. These systems are used in\\nvarious applications, such as robotics, and require safety verification to ensure their correct operation. - Layerwise\\nabstraction: Layerwise abstraction is a method used in neural network verification to simplify the analysis by\\nconsidering each layer of the neural network separately. This allows for more efficient computation of reachable sets\\nand bounds on the system's behavior. - Computational graphs: Computational graphs are graphical representations of the\\ncomputations performed by a neural network. They depict the flow of data through the network's layers and allow for the\\nanalysis and verification of the system's behavior. - Adversarial attacks: Adversarial attacks refer to malicious\\nattempts to manipulate or exploit a neural network system by providing input data that intentionally triggers incorrect\\nor unintended behavior. Safety verification methods aim to detect and prevent such attacks. - Cart-pole system: The\\ncart-pole system is a classic control problem used in robotics. It involves balancing a pole on a cart by applying\\nappropriate forces to the cart. The goal is to keep the pole upright for as long as possible. Safety verification\\nmethods can be used to ensure the stability and safety of the cart-pole system.\", '[Visualization, Computer Science Education, Dynamic Processes, Algorithm Visualization, Data Structure Visualization,\\nProgram Visualization, Hypertextbooks, eTextbooks, Computing Technology, Education Assessment]', '- Zero set of a random analytic function - Perfect spacing under repeated differentiation - Gaussian distribution - Zero\\nset convergence - Poisson process - Convex hull - Pólya-Schur operators - Riemann Hypothesis - Log-concavity of\\ncoefficients - Negative dependence properties - Entire functions - Real zeros - Local density - Riesz theorem - Lattice\\n- Cauchy integral - Random series - Translation-invariant', \"Key features of Shivani Agarwal's work include:  1. Rank Aggregation: Shivani Agarwal has worked on the problem of rank\\naggregation, which involves combining rankings or preferences from multiple sources to create an overall ranking. She\\nhas developed algorithms, such as the Luce spectral ranking (LSR) algorithm, that can efficiently and accurately\\naggregate rankings.  2. Spectral Ranking Algorithms: Shivani Agarwal has contributed to the development of spectral\\nranking algorithms, which use eigenvalues and eigenvectors of matrices derived from pairwise or multiway comparisons to\\ncompute rankings. These algorithms, such as the rank centrality algorithm for pairwise comparisons, provide consistent\\nestimates under certain models, such as the Bradley-Terry-Luce (BTL) model.  3. Provenance in Distributed Systems:\\nShivani Agarwal has also worked on the application of provenance in distributed systems, particularly in the context of\\nnetwork diagnostics and forensic uses. She has developed techniques to track and explain the causes of unexpected\\nbehaviors in computer networks, leveraging the concept of provenance to provide explanations for observed symptoms.  4.\\nMaximum Satisfiability in Program Analysis: Shivani Agarwal has contributed to the field of program analysis,\\nspecifically in the area of maximum satisfiability (MaxSAT) problems. She has developed approaches that utilize MaxSAT\\nto balance different trade-offs in program analysis, leading to improved sample complexity bounds and faster algorithms.\\nOverall, Shivani Agarwal's work spans a range of topics in machine learning, program analysis, and network systems, with\\na focus on developing algorithms and techniques to solve complex problems and improve the understanding and performance\\nof various systems.\", '- High frequency trading - Pricing models - Quantitative finance - Markovian models - Random walk - Statistics -\\nMathematical finance - Algorithmic run-time - Reinforcement learning - Markov decision process - Artificial intelligence\\n- Machine learning - Data science - GPT-3 - LLMs (Large Language Models) - Natural Language Processing - Computer Vision\\n- Image Processing - Concept Bottleneck Models - Language Guided Bottlenecks - Bottleneck layer - Visual recognition -\\nChatGPT - Data provenance', 'There are several key features of Jean Gallier\\'s work that can be identified from the provided information:  1. Concept\\nBottleneck Models (CBM): Gallier has worked on the development of concept bottleneck models, which are interpretable\\nmodels that factor model decisions into human-readable concepts. These models are important for high-stakes applications\\nwhere understanding why a model is failing is critical.  2. Language Guided Bottlenecks (LaBo): Gallier, along with his\\nteam, developed an approach called Language Guided Bottlenecks (LaBo) that leverages a language model (GPT-3) to define\\na large space of possible bottlenecks. This approach eliminates the need for manual specification of concepts and\\nachieves similar accuracy to black box models in visual recognition tasks.  3. Markovian Model: Gallier has worked on a\\nMarkovian model for the price evolution of a stock. This model generalizes many well-studied price evolution models in\\nclassical finance and aims to develop profitable trading strategies.  4. Reinforcement Learning: Gallier has contributed\\nto the field of reinforcement learning by developing new algorithms for achieving near-optimal return in general Markov\\ndecision processes. These algorithms have polynomial bounds on the required resources and explicitly handle the\\nExploration-Exploitation trade-off.  5. Differential Geometry and Lie Groups: Gallier has studied and provided lectures\\non differential geometry and Lie groups. These topics are central to various areas of mathematics and have applications\\nin computer science and engineering.  6. Visualization in Computer Science Education: Gallier has researched the role of\\nvisualization in computer science education and the importance of using visual artifacts to convey dynamic processes,\\nsuch as algorithms and information flow, more effectively. He has focused on the history and development of algorithm\\nvisualization, data structure visualization, and program visualization.  7. Geometric Methods and Applications: Gallier\\nhas authored a book titled \"Geometric Methods and Applications For Computer Science and Engineering,\" which explores the\\napplication of geometric methods in various computer science and engineering domains.  These key features highlight\\nGallier\\'s contributions to machine learning, finance, reinforcement learning, computer science education, differential\\ngeometry, and geometric methods.', '- Reactive Modules - Adaptively Secure Garbled Circuits - Formal Methods in System Design - Parsing Randomness - Free\\ngenerators - Generative Type Abstraction - Neural representations for modeling variation in speech - Adaptive Bottleneck\\nModels - Language Guided Bottlenecks  Each paper explores different concepts and topics related to machine learning.', \"Key features of Eric Eaton's work include:  - Inherently interpretable models  - Factoring model decisions into human-\\nreadable concepts  - Addressing the shortcomings of concept bottleneck models (CBMs)  - Constructing high-performance\\nCBMs without manual specification  - Language Guided Bottlenecks (LaBo) approach using GPT-3  - Producing factual\\nsentences about categories to form candidate concepts  - Efficiently searching for possible bottlenecks through a novel\\nsubmodular utility  - Alignment of GPT-3's sentential concepts to images using CLIP  - Demonstrating that LaBo\\nbottlenecks are effective for visual recognition  - Achieving similar or better performance compared to black box\\napproaches\", '- Bayesian persuasion - Reduced form representations - Joint distribution - Obedience formulation - Receiver actions -\\nProbabilities of actions - Persuasion problems', '- Social Networks - Social Comparison - Social Influence - Online Social Networks - Biased Social Perceptions -\\nConstructed Online Networks - Health Behavior - Algorithm Visualization - Data Structure Visualization - Program\\nVisualization - eTextbooks - Hypertextbooks - Reinforcement Learning - Markov Decision Process - Artificial Intelligence\\n- Algorithmic Run-time - Trading - Markovian Model - High Frequency Trading - Pricing Models - Quantitative Finance -\\nClassical Finance - Random Walk - Statistics - Mathematical Finance - Probabilistic Models - Language Guided Bottlenecks\\n- Concept Bottleneck Models - Machine Learning - Data Science - GPT-3 - LLMs - Natural Language Processing - Computer\\nVision - Image Processing', 'Concept Bottleneck Models (CBM), Language Guided Bottlenecks (LaBo), GPT-3, LLMs (Language and Learning Models), Natural\\nLanguage Processing, Computer Vision, Image Processing, Trading, Markovian Model, High-Frequency Trading, Pricing\\nModels, Quantitative Finance, Classical Finance, Random Walk, Statistics, Mathematical Finance, Probabilistic Models,\\nReinforcement Learning, Markov Decision Process, Artificial Intelligence, Algorithmic Run-time, Algorithm Visualization,\\nData Structure Visualization, Program Visualization, eTextbooks, Hypertextbooks, Replication, Partitioning, Secure\\nDistributed Systems, Modeling, Analysis, and Control of Complex Networked Systems, Engineering Infrastructures, Social\\nNetworks, Technological Infrastructure, Biological Systems, Maximum Satisfiability, Program Analysis, CSS Grid Layout,\\nPermissioned Blockchains, Logistics, Proof-of-Authority Consensus Protocols, Byzantine Fault Tolerance, Malicious\\nAuthorities, Daily Transaction Load.', '- Multi-layer perceptrons - B-splines - Receptive field functions - Function approximation - Nonlinear functions -\\nComplex local structure - Global network formulation - Spline network architectures - Generalization capabilities -\\nComputational efficiency - Learning speed - Simulation results - Spiral problem - Effectiveness of the Spline Net\\napproach', \"Key features of Justin Gottschlich's work include:  - Language Guided Bottlenecks (LaBo): A method for constructing\\nhigh-performance Concept Bottleneck Models (CBMs) without manual specification. LaBo leverages a language model, GPT-3,\\nto define a large space of possible bottlenecks and uses novel submodular utility to promote the selection of\\ndiscriminative and diverse information.  - Concept Bottleneck Models (CBMs): Inherently interpretable models that factor\\nmodel decisions into human-readable concepts. CBMs allow for easy understanding of model failures, making them suitable\\nfor high-stakes applications.  - GPT-3: A language model used in LaBo to generate factual sentences about categories and\\nform candidate concepts.  - CLIP: An image-processing technique used to align GPT-3's sentential concepts to images,\\nforming a bottleneck layer.  - Machine Learning: Gottschlich's work utilizes techniques and concepts from machine\\nlearning to develop and refine models.  - Data Science: Data analysis and preprocessing techniques are applied in\\nGottschlich's work to generate meaningful insights and improve model performance.  - Computer Vision: Gottschlich's work\\ninvolves the application of computer vision techniques to analyze and interpret visual data.  - Image Processing: Image\\nprocessing techniques are used in Gottschlich's work to manipulate and enhance images.  - Reinforcement Learning:\\nAlgorithms for reinforcement learning are explored and developed by Gottschlich for achieving near-optimal return in\\ngeneral Markov decision processes.  - Markov Decision Process: Gottschlich's work involves studying and utilizing Markov\\ndecision processes as models for decision-making in uncertain environments.  - Artificial Intelligence: Gottschlich's\\nwork incorporates various techniques and algorithms from artificial intelligence to solve complex problems and improve\\nmodel performance.  - Algorithmic Run-time: Gottschlich's work focuses on developing algorithms with polynomial bounds\\non their required resources for efficient run-time performance.  Other key concepts and related topics from the papers\\ninclude Trading, High Frequency Trading, Pricing Models, Quantitative Finance, Classical Finance, Random Walk Models,\\nStatistics, Mathematical Finance, Probabilistic Models, Visualization in Computer Science Education, Algorithm\\nVisualization, Data Structure Visualization, Program Visualization, eTextbooks, hypertextbooks, Corpus Phonetics,\\nPhonetic Segmentation, Uncertainty Estimation, Large Language Models, Data Science Education, ChatGPT, and\\nInterdisciplinary Knowledge.\", '- Modularity - Functional modules - Clusters - Similarity - Upstream information - Downstream information - Hidden layer\\nrepresentations - Feedforward networks - Fully connected networks - Hyperparameters - Dropout - Weight regularization -\\nModularity score - Representation-learning - Disentanglement - Compositionality', \"Key features of Mingmin Zhao's work include: - Language Guided Bottlenecks (LaBo) - Concept Bottleneck Models (CBM) -\\nGPT-3 (language model) - CLIP (image-text models) - Visual recognition - Interpretable models - Black box models - Few-\\nshot classification - Linear probes - High-performance models - Image processing - Natural language processing - Machine\\nlearning\", \"Some potential key features of Val B. Tannen's work include: - Language Guided Bottlenecks (LaBo): a method for\\nconstructing high-performance Concept Bottleneck Models (CBMs) without manual specification - GPT-3: a language model\\nused in LaBo to define a large space of possible bottlenecks - CLIP: a method used to align GPT-3's sentential concepts\\nto images and form a bottleneck layer - Machine learning: Tannen's work involves applying machine learning techniques to\\nconstruct interpretable models - Data Science: Tannen's work is relevant to the field of data science, as it focuses on\\nmodeling and analyzing data - Natural Language Processing: Tannen's work incorporates natural language processing\\ntechniques, particularly in the use of GPT-3 - Computer Vision: Tannen's work applies concepts from computer vision,\\nsuch as aligning sentential concepts to images using CLIP - Image Processing: Tannen's work involves processing images\\nusing techniques like CLIP for aligning sentential concepts to images\", \"Key features of Mark Yatskar's work include: - Language Guided Bottlenecks (LaBo) technique for constructing high-\\nperformance Concept Bottleneck Models (CBMs) in machine learning - Leveraging the GPT-3 language model for defining a\\nlarge space of possible bottlenecks in CBMs - Using the CLIP model for aligning GPT-3's sentential concepts to images in\\nCBMs for visual recognition - Achieving comparable or better performance than black box models with inherently\\ninterpretable models - Addressing the shortcomings of CBMs, such as the need for manual specification of concepts and\\nunder-performance compared to black box models - Applications in various domains, including computer vision and image\\nprocessing - Prioritizing discriminative and diverse information through a novel submodular utility in bottleneck\\nselection - Demonstrating the effectiveness of LaBo in few-shot classification tasks.\", '- Machine learning - Data Science - GPT-3 - LLMs - Natural Language Processing - Computer Vision - Image Processing -\\nTrading - Markovian Model - High Frequency Trading - Pricing Models - Quantitative Finance - Classical Finance - Random\\nWalk - Statistics - Mathematical Finance - Probabilistic Models - Reinforcement Learning - Markov Decision Process -\\nArtificial Intelligence - Algorithmic Run-time - Multiserver Models - YOSO Models - Threshold Cryptography - Market\\nMicrostructure - Inference - Predictive Models - Feature Selection - Feature Engineering - Optimized Trade Execution -\\nReinforcement Learning - Secure Distributed Systems - Adaptive Query Processing - Experimental Design - Balance Testing', \"The key features of Li-San Wang's work include:  - Language Guided Bottlenecks (LaBo): A framework that leverages GPT-3,\\na language model, to define a large space of possible bottlenecks in concept bottleneck models (CBMs). LaBo uses GPT-3\\nto produce factual sentences about categories to form candidate concepts, and efficiently searches possible bottlenecks\\nthrough a submodular utility.  - Concept Bottleneck Models (CBMs): Inherently interpretable models that factor model\\ndecisions into human-readable concepts. CBMs allow for easy understanding of why a model is failing, which is critical\\nfor high-stakes applications.  - GPT-3: A language model used in the LaBo framework to define a large space of possible\\nbottlenecks. GPT-3 generates factual sentences about categories to form candidate concepts.  - CLIP: A method for\\naligning GPT-3's sentential concepts to images, used in the LaBo framework to form a bottleneck layer.  - Visual\\nrecognition: The task of identifying and categorizing objects and patterns in images or videos. LaBo demonstrates that\\nits bottlenecks are highly effective for concepts important to visual recognition.  - Machine learning: The field of\\nstudy that focuses on developing algorithms and models that allow computer systems to learn and make decisions or\\npredictions without being explicitly programmed.  - Data science: The interdisciplinary field that combines scientific\\nmethods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.  -\\nNatural language processing (NLP): The field of study that focuses on the interaction between computers and human\\nlanguage, including tasks such as natural language understanding, machine translation, and text generation.  - Computer\\nvision: The field of study that focuses on enabling computers to gain high-level understanding from digital images or\\nvideos.  - Image processing: The analysis, manipulation, and interpretation of images using various mathematical\\noperations and algorithms.  - Linear probes: A method for evaluating the performance of bottleneck models, compared to\\nblack box models, in few-shot classification tasks.  - High-stakes applications: Applications or tasks that have\\nsignificant consequences or impact, such as medical diagnosis, autonomous driving, or financial forecasting.  - Black\\nbox models: Models that are difficult to interpret or explain, as their decision-making processes are not readily\\nunderstandable by humans.  - Few-shot classification: A classification task where the model is trained with only a small\\nnumber of labeled examples for each class.  - Interpretable models: Models that provide explanations or insights into\\ntheir decision-making processes, making their outputs more understandable and trustworthy.  - Algorithmic run-time: The\\ntime or resources required for an algorithm or model to complete its execution or achieve a desired outcome.  -\\nReinforcement learning: A machine learning approach where an agent learns to take actions in an environment to maximize\\na cumulative reward signal.  - Markov decision processes: Mathematical models used to describe decision-making problems\\nin a stochastic environment, where the outcome of an action depends on the current state and is governed by\\nprobabilities.  - Artificial intelligence: The field of study that focuses on creating computer systems that can perform\\ntasks that would typically require human intelligence, such as speech recognition, problem-solving, and decision-making.\\n- Undiscounted and discounted cases: Refers to different formulations of reinforcement learning problems, where the\\nformer does not apply a discount factor to future rewards, while the latter applies a discount factor to give higher\\nweight to immediate rewards.\", \"Key features of Christopher S. Yoo's work: - Language Guided Bottlenecks (LaBo) for creating interpretable models -\\nConcept Bottleneck Models (CBM) for factorizing model decisions into human-readable concepts - Leveraging GPT-3 (a\\nlanguage model) to define a large space of possible bottlenecks - Aligning GPT-3's sentential concepts to images using\\nCLIP for visual recognition - Achieving high-performance CBMs without manual specification - Experiments demonstrating\\nthe effectiveness of LaBo in few-shot classification - Addressing the shortcomings of CBMs and promoting their broad\\nadoption - Comparing the performance of inherently interpretable models to black box approaches - Application of machine\\nlearning, data science, natural language processing, and computer vision.\", '- Automatic recognition of suprasegmentals in speech - Fine-tuning wav2vec 2.0 with CTC - Recognition of syllables,\\ntones, and pitch accents - Utilizing segmental information for Mandarin tone recognition - Language models for tonal\\nsyllables - Combining Mandarin tone recognition with English phoneme recognition', \"Key features of Camillo Taylor's work: - Language Guided Bottlenecks (LaBo) approach for constructing Concept Bottleneck\\nModels (CBMs) - Leveraging GPT-3, a language model, to define a large space of possible bottlenecks - Using GPT-3 to\\nproduce factual sentences about categories to form candidate concepts - Searching for possible bottlenecks through a\\nnovel submodular utility that promotes the selection of discriminative and diverse information - Aligning GPT-3's\\nsentential concepts to images using CLIP to form a bottleneck layer - Highly effective prior for concepts important to\\nvisual recognition - Comparable or better performance than black box approaches in visual recognition tasks -\\nApplication of inherently interpretable models in high-stakes applications\", \"Key features of Cynthia Sung's work: - Language Guided Bottlenecks (LaBo) - Concept Bottleneck Models (CBM) - GPT-3\\n(language model) - CLIP (image alignment) - Visual recognition - Machine learning - Data science - Computer vision -\\nImage processing - Trading - Markovian model - High frequency trading - Pricing models - Quantitative finance -\\nClassical finance - Random walk - Statistics - Mathematical finance - Probabilistic models - Reinforcement learning -\\nMarkov decision process - Artificial intelligence - Algorithmic run-time - Robotics - Visualization in computer science\\neducation - Algorithm visualization - Data structure visualization - Program visualization - eTextbooks - Hypertextbooks\\n- Maximum Satisfiability (MaxSAT) - Program analysis - Connectionist Temporal Classification (CTC) - Syllables - Tones -\\nPitch accents - Automatic recognition of suprasegmentals in speech - Phoneme recognition - Large language models (LLMs)\\n- Data science education with LLMs - ChatGPT - AI-guided programming - Interdisciplinary knowledge\", '- Computational social science - Social and organizational networks - Collective dynamics of human systems - Web-based\\nexperiments - Analysis of large-scale digital data - Production, consumption, and absorption of news - Writing and\\nrevising process - Creativity in science - Climbing - Balance of competing trade-offs in program analysis - Maximum\\nSatisfiability (MaxSAT) problem - Boolean Satisfiability (SAT) problem - Reinforcement learning - Markov decision\\nprocesses - Artificial intelligence - Algorithmic run-time efficiency - Secure distributed systems - Replication and\\npartitioning in building secure distributed systems - Data analytics systems - Instance optimization in data analytics -\\nSystem tuning and knob adjustment - Learned index structures - Data storage layouts and data replication - Partial\\nmaterialized views - Automatic self-optimization of data systems', '[Forced alignment, Phonetics research, Speech recognition, Text-to-speech synthesis, Automatic speech recognition,\\nSpeech science, Corpus linguistics, Phonetics segmentation, Speech technology, Self-attention mechanisms, Sparse\\nvariables, Inductive biases, Transformer networks, Deep learning, Sequential data modeling, Visualization, Computer\\nscience education, Algorithm visualization, Data structure visualization, Program visualization, eTextbooks,\\nHypertextbooks, Neural nets, Robustness, Adversarial examples, Linear program, Deep neural nets, MNIST dataset, CIFAR-10\\ndataset]', \"Key features of Eric Wong's work include:  1. Concept Bottleneck Models (CBM): Wong's work focuses on developing\\ninherently interpretable models that factor model decisions into human-readable concepts. CBMs allow for easy\\nunderstanding of why a model is failing, which is important for high-stakes applications.  2. Language Guided\\nBottlenecks (LaBo): Wong's research introduces LaBo, an approach that leverages a language model (GPT-3) to define a\\nlarge space of possible bottlenecks. LaBo efficiently searches for discriminating and diverse information to form\\nbottleneck layers in CBMs.  3. Machine Learning: Wong's work is centered around the field of machine learning, with a\\nfocus on robustness, reliability, and scalability. He aims to develop principled methods that guarantee the behavior of\\nmachine learning models.  4. GPT-3 and LLMs (Language Models): Wong utilizes the language model GPT-3 to generate\\nfactual sentences about categories, which are used to form candidate concepts for bottlenecks in CBMs. Language models\\nare a key component in Wong's approach.  5. Data Science: Wong's research intersects with the field of data science,\\nparticularly in terms of understanding and debugging machine learning models. He aims to develop methods that are\\npractical and scalable in real-world settings.  6. Computer Vision and Image Processing: Wong's research explores the\\napplication of CBMs and LaBo in the field of computer vision and image recognition. He demonstrates the effectiveness of\\nbottleneck layers formed by aligning GPT-3's sentential concepts to images using CLIP.  7. Trading and Pricing Models:\\nWong's work extends beyond machine learning and delves into the field of quantitative finance. He explores trading\\nstrategies, pricing models, random walk models, and probabilistic models for stock price evolution.  8. Reinforcement\\nLearning and Markov Decision Processes: Wong develops algorithms for reinforcement learning in general Markov decision\\nprocesses. His algorithms have polynomial bounds on resources required and explicitly handle the exploration-\\nexploitation trade-off.\", '- Query processing - Adaptive query processing - Machine learning - Natural Language Processing', 'Features and concepts: - QWIRE is a language for defining quantum circuits and manipulating them within a classical host\\nlanguage - QWIRE is minimal and sound with respect to the physical properties of quantum mechanics - QWIRE is expressive\\nand modular due to its relationship with the host language - The language is designed to be extensible, allowing for\\nanalysis frameworks and dependent types - QWIRE uses a constraint language, such as Datalog, to express the\\nspecification of the static analysis - The language leverages continuous modes of logical reasoning to address\\nchallenges in modern software applications - QWIRE provides a denotational semantics for circuits in terms of density\\nmatrices - The language includes a type system and operational semantics that are safe and strongly normalizing - Open\\nproblems exist in analysis usability, language expressiveness, and solver techniques.', '- Maximum Satisﬁability in Program Analysis: Applications and Techniques - Markovian Model for price evolution of a\\nstock - Reinforcement Learning and resource bounds - Language Guided Bottlenecks (LaBo) for machine learning - Concept\\nBottleneck Models (CBM) - Random generation and parsing - Deformable Keypoint Pyramids for computer vision -\\nVisualization in Computer Science Education - Complex networked systems and engineering infrastructures', \"Based on the provided information, it is not clear what the key features of Lyle Ungar's work are. The information given\\nincludes research interests of other individuals and papers from different fields. To determine the key features of Lyle\\nUngar's work, additional information specifically about Lyle Ungar's research and publications is needed.\", \"Some key features of Oleg Sokolsky's work include: concept bottleneck models, language guided bottlenecks, GPT-3, CLIP,\\ndiscriminative and diverse information, few-shot classification, Markovian model, trading strategy, reinforcement\\nlearning, exploration-exploitation trade-off, generative type abstraction, type-level computation, type generativity,\\nabstract data types, type structure, design languages, Haskell, newtype deriving, type functions, reinforcement\\nlearning, Markov decision process, artificial intelligence, algorithmic run-time, static analysis, constraint solving,\\nprovenance, inference logics, consistency, automatic recognition, suprasegmentals, syllables, Mandarin tones, pitch\\naccents, wav2vec 2.0, multitask, multi-layer perceptrons, B-Spline receptive field functions, Stephen H. Lane, Marshall\\nG. Flax, David A. Handelman, Jack J. Gelfand, human information processing, convolutional filtering, sampled manifolds,\\nZhiyang Wang, Luana Ruiz, Alejandro Ribeiro, static analysis, constraint solving, Datalog, programming languages,\\nfeature engineering, generative type abstraction, unsupervised learning, feature representation, generative type\\nabstraction, abstraction types, design languages, type-level computation, indexing type families, parametric contexts,\\nHaskell source programs, consistent FC2 axiom sets, system FC, Stephanie Weirich, generative type abstraction, abstract\\ntypes, abstract type system, parametric and non-parametric contexts, consistent axiom sets, rewrite system, type safety,\\ncore language, type generativity, non-parametric features, newtype deriving, type functions, Hai-Po Yin, Hongming Pu,\\ngenerative type abstraction, parametric context, non-parametric context, modular languages, abstract type is distinct,\\ntype-level computation, non-parametric type functions, generative type abstraction, non-parametric type-level features,\\ncore language, aiml, document categorization, Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, Ali Farhadi, semantic\\nsparsity, structured visual classification, structured summaries, semantic rarity, complete CRF-based structured\\nprediction model, tensor composition function, semantic data augmentation, top-5 verb noun-role accuracy, semantic\\nsparsity, major syntactic boundaries, semantic roles, marking the boundaries, syntactic-parsing, high-dimensional\\nacoustic space, prosodic phrasing, syntactic constitutions, prosodic phrasing, continuous speech corpora, empirical\\nstudies, syntax-prosody mapping, syntactically-parsed speech corpora, acoustic and syntactic features, syntax-phrasing\\nmapping, reliable correlation, discrete hierarchies, boundary-alignment preference.\", '[Data Intimacy, Machine Learning, Artificial Intelligence, Consumer Privacy, Data Sources, Data Practices, Technology\\nCompanies, Google, Facebook, Predictions, Inferences, User Data, Surface Data, Policy, Regulation]', '- Forced Alignment - Corpus phonetics - Phonetic segmentation - Digital speech - Phonetics research - Phonetic features\\n- Speech recognition - Acoustic models - Hidden Markov Models (HMM) - Gaussian Mixture Model (GMM) - Automatic phonetic\\nsegmentation - Orthographic transcription - Pronouncing dictionary - Grapheme to phoneme rules - Phone boundaries -\\nAcoustic-phonetic features - Fundamental frequency - Speech synthesis systems - Inter-annotator agreement - Speech\\ncorpora - Pronunciation variation', \"From Scott Weinstein's work: - Language Guided Bottlenecks (LaBo) - Concept Bottleneck Models (CBM) - GPT-3 - Inherently\\ninterpretable models - Visual recognition - Computer Vision - Image Processing - High-stakes applications - Manual\\nspecification of concepts - Black box models - Linear probes - Few-shot classification - Performance comparison with\\nblack box approaches  From the second paper: - Trading - Markovian Model - High Frequency Trading - Pricing Models -\\nQuantitative Finance - Classical Finance - Random Walk - Statistics - Mathematical Finance - Probabilistic Models  From\\nthe third paper: - Reinforcement Learning - Markov Decision Process - Artificial Intelligence - Algorithmic Run-time\\nFrom the fourth paper: - Self-attention mechanisms - Inductive biases - Sparse variables - Bounded-norm Transformer\\nnetworks - Sample complexity - Synthetic experiments - Sparse Boolean functions  From the fifth paper: - Zeros of a\\nrandom analytic function - Perfect spacing under repeated differentiation - Gauss-Lucas theorem - Diﬀerentiation\\noperator - Complex zero decreasing - Riemann Hypothesis - Combinatorial reasons for studying locations of zeros - Zero\\nset convergence - Maximum distance between zeros - Convergence of zero set under repeated differentiation - Zero spacing\\n- Evenly spaced zeros  From the sixth paper: - Multi-Layer Perceptrons (MLPs) - B-Spline Receptive Field Functions -\\nNonlinear functions - Global nature of function approximations - Computational efficiency - Learning speed - Spiral\\nproblem - Effectiveness of Spline Net approach  From the seventh paper: - Visualization in Computer Science Education -\\nDynamic processes - Information flow - Static media - Lectures - History of visualization in CS education - Positive\\neducational assessment - Computing technology - Online hypertextbooks - Algorithm visualization - Data structure\\nvisualization - Program visualization - eTextbooks - Hypertextbooks  From the eighth paper: - Social Networks - Social\\nComparison - Social influence processes - Negative consequences of social comparison - Biased social perceptions -\\nConstructed online networks - Changes in health behavior  From the ninth paper: - Tangent Bundle Filters - Tangent\\nBundle Neural Networks (TNNs) - Manifolds - Cellular Sheaves - Connection Laplacian operator - Convergence of discrete\\nTNNs to continuous TNNs - Denoising task - Unit 2-sphere\", \"Concepts, features, and related topics from Stephanie Weirich's work:  1. Language Guided Bottlenecks (LaBo) 2. Concept\\nBottleneck Models (CBM) 3. Machine learning 4. Data Science 5. GPT-3 (language model) 6. LLMs (Language models) 7.\\nNatural Language Processing (NLP) 8. Computer Vision 9. Image Processing 10. Trading 11. Markovian Model 12. High-\\nFrequency Trading 13. Pricing Models 14. Quantitative Finance 15. Classical Finance 16. Random Walk 17. Statistics 18.\\nMathematical Finance 19. Probabilistic Models 20. Reinforcement Learning 21. Markov Decision Process 22. Artificial\\nIntelligence 23. Algorithmic Run-time 24. Adaptive Query Processing 25. Visualization in Computer Science Education 26.\\nAlgorithm Visualization 27. Data Structure Visualization 28. Program Visualization 29. eTextbooks, hypertextbooks 30.\\nData Intimacy 31. Consumer Privacy 32. Auditing Algorithms 33. Understanding Algorithmic Systems 34. Federally Funded\\nResearch and Development in Networking and Information Technology 35. Complex Networked Systems 36. Engineering\\nInfrastructures 37. Social Networks 38. Technological Infrastructure 39. Biological Systems 40. Inductive Biases 41.\\nVariable Creation 42. Self-Attention Mechanisms\", \"Key Features of Steven Zdancemic's Work:  - Language Guided Bottlenecks (LaBo): A methodology for constructing high-\\nperformance Concept Bottleneck Models (CBM) without manual specification. LaBo leverages a language model, GPT-3, to\\ndefine a large space of possible bottlenecks in CBMs. - GPT-3: A language model used in LaBo to generate factual\\nsentences about categories to form candidate concepts. - CLIP: A model used to align GPT-3's sentential concepts to\\nimages in order to form a bottleneck layer in LaBo. - Inherently interpretable models: Steven Zdancemic's work focuses\\non developing interpretable machine learning models, specifically CBMs, that factor model decisions into human-readable\\nconcepts. - Visual recognition: LaBo bottlenecks excel at few-shot classification in visual recognition tasks. - High-\\nstakes applications: The interpretability of CBMs is crucial for high-stakes applications where it is important to\\nunderstand why a model is failing. - Machine learning and data science: Steven Zdancemic's work lies in the intersection\\nof machine learning, data science, natural language processing, and computer vision. - Performance comparison: LaBo\\ndemonstrates that inherently interpretable models can achieve similar or better performance than black box approaches.\", 'Concept: - Workﬂow provenance - Database-style and workﬂow-style provenance - Provenance framework - Pig Latin -\\nProvenance graph - ZoomIn and ZoomOut', \"Key features of Weijie Su's work include: - Language Guided Bottlenecks (LaBo): This approach leverages a language\\nmodel, GPT-3, to define a large space of possible bottlenecks for Concept Bottleneck Models (CBMs). It uses GPT-3 to\\nproduce factual sentences about categories and searches for bottlenecks through a novel submodular utility. - Concept\\nBottleneck Models (CBMs): CBMs are inherently interpretable models that factor model decisions into human-readable\\nconcepts. They allow for easy understanding of why a model is failing, which is important for high-stakes applications.\\n- High-performance CBMs: Su's work shows how to construct high-performance CBMs without manual specification, achieving\\nsimilar accuracy to black box models. This addresses the under-performance issue of CBMs and promotes their broader\\nadoption. - Application to visual recognition: Su demonstrates that LaBo bottlenecks excel at few-shot classification in\\nvisual recognition. They are more accurate than black box linear probes at 1 shot and comparable with more data. - Wide\\napplicability of interpretable models: LaBo shows that inherently interpretable models like CBMs can be widely applied\\nwith similar or better performance than black box approaches.\"]\n","CPU times: user 12.1 s, sys: 1.12 s, total: 13.2 s\n","Wall time: 21min 49s\n"]}]},{"cell_type":"code","source":["import numpy as np\n","results_df = pd.DataFrame()\n","\n","results_df['professor'] = np.array(professor_list)\n","results_df['features'] = np.array(results)"],"metadata":{"id":"NfVW_jJAQ08s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"pp1rqJ47Tedr","outputId":"f76363c9-b145-41ca-fb9b-9020d0ac370c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                professor                                           features\n","0             Bong Ho Kim  In this paper, the authors discuss the concept...\n","1           Junhyong Kim   Key features of Junhyong Kim's work: - Machine...\n","2    Daniel E. Koditschek  - Adaptive Query Processing - Transdiagnostic ...\n","3          Konrad Koering  - Database Provenance - Diagnostic and Forensi...\n","4             Lingjie Liu  - Large Language Models (LLMs) - ChatGPT - Dat...\n","..                    ...                                                ...\n","97        Scott Weinstein  From Scott Weinstein's work: - Language Guided...\n","98      Stephanie Weirich  Concepts, features, and related topics from St...\n","99       Steven Zdancemic  Key Features of Steven Zdancemic's Work:  - La...\n","100         Swapneel Seth  Concept: - Workﬂow provenance - Database-style...\n","101             Weijie Su  Key features of Weijie Su's work include: - La...\n","\n","[102 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-390d02ab-3ded-43c2-bb48-e9fd96bc049d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>professor</th>\n","      <th>features</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Bong Ho Kim</td>\n","      <td>In this paper, the authors discuss the concept...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Junhyong Kim</td>\n","      <td>Key features of Junhyong Kim's work: - Machine...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Daniel E. Koditschek</td>\n","      <td>- Adaptive Query Processing - Transdiagnostic ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Konrad Koering</td>\n","      <td>- Database Provenance - Diagnostic and Forensi...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Lingjie Liu</td>\n","      <td>- Large Language Models (LLMs) - ChatGPT - Dat...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Scott Weinstein</td>\n","      <td>From Scott Weinstein's work: - Language Guided...</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>Stephanie Weirich</td>\n","      <td>Concepts, features, and related topics from St...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>Steven Zdancemic</td>\n","      <td>Key Features of Steven Zdancemic's Work:  - La...</td>\n","    </tr>\n","    <tr>\n","      <th>100</th>\n","      <td>Swapneel Seth</td>\n","      <td>Concept: - Workﬂow provenance - Database-style...</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>Weijie Su</td>\n","      <td>Key features of Weijie Su's work include: - La...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>102 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-390d02ab-3ded-43c2-bb48-e9fd96bc049d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-390d02ab-3ded-43c2-bb48-e9fd96bc049d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-390d02ab-3ded-43c2-bb48-e9fd96bc049d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a4729278-6c81-45e7-9a42-1ea170d33d42\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4729278-6c81-45e7-9a42-1ea170d33d42')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a4729278-6c81-45e7-9a42-1ea170d33d42 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["results_df.to_csv('professor_research_results.csv')"],"metadata":{"id":"S7qy2VO6TfS-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clean-up"],"metadata":{"id":"JQkPNys_EZy8"}},{"cell_type":"code","source":["def text_split(x):\n","  x_new = x.replace('-', ',').replace('1.', ',').replace('2.', ',')\\\n"," .replace('3.', ',').replace('4.', ',').replace('5.', ',').replace('6.', ',')\\\n"," .replace('7.', ',').replace('8.', ',').replace('9.', ',').replace('10.', ',')\\\n"," .replace('11.', ',').replace('12.', ',').replace('13.', ',').replace('14.', ',')\\\n"," .replace('15.', ',').replace('16.', ',').replace('17.', ',').replace('18.', ',')\\\n"," .replace('19.', ',').replace('20.', ',')\n","  return x_new.split(',')"],"metadata":{"id":"DhP_S9gKaQ6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df['features_new'] = results_df['features'].apply(lambda x: text_split(x))"],"metadata":{"id":"6xMzZ4yWajBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df = results_df.explode('features_new')"],"metadata":{"id":"45ayyZ-8a1iu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df.drop(columns=['features'], inplace=True)"],"metadata":{"id":"QzTyzMwZZxaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df[150:190]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ygf8-zWsHvEn","outputId":"71f5256b-fcc9-44f2-af4d-29bc58f9ab87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             professor                                       features_new\n","8   Linh Thi Xuan Phan                                       \\nEncryption\n","8   Linh Thi Xuan Phan                                  Computer Science.\n","9      Rahul Mangharam   Key features of Rahul Mangharam's work include: \n","9      Rahul Mangharam                     Intersection of formal methods\n","9      Rahul Mangharam                                   machine learning\n","9      Rahul Mangharam                  and controls for\\nmedical devices\n","9      Rahul Mangharam                         energy efficient buildings\n","9      Rahul Mangharam                            and autonomous systems \n","9      Rahul Mangharam   Research at the PRECISE Center and direction ...\n","9      Rahul Mangharam                             Recognition and awards\n","9      Rahul Mangharam   including the 2016 US\\nPresidential Early Car...\n","9      Rahul Mangharam              2014 IEEE Benjamin Franklin Key Award\n","9      Rahul Mangharam                         and 2013 NSF CAREER Award \n","9      Rahul Mangharam   Development of\\nLanguage Guided Bottlenecks (...\n","9      Rahul Mangharam              which leverages a language model (GPT\n","9      Rahul Mangharam                           3) for constructing high\n","9      Rahul Mangharam  performance\\nConcept Bottleneck Models (CBMs) ...\n","9      Rahul Mangharam   Application of machine learning to problems i...\n","9      Rahul Mangharam                                     energy systems\n","9      Rahul Mangharam                                    clinical trials\n","9      Rahul Mangharam                       building energy optimization\n","9      Rahul Mangharam                                           and more\n","10           Tal Rabin                   Maximum Satisfiability (MaxSAT) \n","10           Tal Rabin                                  Program analysis \n","10           Tal Rabin                    Balance of competing tradeoffs \n","10           Tal Rabin             Boolean Satisfiability (SAT)\\nproblem \n","10           Tal Rabin                                      Optimization \n","10           Tal Rabin                            Optimization extension \n","10           Tal Rabin                          Diagnosing query answers \n","10           Tal Rabin                               Balancing tradeoffs \n","10           Tal Rabin                             Program optimization\\n\n","10           Tal Rabin                            Large MaxSAT instances \n","10           Tal Rabin                                 General framework \n","10           Tal Rabin                           Solving large instances \n","10           Tal Rabin                                    New techniques \n","10           Tal Rabin                            Reinforcement learning \n","10           Tal Rabin                         \\nMarkov decision process \n","10           Tal Rabin                           Artificial intelligence \n","10           Tal Rabin                                    Algorithmic run\n","10           Tal Rabin                                              time "],"text/html":["\n","  <div id=\"df-fd23a6ff-fe5b-4107-9c8d-a8222584d1fe\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>professor</th>\n","      <th>features_new</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8</th>\n","      <td>Linh Thi Xuan Phan</td>\n","      <td>\\nEncryption</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Linh Thi Xuan Phan</td>\n","      <td>Computer Science.</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>Key features of Rahul Mangharam's work include:</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>Intersection of formal methods</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>machine learning</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>and controls for\\nmedical devices</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>energy efficient buildings</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>and autonomous systems</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>Research at the PRECISE Center and direction ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>Recognition and awards</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>including the 2016 US\\nPresidential Early Car...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>2014 IEEE Benjamin Franklin Key Award</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>and 2013 NSF CAREER Award</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>Development of\\nLanguage Guided Bottlenecks (...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>which leverages a language model (GPT</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>3) for constructing high</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>performance\\nConcept Bottleneck Models (CBMs) ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>Application of machine learning to problems i...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>energy systems</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>clinical trials</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>building energy optimization</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Rahul Mangharam</td>\n","      <td>and more</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Maximum Satisfiability (MaxSAT)</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Program analysis</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Balance of competing tradeoffs</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Boolean Satisfiability (SAT)\\nproblem</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Optimization</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Optimization extension</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Diagnosing query answers</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Balancing tradeoffs</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Program optimization\\n</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Large MaxSAT instances</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>General framework</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Solving large instances</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>New techniques</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Reinforcement learning</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>\\nMarkov decision process</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Artificial intelligence</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>Algorithmic run</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Tal Rabin</td>\n","      <td>time</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd23a6ff-fe5b-4107-9c8d-a8222584d1fe')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fd23a6ff-fe5b-4107-9c8d-a8222584d1fe button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fd23a6ff-fe5b-4107-9c8d-a8222584d1fe');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7e941fbd-6c3c-405d-8096-2e2aeab3bb30\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e941fbd-6c3c-405d-8096-2e2aeab3bb30')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7e941fbd-6c3c-405d-8096-2e2aeab3bb30 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["results_df = results_df[results_df.features_new != '']"],"metadata":{"id":"VwxDRK6QbePl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df.to_csv('professor_research_results.csv')"],"metadata":{"id":"2C3MzG2Ubocc"},"execution_count":null,"outputs":[]}]}